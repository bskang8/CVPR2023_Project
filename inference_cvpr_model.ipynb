{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOBZspk6fA6Kv146TfWP+o0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "509ad686592b45f183b14791556b6d77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_773c31ee9fd746e2942eb69140c2e6d4",
              "IPY_MODEL_284bb3db3b49494dabe2b4237ee9ffcb",
              "IPY_MODEL_54a397d25bcf41a88264dda1f9daf4ce"
            ],
            "layout": "IPY_MODEL_d9e41bd2048746ceb7615bfac9ccc701"
          }
        },
        "773c31ee9fd746e2942eb69140c2e6d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_837f752b8b0244638c1456f40c2cec41",
            "placeholder": "​",
            "style": "IPY_MODEL_fefc70b23cda4516958cb9ebb0ea586b",
            "value": "Downloading (…)/adapter_config.json: 100%"
          }
        },
        "284bb3db3b49494dabe2b4237ee9ffcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aebb6eaa0aa43f7a06faf8d5395a4e4",
            "max": 438,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03b1ecc77e1e4351b6625fcc5f98a6a0",
            "value": 438
          }
        },
        "54a397d25bcf41a88264dda1f9daf4ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e360f594a984299bd467372fb0b21d4",
            "placeholder": "​",
            "style": "IPY_MODEL_5323f74331e145b7a2cf7e24d9445a0e",
            "value": " 438/438 [00:00&lt;00:00, 24.6kB/s]"
          }
        },
        "d9e41bd2048746ceb7615bfac9ccc701": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "837f752b8b0244638c1456f40c2cec41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fefc70b23cda4516958cb9ebb0ea586b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5aebb6eaa0aa43f7a06faf8d5395a4e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03b1ecc77e1e4351b6625fcc5f98a6a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e360f594a984299bd467372fb0b21d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5323f74331e145b7a2cf7e24d9445a0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fcf5d95f10b4d68a3853656b38bcfca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_961a1da8ce7b448186a7b3ad0e349bb7",
              "IPY_MODEL_9c27ddd2d58849879d02a015be35a6b5",
              "IPY_MODEL_d00c43a31bb646fd9458f3f726b9ab78"
            ],
            "layout": "IPY_MODEL_90a291bdf51a455db543c974bb6f260f"
          }
        },
        "961a1da8ce7b448186a7b3ad0e349bb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81cac506ceea41688f4ce6dedcdcbc78",
            "placeholder": "​",
            "style": "IPY_MODEL_139ea49070a3410482853167cbea2e46",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "9c27ddd2d58849879d02a015be35a6b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea5a3427d5d5468caf0f286937723391",
            "max": 715,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_815c7a13e09b43119f732dc47349a213",
            "value": 715
          }
        },
        "d00c43a31bb646fd9458f3f726b9ab78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d92df0cc509e4119997e90f7f141da98",
            "placeholder": "​",
            "style": "IPY_MODEL_2169eaf148804a2e8d3d17ec6a403f53",
            "value": " 715/715 [00:00&lt;00:00, 56.1kB/s]"
          }
        },
        "90a291bdf51a455db543c974bb6f260f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81cac506ceea41688f4ce6dedcdcbc78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "139ea49070a3410482853167cbea2e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea5a3427d5d5468caf0f286937723391": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "815c7a13e09b43119f732dc47349a213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d92df0cc509e4119997e90f7f141da98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2169eaf148804a2e8d3d17ec6a403f53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd0095ac0a1e40d0892cf3930d4ab93c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff101602632145b0ada7f869eccf5ca5",
              "IPY_MODEL_816e17bf37c9413984074c73549e0855",
              "IPY_MODEL_e6c7cb5fc55e46478349ec466f23cb6f"
            ],
            "layout": "IPY_MODEL_d7e8887501d84754b84366d6c76e0430"
          }
        },
        "ff101602632145b0ada7f869eccf5ca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a520f5f4d81d4f78a983d8a2a2ab4fb6",
            "placeholder": "​",
            "style": "IPY_MODEL_8de20f816a414827bc9d28e2f300deb1",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "816e17bf37c9413984074c73549e0855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26d12fd5169f4444b8a2daa5c9657db0",
            "max": 6005153394,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f8cfdca0a284a80b85996ec3ab6432a",
            "value": 6005153394
          }
        },
        "e6c7cb5fc55e46478349ec466f23cb6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d4ac35b28f44f269e723421ce329d95",
            "placeholder": "​",
            "style": "IPY_MODEL_db5ad5b3708245b7b90870fd749fec81",
            "value": " 6.01G/6.01G [01:56&lt;00:00, 43.8MB/s]"
          }
        },
        "d7e8887501d84754b84366d6c76e0430": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a520f5f4d81d4f78a983d8a2a2ab4fb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8de20f816a414827bc9d28e2f300deb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26d12fd5169f4444b8a2daa5c9657db0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f8cfdca0a284a80b85996ec3ab6432a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d4ac35b28f44f269e723421ce329d95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db5ad5b3708245b7b90870fd749fec81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a22aaf6d729480288201777b02c829f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c8aeed7faab4ce98d75243468122c9d",
              "IPY_MODEL_1a35e0b16b234b4ba3d9f2e83e4ca1ce",
              "IPY_MODEL_af059bbe302745c1b77294914b38f4a3"
            ],
            "layout": "IPY_MODEL_793c1073dbac486cac6cc33710497871"
          }
        },
        "1c8aeed7faab4ce98d75243468122c9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d8a4dddec4843d3a236fa72a472de3c",
            "placeholder": "​",
            "style": "IPY_MODEL_813e47c48be74e2e9b3d0eb4abd62058",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "1a35e0b16b234b4ba3d9f2e83e4ca1ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05346e42bba543a080c3bfebe10870ac",
            "max": 199,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ea6e51a257e4069a2ec75f77b4ef548",
            "value": 199
          }
        },
        "af059bbe302745c1b77294914b38f4a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b704c5abec94084b75991884d8a8122",
            "placeholder": "​",
            "style": "IPY_MODEL_25616d03f6f6476b853a95a66ac01387",
            "value": " 199/199 [00:00&lt;00:00, 15.2kB/s]"
          }
        },
        "793c1073dbac486cac6cc33710497871": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d8a4dddec4843d3a236fa72a472de3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "813e47c48be74e2e9b3d0eb4abd62058": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05346e42bba543a080c3bfebe10870ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ea6e51a257e4069a2ec75f77b4ef548": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b704c5abec94084b75991884d8a8122": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25616d03f6f6476b853a95a66ac01387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9f6fd2904554305af47086c049826b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48c8fb3c51ac41cca608f581aac048c6",
              "IPY_MODEL_9a6d38a49bb548b182cb4cc2ce2d438d",
              "IPY_MODEL_4a6f9f5ed8ee426897eba10a8e7597b7"
            ],
            "layout": "IPY_MODEL_b4f96e6970714f5fade3e38195ba8850"
          }
        },
        "48c8fb3c51ac41cca608f581aac048c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44aaacda82a24f86927ee1715a483acd",
            "placeholder": "​",
            "style": "IPY_MODEL_eed0facf9f6c455f846c4968133f4a38",
            "value": "Downloading tokenizer.json: 100%"
          }
        },
        "9a6d38a49bb548b182cb4cc2ce2d438d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89f1a3dbff984860b5fc69cf530bbf72",
            "max": 14500438,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_936844d785534f0cb45d58590f475fdd",
            "value": 14500438
          }
        },
        "4a6f9f5ed8ee426897eba10a8e7597b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c2aa5dfbcfb485ea02a1b6294cf79dc",
            "placeholder": "​",
            "style": "IPY_MODEL_58a4d30727ea47bbb658b4bd988fd422",
            "value": " 14.5M/14.5M [00:00&lt;00:00, 44.2MB/s]"
          }
        },
        "b4f96e6970714f5fade3e38195ba8850": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44aaacda82a24f86927ee1715a483acd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eed0facf9f6c455f846c4968133f4a38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89f1a3dbff984860b5fc69cf530bbf72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "936844d785534f0cb45d58590f475fdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c2aa5dfbcfb485ea02a1b6294cf79dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58a4d30727ea47bbb658b4bd988fd422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04d622bea1714db6b26ee87293df6666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7fa0628d5e094fa6bf799992e78752a3",
              "IPY_MODEL_5b23cc84f2e74c70b6421165d543d039",
              "IPY_MODEL_ab377ecb3437456f9a55015bcff8e3ba"
            ],
            "layout": "IPY_MODEL_3f2fa46cdab34d2aac8119dff775d402"
          }
        },
        "7fa0628d5e094fa6bf799992e78752a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fd807499d1e4fba94d36a1559fcbc1a",
            "placeholder": "​",
            "style": "IPY_MODEL_7a4a26a6e90b4981a260a77147d9b617",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "5b23cc84f2e74c70b6421165d543d039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e26285538364ff38140f6589ee73898",
            "max": 85,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0dd09f3707814488ab843047c73036bd",
            "value": 85
          }
        },
        "ab377ecb3437456f9a55015bcff8e3ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_761c08e6b7a34b78a7ee0feddaf0c2c1",
            "placeholder": "​",
            "style": "IPY_MODEL_add0d6c167fb421b8692fc4c0ee76e76",
            "value": " 85.0/85.0 [00:00&lt;00:00, 7.04kB/s]"
          }
        },
        "3f2fa46cdab34d2aac8119dff775d402": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fd807499d1e4fba94d36a1559fcbc1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a4a26a6e90b4981a260a77147d9b617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e26285538364ff38140f6589ee73898": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dd09f3707814488ab843047c73036bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "761c08e6b7a34b78a7ee0feddaf0c2c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "add0d6c167fb421b8692fc4c0ee76e76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f444f44ab3db42d6b08b2f35139d7eef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_822ac4bc085d4b4c9b05ab3006953591",
              "IPY_MODEL_22c03e22e2024dc0bde0ef2373ec5afa",
              "IPY_MODEL_98c0318c359e4e3ea306bd848e407877"
            ],
            "layout": "IPY_MODEL_086c9d77aa95494a970bbc105b54c559"
          }
        },
        "822ac4bc085d4b4c9b05ab3006953591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42a5d0f5113a485c80b7cd2de1e03477",
            "placeholder": "​",
            "style": "IPY_MODEL_20da57a2415e445b840a0fba237267de",
            "value": "Downloading adapter_model.bin: 100%"
          }
        },
        "22c03e22e2024dc0bde0ef2373ec5afa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15a24b72f8b74e67a260d7274bd1306e",
            "max": 19683045,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f1447b26dc2487baf92e65189bcdc51",
            "value": 19683045
          }
        },
        "98c0318c359e4e3ea306bd848e407877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d91aed1317a4f38a8a7e938230ad309",
            "placeholder": "​",
            "style": "IPY_MODEL_ae0cdc76b12745eba1bf1d02dd261460",
            "value": " 19.7M/19.7M [00:00&lt;00:00, 28.2MB/s]"
          }
        },
        "086c9d77aa95494a970bbc105b54c559": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42a5d0f5113a485c80b7cd2de1e03477": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20da57a2415e445b840a0fba237267de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15a24b72f8b74e67a260d7274bd1306e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f1447b26dc2487baf92e65189bcdc51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d91aed1317a4f38a8a7e938230ad309": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae0cdc76b12745eba1bf1d02dd261460": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22e751c10f6245c4908a0bb49489c33c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19622a8f034444b1894bd7a97663af71",
              "IPY_MODEL_a7f9508ee09548f98d789f0877a870b8",
              "IPY_MODEL_e74bb15305454ff3b62b2269bc5d6937"
            ],
            "layout": "IPY_MODEL_961570c4b11e400395834d69dcac1100"
          }
        },
        "19622a8f034444b1894bd7a97663af71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b658900435a24a94af30ed0544088e20",
            "placeholder": "​",
            "style": "IPY_MODEL_187e13ab92c54a2280baa5a976592b54",
            "value": "Downloading readme: 100%"
          }
        },
        "a7f9508ee09548f98d789f0877a870b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a30c09439fe448d2945e7a30cbe7d4ff",
            "max": 538,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a419331acbf44577a14539569a6a5d71",
            "value": 538
          }
        },
        "e74bb15305454ff3b62b2269bc5d6937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8075bd9fdb8940d2b8a8ec1739a00668",
            "placeholder": "​",
            "style": "IPY_MODEL_4a7805867e674950b900fe269b407d95",
            "value": " 538/538 [00:00&lt;00:00, 40.7kB/s]"
          }
        },
        "961570c4b11e400395834d69dcac1100": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b658900435a24a94af30ed0544088e20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "187e13ab92c54a2280baa5a976592b54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a30c09439fe448d2945e7a30cbe7d4ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a419331acbf44577a14539569a6a5d71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8075bd9fdb8940d2b8a8ec1739a00668": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a7805867e674950b900fe269b407d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c0fb0bd0f814621b9f897f47c5a9110": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c53b5f5e72fb48ad9958c3b7a381792f",
              "IPY_MODEL_1f88e584ee5f45a5b1f2c729e821b2f0",
              "IPY_MODEL_ed2d7c45daf24795bc7d3f298ede1ca8"
            ],
            "layout": "IPY_MODEL_0d3d18fa2c7f4ef181454be2e352c491"
          }
        },
        "c53b5f5e72fb48ad9958c3b7a381792f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1810a82c420d42a1ae1f3fe14f46e5ab",
            "placeholder": "​",
            "style": "IPY_MODEL_1f945947684546f0ac55ab276500d271",
            "value": "Downloading data files: 100%"
          }
        },
        "1f88e584ee5f45a5b1f2c729e821b2f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e95c085ebc34ddc97a03918ce3f9e0c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c7881f13893343e9a23dde7ab606c84c",
            "value": 1
          }
        },
        "ed2d7c45daf24795bc7d3f298ede1ca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f21a1f857114dfc9454c985eae76acb",
            "placeholder": "​",
            "style": "IPY_MODEL_1942ddff53664095abfb974a3ba5fb6a",
            "value": " 1/1 [00:00&lt;00:00,  1.61it/s]"
          }
        },
        "0d3d18fa2c7f4ef181454be2e352c491": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1810a82c420d42a1ae1f3fe14f46e5ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f945947684546f0ac55ab276500d271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e95c085ebc34ddc97a03918ce3f9e0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7881f13893343e9a23dde7ab606c84c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f21a1f857114dfc9454c985eae76acb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1942ddff53664095abfb974a3ba5fb6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "856888efd147400493faff87ba11fb0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c467b6eb493b4859abcbb2888e5d1ca3",
              "IPY_MODEL_4a15fcac17c74efd81890ad0edd690c0",
              "IPY_MODEL_8f4b75111e474a65abbda2edecef32b3"
            ],
            "layout": "IPY_MODEL_4b3f7293c63e470a87c9775152913528"
          }
        },
        "c467b6eb493b4859abcbb2888e5d1ca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75e7fcfaad7e4da6af2b2ae2a171caf5",
            "placeholder": "​",
            "style": "IPY_MODEL_d9b3721e2bf04904ae0f1cbae1c85be2",
            "value": "Downloading data: 100%"
          }
        },
        "4a15fcac17c74efd81890ad0edd690c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c93a2cf6b95840df814fe89fa1e673a1",
            "max": 9340289,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_292e429e864e4b98bb0426021cd1fd35",
            "value": 9340289
          }
        },
        "8f4b75111e474a65abbda2edecef32b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85ac715d4c864ae7b1120979f77dbe89",
            "placeholder": "​",
            "style": "IPY_MODEL_cf696cd6d73d4a8188d7491ceeb41344",
            "value": " 9.34M/9.34M [00:00&lt;00:00, 7.13MB/s]"
          }
        },
        "4b3f7293c63e470a87c9775152913528": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75e7fcfaad7e4da6af2b2ae2a171caf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9b3721e2bf04904ae0f1cbae1c85be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c93a2cf6b95840df814fe89fa1e673a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "292e429e864e4b98bb0426021cd1fd35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85ac715d4c864ae7b1120979f77dbe89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf696cd6d73d4a8188d7491ceeb41344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86c6a0bb8a2c4cba9c46bf31a479af95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_965ea093e242456d9c38857465c678f9",
              "IPY_MODEL_0459e1dce7d84f91a5b6d383f4c373cb",
              "IPY_MODEL_932c5de3965040a58d812e02ef23ba50"
            ],
            "layout": "IPY_MODEL_6c3cfb25a02749cdbf8919e87a4a674d"
          }
        },
        "965ea093e242456d9c38857465c678f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83fc373097344ba995b7ac72e26e73aa",
            "placeholder": "​",
            "style": "IPY_MODEL_c85c57655fe045a9a4576c418189a979",
            "value": "Extracting data files: 100%"
          }
        },
        "0459e1dce7d84f91a5b6d383f4c373cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b79202647b3942efb069a38ea442f1e5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8de574176ba4ff0a54c4c6591f89294",
            "value": 1
          }
        },
        "932c5de3965040a58d812e02ef23ba50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b1c71a3804a44df847a16cd3a6787f7",
            "placeholder": "​",
            "style": "IPY_MODEL_83e7b109c9f04a7f8bc54acc84b624cc",
            "value": " 1/1 [00:00&lt;00:00, 49.87it/s]"
          }
        },
        "6c3cfb25a02749cdbf8919e87a4a674d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83fc373097344ba995b7ac72e26e73aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c85c57655fe045a9a4576c418189a979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b79202647b3942efb069a38ea442f1e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8de574176ba4ff0a54c4c6591f89294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b1c71a3804a44df847a16cd3a6787f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83e7b109c9f04a7f8bc54acc84b624cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b562489e63754a1fafc6cb06b67331b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_534b9422a93c453ea88ee5a1446e75e8",
              "IPY_MODEL_01bbf2201f7346739d86aa8b8b62c2bd",
              "IPY_MODEL_e529c7ea21314038a18383aff92a0002"
            ],
            "layout": "IPY_MODEL_62154756764e41f8b3de9da95072b2a6"
          }
        },
        "534b9422a93c453ea88ee5a1446e75e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2a724ea5c394170b900967bf2b1a295",
            "placeholder": "​",
            "style": "IPY_MODEL_a31cd0acbf164973bf4b155059b2d755",
            "value": "Generating train split: 100%"
          }
        },
        "01bbf2201f7346739d86aa8b8b62c2bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b7f9383c4c14a7a9f2d032c680709c7",
            "max": 2335,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88a61110680444348497134010611930",
            "value": 2335
          }
        },
        "e529c7ea21314038a18383aff92a0002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01007798e0ed44d199673aabd6ae6a6a",
            "placeholder": "​",
            "style": "IPY_MODEL_5122b16a5bb44726b0da6d1fe98a5b48",
            "value": " 2335/2335 [00:00&lt;00:00, 15776.49 examples/s]"
          }
        },
        "62154756764e41f8b3de9da95072b2a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2a724ea5c394170b900967bf2b1a295": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a31cd0acbf164973bf4b155059b2d755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b7f9383c4c14a7a9f2d032c680709c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88a61110680444348497134010611930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "01007798e0ed44d199673aabd6ae6a6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5122b16a5bb44726b0da6d1fe98a5b48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bskang8/CVPR2023_Project/blob/main/inference_cvpr_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyzcMObd8_9Q",
        "outputId": "2933c027-4f48-4037-bf68-7188821dfbcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "HUGGING_FACE_USER_NAME = \"bskang\"\n",
        "model_name = \"trained_cvpr2023_data_300\"\n",
        "\n",
        "peft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model_name}\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=False)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ],
      "metadata": {
        "id": "Yz7RQeE03v72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241,
          "referenced_widgets": [
            "509ad686592b45f183b14791556b6d77",
            "773c31ee9fd746e2942eb69140c2e6d4",
            "284bb3db3b49494dabe2b4237ee9ffcb",
            "54a397d25bcf41a88264dda1f9daf4ce",
            "d9e41bd2048746ceb7615bfac9ccc701",
            "837f752b8b0244638c1456f40c2cec41",
            "fefc70b23cda4516958cb9ebb0ea586b",
            "5aebb6eaa0aa43f7a06faf8d5395a4e4",
            "03b1ecc77e1e4351b6625fcc5f98a6a0",
            "3e360f594a984299bd467372fb0b21d4",
            "5323f74331e145b7a2cf7e24d9445a0e",
            "3fcf5d95f10b4d68a3853656b38bcfca",
            "961a1da8ce7b448186a7b3ad0e349bb7",
            "9c27ddd2d58849879d02a015be35a6b5",
            "d00c43a31bb646fd9458f3f726b9ab78",
            "90a291bdf51a455db543c974bb6f260f",
            "81cac506ceea41688f4ce6dedcdcbc78",
            "139ea49070a3410482853167cbea2e46",
            "ea5a3427d5d5468caf0f286937723391",
            "815c7a13e09b43119f732dc47349a213",
            "d92df0cc509e4119997e90f7f141da98",
            "2169eaf148804a2e8d3d17ec6a403f53",
            "bd0095ac0a1e40d0892cf3930d4ab93c",
            "ff101602632145b0ada7f869eccf5ca5",
            "816e17bf37c9413984074c73549e0855",
            "e6c7cb5fc55e46478349ec466f23cb6f",
            "d7e8887501d84754b84366d6c76e0430",
            "a520f5f4d81d4f78a983d8a2a2ab4fb6",
            "8de20f816a414827bc9d28e2f300deb1",
            "26d12fd5169f4444b8a2daa5c9657db0",
            "0f8cfdca0a284a80b85996ec3ab6432a",
            "8d4ac35b28f44f269e723421ce329d95",
            "db5ad5b3708245b7b90870fd749fec81",
            "9a22aaf6d729480288201777b02c829f",
            "1c8aeed7faab4ce98d75243468122c9d",
            "1a35e0b16b234b4ba3d9f2e83e4ca1ce",
            "af059bbe302745c1b77294914b38f4a3",
            "793c1073dbac486cac6cc33710497871",
            "7d8a4dddec4843d3a236fa72a472de3c",
            "813e47c48be74e2e9b3d0eb4abd62058",
            "05346e42bba543a080c3bfebe10870ac",
            "3ea6e51a257e4069a2ec75f77b4ef548",
            "4b704c5abec94084b75991884d8a8122",
            "25616d03f6f6476b853a95a66ac01387",
            "c9f6fd2904554305af47086c049826b4",
            "48c8fb3c51ac41cca608f581aac048c6",
            "9a6d38a49bb548b182cb4cc2ce2d438d",
            "4a6f9f5ed8ee426897eba10a8e7597b7",
            "b4f96e6970714f5fade3e38195ba8850",
            "44aaacda82a24f86927ee1715a483acd",
            "eed0facf9f6c455f846c4968133f4a38",
            "89f1a3dbff984860b5fc69cf530bbf72",
            "936844d785534f0cb45d58590f475fdd",
            "6c2aa5dfbcfb485ea02a1b6294cf79dc",
            "58a4d30727ea47bbb658b4bd988fd422",
            "04d622bea1714db6b26ee87293df6666",
            "7fa0628d5e094fa6bf799992e78752a3",
            "5b23cc84f2e74c70b6421165d543d039",
            "ab377ecb3437456f9a55015bcff8e3ba",
            "3f2fa46cdab34d2aac8119dff775d402",
            "7fd807499d1e4fba94d36a1559fcbc1a",
            "7a4a26a6e90b4981a260a77147d9b617",
            "4e26285538364ff38140f6589ee73898",
            "0dd09f3707814488ab843047c73036bd",
            "761c08e6b7a34b78a7ee0feddaf0c2c1",
            "add0d6c167fb421b8692fc4c0ee76e76",
            "f444f44ab3db42d6b08b2f35139d7eef",
            "822ac4bc085d4b4c9b05ab3006953591",
            "22c03e22e2024dc0bde0ef2373ec5afa",
            "98c0318c359e4e3ea306bd848e407877",
            "086c9d77aa95494a970bbc105b54c559",
            "42a5d0f5113a485c80b7cd2de1e03477",
            "20da57a2415e445b840a0fba237267de",
            "15a24b72f8b74e67a260d7274bd1306e",
            "2f1447b26dc2487baf92e65189bcdc51",
            "5d91aed1317a4f38a8a7e938230ad309",
            "ae0cdc76b12745eba1bf1d02dd261460"
          ]
        },
        "outputId": "82b8959b-b1bb-4570-dab8-84ffb4efbdb4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/adapter_config.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "509ad686592b45f183b14791556b6d77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fcf5d95f10b4d68a3853656b38bcfca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/6.01G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd0095ac0a1e40d0892cf3930d4ab93c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/199 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a22aaf6d729480288201777b02c829f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9f6fd2904554305af47086c049826b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04d622bea1714db6b26ee87293df6666"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading adapter_model.bin:   0%|          | 0.00/19.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f444f44ab3db42d6b08b2f35139d7eef"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "rAxkEfmVCYpr",
        "outputId": "a8006f5a-a6dc-4a3b-c74e-6e364292a249",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): BloomForCausalLM(\n",
            "      (transformer): BloomModel(\n",
            "        (word_embeddings): Embedding(250880, 2560)\n",
            "        (word_embeddings_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "        (h): ModuleList(\n",
            "          (0-29): 30 x BloomBlock(\n",
            "            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "            (self_attention): BloomAttention(\n",
            "              (query_key_value): Linear(\n",
            "                in_features=2560, out_features=7680, bias=True\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=7680, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
            "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): BloomMLP(\n",
            "              (dense_h_to_4h): Linear(in_features=2560, out_features=10240, bias=True)\n",
            "              (gelu_impl): BloomGelu()\n",
            "              (dense_4h_to_h): Linear(in_features=10240, out_features=2560, bias=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=2560, out_features=250880, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"bskang/CVPR2023_title_abstract_intro\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286,
          "referenced_widgets": [
            "22e751c10f6245c4908a0bb49489c33c",
            "19622a8f034444b1894bd7a97663af71",
            "a7f9508ee09548f98d789f0877a870b8",
            "e74bb15305454ff3b62b2269bc5d6937",
            "961570c4b11e400395834d69dcac1100",
            "b658900435a24a94af30ed0544088e20",
            "187e13ab92c54a2280baa5a976592b54",
            "a30c09439fe448d2945e7a30cbe7d4ff",
            "a419331acbf44577a14539569a6a5d71",
            "8075bd9fdb8940d2b8a8ec1739a00668",
            "4a7805867e674950b900fe269b407d95",
            "6c0fb0bd0f814621b9f897f47c5a9110",
            "c53b5f5e72fb48ad9958c3b7a381792f",
            "1f88e584ee5f45a5b1f2c729e821b2f0",
            "ed2d7c45daf24795bc7d3f298ede1ca8",
            "0d3d18fa2c7f4ef181454be2e352c491",
            "1810a82c420d42a1ae1f3fe14f46e5ab",
            "1f945947684546f0ac55ab276500d271",
            "7e95c085ebc34ddc97a03918ce3f9e0c",
            "c7881f13893343e9a23dde7ab606c84c",
            "2f21a1f857114dfc9454c985eae76acb",
            "1942ddff53664095abfb974a3ba5fb6a",
            "856888efd147400493faff87ba11fb0d",
            "c467b6eb493b4859abcbb2888e5d1ca3",
            "4a15fcac17c74efd81890ad0edd690c0",
            "8f4b75111e474a65abbda2edecef32b3",
            "4b3f7293c63e470a87c9775152913528",
            "75e7fcfaad7e4da6af2b2ae2a171caf5",
            "d9b3721e2bf04904ae0f1cbae1c85be2",
            "c93a2cf6b95840df814fe89fa1e673a1",
            "292e429e864e4b98bb0426021cd1fd35",
            "85ac715d4c864ae7b1120979f77dbe89",
            "cf696cd6d73d4a8188d7491ceeb41344",
            "86c6a0bb8a2c4cba9c46bf31a479af95",
            "965ea093e242456d9c38857465c678f9",
            "0459e1dce7d84f91a5b6d383f4c373cb",
            "932c5de3965040a58d812e02ef23ba50",
            "6c3cfb25a02749cdbf8919e87a4a674d",
            "83fc373097344ba995b7ac72e26e73aa",
            "c85c57655fe045a9a4576c418189a979",
            "b79202647b3942efb069a38ea442f1e5",
            "c8de574176ba4ff0a54c4c6591f89294",
            "3b1c71a3804a44df847a16cd3a6787f7",
            "83e7b109c9f04a7f8bc54acc84b624cc",
            "b562489e63754a1fafc6cb06b67331b7",
            "534b9422a93c453ea88ee5a1446e75e8",
            "01bbf2201f7346739d86aa8b8b62c2bd",
            "e529c7ea21314038a18383aff92a0002",
            "62154756764e41f8b3de9da95072b2a6",
            "d2a724ea5c394170b900967bf2b1a295",
            "a31cd0acbf164973bf4b155059b2d755",
            "3b7f9383c4c14a7a9f2d032c680709c7",
            "88a61110680444348497134010611930",
            "01007798e0ed44d199673aabd6ae6a6a",
            "5122b16a5bb44726b0da6d1fe98a5b48"
          ]
        },
        "id": "mDx8shhp91Xc",
        "outputId": "5595694b-b3df-4ffb-fe72-f993cad0c267"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/538 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22e751c10f6245c4908a0bb49489c33c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c0fb0bd0f814621b9f897f47c5a9110"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/9.34M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "856888efd147400493faff87ba11fb0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86c6a0bb8a2c4cba9c46bf31a479af95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/2335 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b562489e63754a1fafc6cb06b67331b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['title', 'abstract', 'introduction'],\n",
            "        num_rows: 2335\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def make_inference(title, abstract, tokens_len=1000):\n",
        "  batch = tokenizer(f\"### INSTRUCTION\\nBelow is a paper's title and abstract, please write a introduction for this paper.\\n\\n### Title:\\n{title}\\n### Abstract:\\n{abstract}\\n\\n### Introduction:\\n\", return_tensors='pt')\n",
        "\n",
        "  with torch.cuda.amp.autocast():\n",
        "    output_tokens = model.generate(**batch, max_new_tokens=tokens_len)\n",
        "\n",
        "  display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))"
      ],
      "metadata": {
        "id": "GfME_cTpSZhJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(dataset['train'][100]['introduction'])"
      ],
      "metadata": {
        "id": "yY8SKR_3aqn3",
        "outputId": "b5673533-a30e-4f74-a763-1a39ecf7f242",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "1.34×faster for DeiT-base on ImageNet-1k). The code is available at https : / / github . com / huawei -noah / Efficient -Computing / tree / master / TrainingAcceleration / NetworkExpansion and https : / / gitee . com / mindspore / hub / blob / master / mshub _ res / assets / noah -cvlab / gpu / 1.8/networkexpansion_v1.0_imagenet2012.md . 1. Introduction Deep neural networks have demonstrate their excellent performance on multiple vision tasks, such as classifica-⋆Corresponding authors.tion [15, 30, 44], object detection [12, 43], semantic seg-mentation [32, 35], etc.In spite of their success, these net-works usually come with heavy architectures and severe over-parameterization, and therefore it takes many days or even weeks to train such networks from scratch. The ever-increasing model complexity [23,24,34,42] and train-ing time cause not only a serious slowdown for the re-search schedule, but also a huge waste of time and com-puting resources. However, CNNs are still going deeper and bigger for higher capacity to cope with extremely large datasets [27, 45]. Recently, a new type of architecture named vision transformers (ViTs) have emerged and soon achieved state-of-the-art performances on multiple com-puter vision tasks [16, 48, 52, 57]. Originating from Natural Language Processing, the vision transformer has a different network topology and larger computational complexity than CNNs. Besides, transformer-based models usually require more epochs to converge. From another perspective, compared with purchasing ex-pensive GPU servers, many researchers and personal users nowadays choose cloud computing service to run experi-ments and pay their bills by GPU-hours. Thus, an acceler-ated training framework is obviously cost-efficient. On the other hand, shortened training time leads to not only quicker idea verification but also more refined hyper-parameter tun-ing, which is crucial to the punctual completion of the project and on-time product delivery. There are some existing methods about efficient model training [36, 51, 53, 55], but few of them can achieve high practical acceleration on geneal GPU platforms. [53] pro-poses to prune the gradients of feature maps during back-propagation to reduce train-time FLOPs, and achieve train-ing speedup on CPU platform. [51] conducts efficient CNN training on ARM and FPGA devices to reduce power con-sumption. [36] prunes weights of the network to achieve training acceleration but eventually yield a pruned sparse model with non-negligible performance drop. [55] skips easy samples that contribute little to loss reduction by using an assistant model asynchronously running on CPU. Yet it This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 20269 requires sophisticated engineering implementation. Though the prior works claim an ideal theoretical acceleration ra-tio, none of them achieve obviously practical acceleration on common GPU platforms. Most of these works overlook the most general scenario, i.e. accelerating training on gen-eral GPU platforms with popular deep learning frameworks such as PyTorch [40] and TensorFlow [1]. The lack of re-lated research is probably because GPU servers are not so power-constrained as edge devices. In this paper, we propose a general training accelera-tion framework (network expansion) for both CNN and ViT models to reduce the practical training time. We first sample a sub-network from the original dense model as the starting point of training. Then this sparse architecture will grad-ually expand its network topology by adding new parame-ters, which increases the model capacity along the training procedure. When performing network expansion, we fol-low the principle of avoiding the introduction of redundant parameters. For CNN, new filters are progressively added whose weights are initialized by imposing filter-level or-thogonality. This reduces the correlation between old and new feature maps and improves the expressiveness of the convolutional network. For vision transformers, we first train a shallow sub-network with fewer layers, and create an exponential moving average (EMA) version of the trained model. As the training continues, some layers of the EMA model will be inserted into the trained model to construct a deeper one. With the network expansion training paradigm, the sampled sub-network eventually grows into the desired dense architecture, and thus the total training FLOPs and time are greatly reduced. Our method can be easily integrated into popular deep learning frameworks on general GPU platforms. Without changing the original optimizer and hyper-parameters (such as epochs and learning rate), our method can achieve 1.42 × wall-time acceleration for training ResNet-101, 1.34 ×wall-time acceleration for training DeiT-base, on ImageNet-1k dataset with negligible top-1 accuracy gap, compared with normal training baseline. Moreover, experiments show that our acceleration framework can generalize to downstream tasks such as semantic segmentation. "
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title_here = dataset['train'][100]['title']\n",
        "abstract_here = dataset['train'][100]['abstract']\n",
        "\n",
        "make_inference(title_here, abstract_here)"
      ],
      "metadata": {
        "id": "GhZyCxdXX3Jc",
        "outputId": "2326b3e9-c48d-4098-83ff-78e5df5febf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### INSTRUCTION\nBelow is a paper's title and abstract, please write a introduction for this paper.\n\n### Title:\nDing_Network_Expansion_for_Practical_Training_Acceleration_CVPR_2023\n### Abstract:\nAbstract Recently, the sizes of deep neural networks and train-ing datasets both increase drastically to pursue better performance in a practical sense. With the prevalence of transformer-based models in vision tasks, even more pressure is laid on the GPU platforms to train these heavy models, which consumes a large amount of time and computing resources as well. Therefore, it’s crucial to accelerate the training process of deep neural networks. In this paper, we propose a general network expansion method to reduce the practical time cost of the model training process. Specifically, we utilize both width-and depth-level sparsity of dense models to accelerate the training of deep neural networks. Firstly, we pick a sparse sub-network from the original dense model by reducing the number of parameters as the starting point of training. Then the sparse architecture will gradually expand during the training procedure and finally grow into a dense one. We design different expanding strategies to grow CNNs and ViTs respectively, due to the great heterogeneity in between the two architectures. Our method can be easily integrated into popular deep learning frameworks, which saves considerable training time and hardware resources. Extensive experiments show that our acceleration method can significantly speed up the training process of modern vision models on general GPU devices with negligible performance drop ( e.g. 1.42×faster for ResNet-101 and \n\n### Introduction:\n1.34×faster for GANs). The code is available at https: //github.com/dpingzaixin/NEM . 1. Introduction Deep neural networks have achieved remarkable performances on various vision tasks with the help of modern large-scale datasets [37, 38]. However, the large models and long training times are not ideal for most users. Therefore, there is a need to accelerate the training process of deep neural networks. Generally, the acceleration methods can be divided into two categories: model-level and dataset-level acceleration. The model-level acceleration methods are based on reducing the model’s size ( e.g. pruning [26, 34], network expansion [22, 25, 29, 31, 35], partitioning [21, 31, 36], parallelization [25, 29, 35] and so on) or running the model on a different device or platform with different resources ( e.g. parallelization [25, 35], pruning [21, 36] and so on). The dataset-level acceleration methods are based on reducing the training time ( e.g. reducing the number of training samples [26] or the size of the training dataset [21, 22, 29, 31, 36]). Recently, the long-running models on cheap and commodity GPU devices like Intel Xeon E5-2700s and AMD Opteron 2400+ are under pressure to improve their practical performances as well as their competitiveness in the market. To this end, some methods have tried to accelerate the training process of heavy models on general GPU devices [21, 22, 29, 31, 36]. However, most of them are designed for CNN models and do not take into account the heterogeneity between different types of networks (like different layers of CNNs or different types of VTs etc.), which makes them difficult to apply to existing deep learning frameworks. In this paper, we propose a general network expansion method to reduce the practical time cost of the model training process. Our method can prune layers or parameters of the original model to make the sparse model grow into a dense one, and can also adapt to different architectures (like CNNs and ViTs). We utilize both the width-and depth-level sparsity of dense models to accelerate the training of deep neural networks. Firstly, we pick a sparse sub-network from the original dense model as the starting point of training. Then the sparse architecture will gradually expand during the training procedure and finally grow into a dense one. The expanding strategy for CNNs is straightforward, where we design different expanding strategies for layers with different sizes and learns to select some layers or parameters to prune based on a threshold value. However, the expanding strategy for VT is different due to the heterogeneity between different transformer models. Therefore, our method can prune layers or parameters of the original model to make the sparse model grow into a dense one, and can also adapt to different architectures (like CNNs and ViTs). We utilize both the width-and depth-level sparsity of dense models to accelerate the training of deep neural networks. Fig. 1 shows the general process to achieve our goal. In the first step, we train a sparse model on the original dense model and get a sparse model. Then the original dense model is our sparse model during the training process. In the second step, we evaluate our method on the sparse model to measure the performance gap between the sparse and dense models. If the performance gap is small ( e.g. <0.5% in FPS for ResNet-101 and <1.0% in FPS for GANs) then the sparse model is our final sparse model. After that, we can load the final sparse model into the original dense model to run the model training procedure on the original dense model. In the third step, we evaluate our method on the original dense model to measure the performance gap between the original dense and sparse models. If the performance gap is small ( e.g. <0.5% in FPS for ResNet-101 and <1.0% in FPS for GANs) then the original dense model is our final dense model. The process can be repeated until our method finishes. The main contributions of this paper can be summarized as follows: • We propose a general network expansion method to reduce the practical time cost of the model training process. Our method can prune layers or parameters of the original model to make the sparse model grow into a dense one, and can also adapt to different architectures (like CNNs and ViTs). • We utilize both the width-and depth-level sparsity of dense models to accelerate the training of deep neural networks. The sparse model will gradually expand during the training process and finally grow into a dense one. • Our method can significantly speed up the training process of modern vision models ( e.g. 1.42×faster for ResNet-101 and 1.34×faster for G"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title_here = dataset['train'][0]['title']\n",
        "abstract_here = dataset['train'][0]['abstract']\n",
        "\n",
        "make_inference(title_here, abstract_here)"
      ],
      "metadata": {
        "id": "r4hTqeg0R7b9",
        "outputId": "784abc46-6aea-4ca5-ddab-80d005bb4b2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1296: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1494: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### INSTRUCTION\nBelow is a paper's title and abstract, please write a introduction for this paper.\n\n### Title:\nDong_Fast_Monocular_Scene_Reconstruction_With_Global-Sparse_Local-Dense_Grids_CVPR_2023\n### Abstract:\nAbstract Indoor scene reconstruction from monocular images has long been sought after by augmented reality and robotics developers. Recent advances in neural field representa-tions and monocular priors have led to remarkable re-sults in scene-level surface reconstructions. The reliance on Multilayer Perceptrons (MLP), however, significantly limits speed in training and rendering. In this work, we propose to directly use signed distance function (SDF) in sparse voxel block grids for fast and accurate scene recon-struction without MLPs. Our globally sparse and locally dense data structure exploits surfaces’ spatial sparsity, en-ables cache-friendly queries, and allows direct extensions to multi-modal data such as color and semantic labels. To apply this representation to monocular scene reconstruc-tion, we develop a scale calibration algorithm for fast geo-metric initialization from monocular depth priors. We apply differentiable volume rendering from this initialization to refine details with fast convergence. We also introduce effi-cient high-dimensional Continuous Random Fields (CRFs) to further exploit the semantic-geometry consistency be-tween scene objects. Experiments show that our approach is 10× faster in training and 100× faster in rendering while achieving comparable accuracy to state-of-the-art neural implicit methods. \n\n### Introduction:\n1. Introduction Reconstructing indoor scenes from monocular in-door images is a fundamental problem in computer vision and graphics with applications in augmented reality (AR), robotics, and visual cognition. Recent neural field representations [3, 4, 21, 30] and monocular priors [8, 9, 11, 12, 16, 26] have led to significant progress in scene-level surface reconstruc-tion. With the success of neural radiance repre-sentation, the recent surge of monocular scene reconstruction has been driven by the demand for fast and accurate scene recon-struction in visual cognition. While the recent neural radiance models are fast and accurate, the reliance on Multilayer Perceptrons (MLPs) limits their applicability in practice. Monocular scene reconstruction from small labeled samples remains an open problem. The recent work Sparse NeRF [21] applies a MLP to sparse grid voxel blocks to recon-struct the scene without training from large labeled"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(dataset['train'][2000]['introduction'])"
      ],
      "metadata": {
        "id": "nlQ0tZabq1gg",
        "outputId": "dc250a80-5e11-4db7-f979-1276d35a6840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "1. Introduction\nGenerating realistic and editable 3D content is a long-\nstanding problem in computer vision and graphics that has\nrecently gained more attention due to the increased demand\nfor 3D objects in AR/VR, robotics and gaming applications.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n4466\nHowever, manual creation of 3D models is a laborious en-\ndeavor that requires technical skills from highly experi-\nenced artists and product designers. On the other hand, edit-\ning 3D shapes, typically involves re-purposing existing 3D\nmodels, by manually changing faces and vertices of a mesh\nand modifying its respective UV-map [95]. To accommo-\ndate this process, several recent works introduced genera-\ntive models that go beyond generation and allow editing the\ngenerated instances [13,18,52,55,62,77,101,116,117,124].\nShape editing involves making local changes on the shape\nand the appearance of different parts of an object. There-\nfore, having a basic understanding of the decomposition of\nthe object into parts facilitates controlling what to edit .\nWhile Generative Adversarial Networks (GANs) [30]\nhave emerged as a powerful tool for synthesizing photore-\nalistic images [7, 15, 16, 47–49], scaling them to 3D data\nis non-trivial as they ignore the physics of image forma-\ntion process.To address this, 3D-aware GANs incorporate\n3D representations such as voxel grids [38, 72, 75] or com-\nbine them with differentiable renderers [57, 126]. While\nthey faithfully recover the geometry and appearance, they\ndo not allow changing specific parts of the object.\nInspired by the rapid evolution of neural implicit ren-\ndering techniques [68], recent works [8, 32, 77, 96, 114]\nproposed to combine them with GANs in order to allow\nfor multi-view-consistent generations of high quality. De-\nspite their impressive performance on novel view synthe-\nsis, their editing capabilities are limited. To this end,\nediting operations in the latent space have been explored\n[21, 42, 62, 115, 124] but these approaches lack intuitive\ncontrol over the shape. By decomposing shapes into parts,\nother works facilitate structure-aware shape manipulations\n[40,70,88,111]. However, they require 3D supervision dur-\ning training and can only operate on textureless shapes.\nTo address these limitations, we devise PartNeRF, a\nnovel part-aware generative model, implemented as an auto-\ndecoder [5]. Our model enables part-level control, which fa-\ncilitates various editing operations on the shape and appear-\nance of the generated instance. These operations include\nrigid and non-rigid transformations on the object parts, part\nmixing from different objects, removing/adding parts and\nediting the appearance of specific parts of the object.\nOur key idea is to represent objects using a set of locally\ndefined Neural Radiance Fields (NeRFs) that are arranged\nsuch that the object can be plausibly rendered from a novel\nview. To enable part-level control, we enforce a hard as-\nsignment between parts and rays that ensures that altering\none part does not affect the shape and appearance of the\nothers. Our model does not require 3D supervision; we only\nassume supervision from images and object masks captured\nfrom known cameras. We evaluate PartNeRF on various\nShapeNet categories and demonstrate that it generates tex-\ntured shapes of higher fidelity than both part-based as wellas NeRF-based generative models. Furthermore, we show-\ncase several editing operations, not previously possible.\nIn summary, we make the following contributions :\nWe propose the first part-aware generative model that\nparametrizes parts as NeRFs and can generate editable 3D\nshapes. Unlike prior part-based approaches, our model\ndoes not require explicit 3D supervision and can gener-\nate textured shapes. Compared to NeRF-based generative\nmodels, our work is the first that reasons about parts and\nhence enables operations both on the shape and the tex-\nture of the generated object. Code and data is available at\nhttps://ktertikas.github.io/part nerf.\n"
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title_here = dataset['train'][2000]['title']\n",
        "abstract_here = dataset['train'][2000]['abstract']\n",
        "\n",
        "make_inference(title_here, abstract_here)"
      ],
      "metadata": {
        "id": "gcNwGSP5q3ae",
        "outputId": "b627bc8e-c5dd-4674-fa7c-beaddae9e40c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### INSTRUCTION\nBelow is a paper's title and abstract, please write a introduction for this paper.\n\n### Title:\nTertikas_Generating_Part-Aware_Editable_3D_Shapes_Without_3D_Supervision_CVPR_2023\n### Abstract:\nAbstract\nImpressive progress in generative models and implicit\nrepresentations gave rise to methods that can generate 3D\nshapes of high quality. However, being able to locally con-\ntrol and edit shapes is another essential property that can\nunlock several content creation applications. Local control\ncan be achieved with part-aware models, but existing meth-\nods require 3D supervision and cannot produce textures. In\nthis work, we devise PartNeRF , a novel part-aware gener-\native model for editable 3D shape synthesis that does not\nrequire any explicit 3D supervision. Our model generates\nobjects as a set of locally defined NeRFs, augmented with\nan affine transformation. This enables several editing op-\nerations such as applying transformations on parts, mixing\n*Work done during internship at Stanford.parts from different objects etc. To ensure distinct, manip-\nulable parts we enforce a hard assignment of rays to parts\nthat makes sure that the color of each ray is only determined\nby a single NeRF . As a result, altering one part does not af-\nfect the appearance of the others. Evaluations on various\nShapeNet categories demonstrate the ability of our model to\ngenerate editable 3D objects of improved fidelity, compared\nto previous part-based generative approaches that require\n3D supervision or models relying on NeRFs.\n\n\n### Introduction:\n1. Introduction The ability to generate 3D shapes with in-formative colors and textures has enabled several applications such as AR/VR/UI in-the-wild, movie and video game render-ings, and many others. While the\ngeneration of 3D shapes has been a topic of intense re-search for many years, the ability to edit and modify the appearance of 3D objects is an important aspect of in-terest for many applications. In recent years, the field of generative models and implicit representation has achieved impressive progress and produced models that can generate high-quality, realistic and diverse 3D scenes [ 11,13,22,45]. However, the ability to edit the 3D shape of an object is an essential property that can unlock several content cre-ation applications. Editing a 3D object can be achieved by placing it in a different scene or environment, changing its texture or color, or both. A common approach to achieve this is to train 3D models that generate high-fidelity scenes [ 11,22,45]. However, these models are trained for generalizable scenes and do not provide specific instructions on how to place objects in a different scene or modify them. Another approach to achieve this is to use photometric 3D models [ 13,28,44] to generate local 3D scenes, but the quality of the render-ings can be poor compared to methods that use supervised learning . To achieve better quality and more realistic renderings, several methods impose constraints on the photometric part-based model [ 17,18,31,48]. However, these methods train the model for photometrically consistent scenes and do not provide specific instructions on how to place objects or modify them. In this work, we devise a method that enables the generation of locally editable 3D shapes. Our method generates 3D shapes as a set of locally defined NeRFs (NeRFs are implicit representations for 3D objects), augmented with an affine tran-sformation for editing. To ensure that each NeRF obtains the same appearance as its neighbors, we impose a hard assignment of rays to parts that make sure that each ray is only deter-mined by a single NeRF. This ensures that each NeRF has a distinct appearance and can be edited locally. To train our method, we use ShapeNet [ 14], a dataset that contains a large set of 3D shapes in a photometrically consistent manner. However, due to the high dimensionality of ShapeNet, training the NeRF model requires several hours of computation and requires several days to train. To reduce the training time, we reduce the model’s dimensionality using prun-ing and train the model using only a small fraction of the data. We evaluate our method on several datasets and demonstrates that our method can generate locally editable 3D shap-ules of improved fidelity compared to previous part-based generative ap-proaches that require 3D supervision. To summarize, the main contributions of this work are as follows: • We propose PartNeRF , a novel method that generates locally editable 3D shapes without 3D supervi-sion. • We devise a method that uses NeRFs as implicit repre-sentations for 3D objects. • We demonstrate that the proposed method can im-prove the quality of the generated shapes compared to methods that use 3D supervision or use implicit representations from other type of models . 2. Related Work 2D Editable Rendering and Generation of Images and Maps With the advent of neural rendering and generative models, the field of generative models and im-plicit representation has seen significant advancements. The implicit representation methods such as rendering from density fields [ 11], density fields with field transformation [ 13], implicit surface models [ 2,28,31,32], implicit volume models [ 13,44], and implicit renderings from density fields with field transformation [ 44] have achieved impressive quality renderings. The implicit rendering methods have been extended to 2D generative models that can generate novel views of an object [ 9,22,45]. These methods learn a set of conditional densities to generate a new viewpoint. These conditional densities are learned using a set of training views and the difference in appearance of two given views. The implicit rendering methods and the related 2D generative models have been extended to 3D [ 24,46]. These methods learn a set of conditional densities to generate a novel viewpoint of an object. However, these methods cannot be used for local editing of 3D objects as they require 3D supervision and cannot generate 3D scenes. 3D Editable Rendering and Generation of Images and Maps Several methods have been proposed to generate 3D scenes that are photometrically consistent with real scenes [ 13,18,31,48]. These methods generate 3D scenes by learning conditional densities that are used to generate a novel viewpoint. However, these methods do not provide any method for local editing of the 3D objects. 3D Generative Adaption (3DGA) [ 27] and its extension 3DGA Refinement (3DGA Refinement"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title_here = dataset['train'][0]['title']\n",
        "abstract_here = dataset['train'][0]['abstract']\n",
        "\n",
        "make_inference(title_here, abstract_here)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "75wcDAhVXf_5",
        "outputId": "b71d08bc-167f-4be9-c62b-8a4268b42b53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### INSTRUCTION\nBelow is a paper's title and abstract, please write a introduction for this paper.\n\n### Title:\nDong_Fast_Monocular_Scene_Reconstruction_With_Global-Sparse_Local-Dense_Grids_CVPR_2023\n### Abstract:\nAbstract Indoor scene reconstruction from monocular images has long been sought after by augmented reality and robotics developers. Recent advances in neural field representa-tions and monocular priors have led to remarkable re-sults in scene-level surface reconstructions. The reliance on Multilayer Perceptrons (MLP), however, significantly limits speed in training and rendering. In this work, we propose to directly use signed distance function (SDF) in sparse voxel block grids for fast and accurate scene recon-struction without MLPs. Our globally sparse and locally dense data structure exploits surfaces’ spatial sparsity, en-ables cache-friendly queries, and allows direct extensions to multi-modal data such as color and semantic labels. To apply this representation to monocular scene reconstruc-tion, we develop a scale calibration algorithm for fast geo-metric initialization from monocular depth priors. We apply differentiable volume rendering from this initialization to refine details with fast convergence. We also introduce effi-cient high-dimensional Continuous Random Fields (CRFs) to further exploit the semantic-geometry consistency be-tween scene objects. Experiments show that our approach is 10× faster in training and 100× faster in rendering while achieving comparable accuracy to state-of-the-art neural implicit methods. \n\n### Introduction:\n1. Introduction Reconstructing indoor spaces into 3D representations is a key requirement for many real-world applications, includ-ing robot navigation, immersive virtual/augmented reality experiences, and architectural design. Recent neural field representations [3, 6, 23, 37, 40] offer fast and accurate reconstruc-tions of 3D scenes with limited geometric constraints. However, these methods rely on MLPs to represent complex surfaces with weights that are not directly trans-posable to 3D points. On the other hand, there is a growing interest in developing monocular scene reconstruc-tion methods using only monocular images [9, 25, 26, 29]. These methods can be applied to unconstrained environments since they do not require geometrically annotated meshes. However, the lack of geometric constraints leads to limited re-sults for complex objects and limited application to multi-modal data, including color and semantic labels. In this work, we seek a balance between speed and accuracy in monocular scene reconstruction. Our approach is based on the fast and accurate neural volume repre-sentation with global sparse local dense grids developed by [11]. We apply this to monocular scene reconstruction by applying differentiable volume rendering with initial geome-try initialized from depth aware monocular priors. In this work, we focus on monocular scene reconstruction with the following key requirements: • Fast: we want our method to be 10× faster in training and 100× faster in re-training and evaluation. • Accuracy: our method should be comparable to or even supe-rior to neural implicit methods based on geometric annotation. • Stability: our method should be robust to noise and occlusion. • Can be extended to multi- modal data. We present a fast monocular scene recon-struction approach based on our locally dense global sparse data structure. In particular, we apply differentiable volume rendering from initial monocular depth estimates without geometric annotation. To improve speed, we develop a scale con-stantization algorithm that can estimate the volume scale based on the monocular normals and apply rendering with optimized parameters. In summary, our work is as follows: • We propose a globally sparse, locally dense data struc-ture for fast and accurate monocular scene reconstruction. • We develop a scale calibration algorithm that initial-izes monocular depth estimates and applies optimized rendering with initial scale. • We apply differentiable volume rendering from monocular depth estimates for monocular scene recon-struction with good quality and stability. • We apply continuous neural implicit methods for scene understanding with good generalization ability since our representation is high-dimensional and continuous. ### Introduction:\n2. Related Work 3D Reconstruction from Monocular Images. Recent neural field representations [3, 6, 23, 37, 40] offer fast and accurate reconstructions of 3D scenes with limited geometric constraints. Global Monocular Reconstruction. Recent work focuses on monocular scene recon-struction with multi-view or RGB-D data. [25, 26] use multi-view data for semantic segmentation and 3D reconstruction. [29] uses multi-view data for semantic segmentation and object detection. [10] uses multi-view data for 3D scene understanding. [3, 11] use multi-view data for monocular scene reconstruction. [11, 19, 20] use multi-view data for monocular scene reconstruc-tion. 19] uses color and semantic labels. Our method is fast enough for real-world applications and is fast enough for training and evaluation. Semantic Monocular Reconstruction. The semantic information from monocular images enables better understanding of objects and environments. [25, 26, 29, 31, 32] use semantic labels for monocular scene reconstruc-tion. [10, 31, 32] use semantic labels for monocular object detec-tion. [10, 31] uses semantic labels for object segmentation. [19] uses semantic labels for object segmentation and segmentation. [3] uses semantic labels for geometric reconstruction. We use semantic labels for monocular scene reconstruc-tion. Semantic-based Monocular Scene Understanding. There is a strong demand for multi- modal data in the semantic and geometric understanding of indoor scenes. [22, 33] use color and semantic labels for geometric reconstruction. [2, 15] use semantic labels for object segmentation and segmentation. Our work is related to these works since we also use monocular images and semantic labels for monocular scene reconstruction. Reconstruction from Monocular Data. There are many monocular reconstruction ap-proaches. [26, 28] use depth aware monocular priors for geometric reconstruction. [20] use color and semantic labels for monocular scene understanding. [17] use semantic labels for monocular scene understanding. [18] use multi-view data for monocular scene reconstruction. Our work is fast enough for training and eval-uation since we apply differentiable volume rendering from initial monocular depth estimates. Semantic Monocular Reconstruction with Global-Sparse Local-Dense Data Structures. There are many global sparse local dense data structures proposed in the literature. [3, 11, 12, 14, 16, 18, 19, 22, 23, 25, 28, 30, 32] use voxel grids for geometric reconstruction. [25] uses voxel grids for monocular scene reconstruction. [20] uses voxel grids for object segmentation. [30] uses voxel grids for object segmentation and segmentation. [22] uses octree-based grids for geometric reconstruction. [14] uses grid-based data structures for geometric repre-sentation. [11] uses voxel blocks for monocular scene reconstruction. [12] uses grid-based data structures for semantic and semantic-geometry representations. [16] uses graph-based data structures for semantic and geometric repre-sentations. [14] uses graph-based data structures for semantic and geometric representations. 13] uses voxel blocks for monocular scene reconstruction. We use a different data structure from [11], namely, globally sparse local dense voxel grids. Our fast monocular scene reconstruction method also uses this structure. 3. Proposed Method 3.1. Fast Monocular Scene Reconstruction with Global-Sparse Local-Dense V oxel Blocks Globally Sparse. We use a globally sparse data struc-ture since it is more cache-friendly and enables fast queries. [20] uses a multi-layer perceptron (MLP)-based network for geometric repre-sentation and has fast cache-friendly queries. [25] uses a MLP-based network for monocular scene reconstruction and has fast cache-friendly queries. [18] uses a graph-based data structure for semantic and geometric repre-sentation. [14] uses a graph-based data structure for semantic repre-sentation. 13] uses voxel blocks for monocular scene reconstruction. [11] uses a MLP-based network for monocular scene reconstruction. We apply differentiable volume rendering from initial monocular depth estimates and use a globally sparse local dense voxel block grid for fast and accurate monocular scene reconstruction. Semantic Monocular Reconstruction with Global-Sparse Local-Dense V oxel Blocks. We apply differentiable volume rendering from monocular depth estimates and use a globally sparse local dense voxel block grid for monocular scene reconstruction with semantic and geometric repre-sentations. 3.2. Scale Calibration with Monocular Normal Estimates Monocular normals can be used to estimate the scale of a 3D object. [21,28] estimate the scale of a 3D object using a neural network that takes in input shape descriptors and monocular normals. [30] estimates the scale of a 3D object using a neural network that takes in input shape descriptors and camera poses. [22] estimate the scale of a 3D object using a neural network that takes in input shape descriptors and monocular normals. [15] estimates the scale of a 3D object using a neural network that takes in input shape descriptors and RGB images. [16] estimates the scale of a 3D object using a neural network that takes in input shape descriptors and RGB images. 3.3. Continual Uncertainty Rejection with High-Dimensional Continuous Random Fields High-dimensional continuous random fields (CRFs) represent continuous semantic and geometric information in an implicit form. CRFs have been used in 3D object recognition [33,34], semantic recog-nition [25] and semantic segmentation [2,15]. Uncertainty in a 3D object is represented by continuous random variables. We use a continuous CRF representation for semantic and geometric representations since they are high-dimensional and continuous. 3.4. Scale Calibration with Monocular Normal Estimates Monocular normals can be used to scale the initial geometric representation. [21] uses a neural network that takes in input shape descriptors and monocular normals to scale the initial geometric representation. [30] uses a MLP-based network that takes in input shape descriptors and monocular normals to scale the initial geometric representation. [22] uses a MLP-based network that takes in input shape descriptors and monocular normals to scale the initial geometric representation. [15] uses a graph-based representation that takes in input shape descriptors and monocular normals to scale the initial geometric representation. [16] uses a MLP-based network that takes in input shape descriptors and monocular normals to scale the initial geometric representation. 3.5. Fast Monocular Scene Reconstruction with Global-Sparse Local-Dense V oxel Blocks Our method is fast enough for real-world applications since we apply differentiable volume rendering from initial monocular depth estimates. We develop a scale calibration algorithm that initializes monocular depth estimates from monocular normals. 3.6. Fast Monocular Scene Understanding with High-Dimensional Continuous Random Fields High-dimensional continuous random fields (CRFs) represent continuous semantic and geometric information in an implicit form. CRFs have been used in 3D object recognition [33], semantic recog-nition [25] and semantic segmentation [2,15]. Uncertainty in a 3D object is represented by continuous random variables. We apply high-dimensional continuous CRFs to represent monocular scene understanding since they have good generalization ability. 3.7. Fast Monocular Scene Reconstruction with Global"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Different instruction style**"
      ],
      "metadata": {
        "id": "ltbQMYmTaI6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = tokenizer(f\"### INSTRUCTION\\nBelow are sum cvpr2023 paper's titles, please explain main technics in these papers.\\n\\n### Titles:\\nDong_Fast_Monocular_Scene_Reconstruction_With_Global-Sparse_Local-Dense_Grids_CVPR_2023andDashpute_Thermal_Spread_Functions_TSF_Physics-Guided_Material_Classification_CVPR_2023andJohari_ESLAM_Efficient_Dense_SLAM_System_Based_on_Hybrid_Representation_of_CVPR_2023andGan_CNVid-3.5M_Build_Filter_and_Pre-Train_the_Large-Scale_Public_Chinese_Video-Text_CVPR_2023andChen_iQuery_Instruments_As_Queries_for_Audio-Visual_Sound_Separation_CVPR_2023\\n### Explain:\\n\", return_tensors='pt')\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  output_tokens = model.generate(**batch, max_new_tokens=200)\n",
        "\n",
        "display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))"
      ],
      "metadata": {
        "id": "REtEhM4g2WQr",
        "outputId": "f36887b2-2a99-462b-a9e5-efab42672f52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1494: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### INSTRUCTION\nBelow are sum cvpr2023 paper's titles, please explain main technic in these papers.\n\n### Titles:\nDong_Fast_Monocular_Scene_Reconstruction_With_Global-Sparse_Local-Dense_Grids_CVPR_2023andDashpute_Thermal_Spread_Functions_TSF_Physics-Guided_Material_Classification_CVPR_2023andJohari_ESLAM_Efficient_Dense_SLAM_System_Based_on_Hybrid_Representation_of_CVPR_2023andGan_CNVid-3.5M_Build_Filter_and_Pre-Train_the_Large-Scale_Public_Chinese_Video-Text_CVPR_2023andChen_iQuery_Instruments_As_Queries_for_Audio-Visual_Sound_Separation_CVPR_2023\n### Explain:\n1.Introduction Recently, deep learning has been successfully applied to various vision tasks, such as object detection, segmentation, and 3D reconstruction. Among them, monocular scene reconstruction aims at reconstructing a complete scene from only RGB images. It is a challenging task due to the high complexity of the real world, which includes physical effects ( e.g., thermal expansion, atmospheric *Corresponding author. †Corresponding author. [Color]. Material Class: Aabb Material Effects: (a) Monocular Reconstruction. (b) Deep Learning.Material Effects: (c) Fast Monocular Reconstruction. (d) Thermal Spread Functions. (e) Object Detection. (f) Segmentation. The difference is that we leverage global sparse dense grids to reconstruct the scene. The effect is shown in Figure 5. atmospheric conditions), geometric changes (e.g., changes in temperature or lighting), and physical materials. It is also highly non-trivial to associate RGB images with 3D information ( e"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = tokenizer(f\"### INSTRUCTION\\n You are a model trained on 300 papers from CVPR 2023. Please introduce five notable techniques you learned from those papers. \\n### Introduce:\\n\", return_tensors='pt')\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  output_tokens = model.generate(**batch, max_new_tokens=550)\n",
        "\n",
        "display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))"
      ],
      "metadata": {
        "id": "xxWdxYypYqpz",
        "outputId": "9579a99c-ec3e-47f8-a08b-f322d296af4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### INSTRUCTION\n You are a model trained on 300 papers from CVPR 2023. Please introduce five notable techniques you learned from those papers. \n### Introduce:\n #### your work here\n### Title:\n#### Title:\n### Abstract:\n### Abstract:\n### Introduction:\n### Introduction:\n### Exposition:\n### Document:\n### Document:\n### Document:\n### Document:\n### Exposition:\n### Document:\n### Document:\n### Exposition:\n### Document:\n### Exposition:\n### Document:\n### Exposition:\n### Document:\n### Exposition:\n### Document:\n### Exposition:\n### Document:\n### Exposition:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Document:\n### Exposition:\n### Exposition:\n### Document:\n### Exposition:\n### Exposition:\n### Exposition:\n### Exposition:\n### Exposition:\n### Exposition:\n### Exposition:\n### Exposition:\n### Exposition:\n### Exposition:\n### Exposition:\n### Exposition:\n### Exposition:\n### Exposition:\n### Exposition:\n### Exposition:\n### Exposition:\n### Exposition:"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title_here = 'Request for Presentation on Key Technologies at CVPR 2023'\n",
        "abstract_here = 'We are in the latest developments in the field of computer vision and pattern recognition. As the computer vision landscape continues to rapidly evolve, novel techniques and algorithms are continuously advancing the boundaries of whats possible. In light of this, we are particularly interested in CVPR 2023 one of the most esteemed academic conferences in the field, which offers an excellent platform for presenting and sharing cutting-edge technologies and research findings. With the aim to gain insightful perspectives on the latest trends and research breakthroughs. Hence, we kindly request your valuable time to deliver a brief presentation on the key technologies showcased at CVPR2023. We look forward to learning about state-of-the-art advancements, novel methodologies, application insights, etc. Additionally, we are eager to explore the innovative approaches and technical nuances led by your institution/organization. Thank you for considering our request, and we anticipate an enlightening presentation from you.'\n",
        "\n",
        "make_inference(title_here, abstract_here)"
      ],
      "metadata": {
        "id": "Ufns4Vy7cOQ0",
        "outputId": "b3a10b2f-3298-4553-c40d-fd49a0cdce3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### INSTRUCTION\nBelow is a paper's title and abstract, please write a introduction for this paper.\n\n### Title:\nRequest for Presentation on Key Technologies at CVPR 2023\n### Abstract:\nWe are in the latest developments in the field of computer vision and pattern recognition. As the computer vision landscape continues to rapidly evolve, novel techniques and algorithms are continuously advancing the boundaries of whats possible. In light of this, we are particularly interested in CVPR 2023 one of the most esteemed academic conferences in the field, which offers an excellent platform for presenting and sharing cutting-edge technologies and research findings. With the aim to gain insightful perspectives on the latest trends and research breakthroughs. Hence, we kindly request your valuable time to deliver a brief presentation on the key technologies showcased at CVPR2023. We look forward to learning about state-of-the-art advancements, novel methodologies, application insights, etc. Additionally, we are eager to explore the innovative approaches and technical nuances led by your institution/organization. Thank you for considering our request, and we anticipate an enlightening presentation from you.\n\n### Introduction:\n1. Introduction Computer vision, also known as computer vis-ual science, is the study of providing solutions to complex problems in visual science and the human-machine interface. With the advancement in hardware and software technologies, computer vision has evolved into a mature field with a wide range of applications in visual recognition, image processing, 3D reconstruction, and autonomous driving etc. This work is licensed to the Open Data Access Foundation, if you are not already a subscriber, you can request access by sending a request to <kiev@odaf.ru> and selecting the option below. In this work, we focus on the area of computer vision and pattern recognition, which is a crucial component of modern-day technology. Pattern re-gistry is a crucial component of many machine learning ap-proaches and is essential for developing intelligent systems. It consists of identifying and classifying patterns that occur repeatedly across different images or instances. These patterns are represented as points in space or as a sparse matrix with re-petition. Pattern recognition has been a field of interest for many years and has grown exponentially over the last decade as a result of the advancement in technology and the emergence of deep learning. Today, deep learning is the de-facto standard for many computer vision applications. In this work, we present novel techniques that significantly improve the performance of state-of-the-art pattern recognition models, particularly in terms of accuracy. We focus on three key topics, each of which has shown promising results in recent years. The first focuses on point-based representations, the second on sparse matrix representations, and the third on network-based approaches. Our key contributions are summarized below. • Point-Based Representations: We present four novel point-based representations, including sparse point representations, dense point representations, and two composite representations. We show that point-based repre-sentations are sensitive to orientation, so we design two methods to improve the robustness of sparse point representation. • Sparse Matrix Representations: We propose four novel sparse matrix representations, including matrix factorization-based methods, matrix-median-based methods, and two composite representations. We show that sparse matrix representations are sensitive to noise, so we design two methods to improve the robustness of sparse matrix representation. • Network-Based Representations: We propose two network-based approaches, one is a transformer-based approach, and the other is a self-supervised approach. We show that transformer-based approaches produce better results than the ones that use the constant learning rate, while self-supervised approaches produce better results than the ones that use the regular learning rate. We also conduct extensive experiments to compare the performance of these four approaches on CIFAR-100 and CIFAR-10/100 datasets, and the results show that these four approaches significantly outperform the state-of-the-art methods.  • Point and Sparse Matrix Representations. • Network-based Representations. • Methods: We present four point-based repre-sentations, two sparse matrix representations, and two network-based approaches. 2. Related Work 2.1. Computer Vision and Pattern Recognition This section introduces the background of computer vision and pattern recognition, including computer vision techniques, pattern recognition techniques, and their applications. 2.1.1. Computer Vision Techniques The field of computer vision and pattern recognition has grown exponentially over the years. The growth is driven by the advan-tage of new technologies, such as deep learning and the emergence of gen-erative algorithms. These technologies enable us to understand and simulate human vision, allowing us to build intelligent systems that can interact with us. In particular, the field of computer vision has seen a boom in the past few years as new technologies, such as the development of the ar-chive device, the chipset, and the operating system, continue to advance. We can divide the field of computer vision into three categories, namely image processing, recognition, and autonomous driving. Image processing focuses on device processing of images, such as edge detection, image inpainting, image super-resolution, and so on. Recognition focuses on understanding the meaning of the images that we see, such as classi-fying objects in images, such as object detection, object recognition, and so on. Autonomous driving focuses on developing a set of techniques to control the autonomous system based on images we see, such as sensing, mapping, and so on. 2.1.2. Pattern Recognition Pattern recognition is the study of finding patterns in data. Patterns are represented either as a set of features that describe the pattern, or as a set of features and their associations. The data may be images, sounds, texts, or any other form of data. The data is searched for by a pattern recognition model, which can be a machine learning or a statistical model. The model can be uni or multi-class. Pattern recognition models can be used in many different areas, such as text recognition [53], audio recognition [46], image recognition [31, 34, 41], and so on. 2"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title_here = 'Request for Information on Key Techniques and Titles of Papers Utilizing CVPR 2023 Published Research'\n",
        "abstract_here = 'We kindly request information from experts regarding the key techniques presented in the research paper unveiled at CVPR 2023. Specifically, we are interested in understanding the innovative methodologies proposed in the paper, such as transformer-based and self-supervised approaches, that are expected to enhance various computer vision tasks. Moreover, we seek details on any other research papers that have utilized or built upon the aforementioned key techniques to achieve significant advancements in the field of computer vision. These papers titles and related information would be invaluable in gauging the widespread impact and applicability of the novel methodologies introduced in the CVPR 2023 paper. Our intent is to gain a comprehensive understanding of the state-of-the-art approaches in computer vision research, enabling us to identify potential areas of collaboration and further exploration. We sincerely appreciate any assistance from experts in sharing insights and references to relevant research work.'\n",
        "\n",
        "make_inference(title_here, abstract_here)"
      ],
      "metadata": {
        "id": "MiaMncg_qoTz",
        "outputId": "adf3a8aa-5e81-4fb9-9688-0123fa17dd75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### INSTRUCTION\nBelow is a paper's title and abstract, please write a introduction for this paper.\n\n### Title:\nRequest for Information on Key Techniques and Titles of Papers Utilizing CVPR 2023 Published Research\n### Abstract:\nWe kindly request information from experts regarding the key techniques presented in the research paper unveiled at CVPR 2023. Specifically, we are interested in understanding the innovative methodologies proposed in the paper, such as transformer-based and self-supervised approaches, that are expected to enhance various computer vision tasks. Moreover, we seek details on any other research papers that have utilized or built upon the aforementioned key techniques to achieve significant advancements in the field of computer vision. These papers titles and related information would be invaluable in gauging the widespread impact and applicability of the novel methodologies introduced in the CVPR 2023 paper. Our intent is to gain a comprehensive understanding of the state-of-the-art approaches in computer vision research, enabling us to identify potential areas of collaboration and further exploration. We sincerely appreciate any assistance from experts in sharing insights and references to relevant research work.\n\n### Introduction:\n1. Introduction Vision transformers [12] are a well-known learning technique used in a variety of computer vision tasks, including image classification and object detection. These networks consist of sequential convolutional layers with self-attention mechanisms, and were originally proposed to learn features that are invariant to image cro-tions and rotations. However, recent studies showed that they are able to learn highly effective representations that are able to generalize to various tasks and sizes [14, 21, 24, 28]. These benefits are attributed to the fact that transformer architectures are able to leverage large scale data by aggregating and aggregating features from multiple layers. In addition, there has been a recent trend in using transformer architectures for self-supervised learning. For example, [28, 34] utilized transformer architectures for learning features that are computable on GPUs, while [4, 21] learned features that can be computed on a CPU or GPUs using a self-optimized algorithm. Both of these studies showed that using a transformer-based architecture and self-supervised learning can achieve state-of-the-art performance. However, there has not yet been a study that demonstrates the benefits of using a transformer-based or self-supervised architecture and a large dataset on performance in computer vision tasks. In this paper, we propose a study that demonstrates the benefits of using a transformer-based and large-scale dataset on computer vision tasks. Specifically, we propose a series of experiments using a transformer-based architecture with self-supervised learning for image classification and object detection. For image classification, we conduct experiments on CIFAR-10C, CIFAR-100C, SVHN, and ImageNet, using the standard CIFAR-10C dataset with and without a self-supervision step. For object detection, we use the ResNet-50 architecture with and without a self-supervision step. Our results show that using a transformer-based architecture and a large self-supervised dataset leads to significant performance gains for a variety of computer vision tasks. This code is available at https://github.com/hkaalab/ request . \n\n### Introduction:\n1. Introduction Vision transformers are a well-known learning technique used in a variety of computer vision tasks, including image classification and object detection. These networks consist of sequential convolutional layers with self-attention mechanisms, and were originally proposed to learn features that are invariant to image cro-tions and rotations [23, 25]. However, recent studies showed that they are able to learn highly effective representations that are able to generalize to various tasks and sizes [14, 21, 24, 28]. These benefits are attributed to the fact that transformer architectures are able to leverage large scale data by aggregating and aggregating features from multiple layers. In addition, there has been a recent trend in using transformer architectures for self-supervised learning. For example, [28, 34] utilized transformer architectures for learning features that are computable on GPUs, while [4, 21] learned features that can be computed on a CPU or GPUs using a self-optimized algorithm. These studies showed that using a transformer-based architecture and a large self-supervised dataset leads to significant performance gains for a variety of computer vision tasks. In this paper, we propose a study that demonstrates the benefits of using a transformer-based and large-scale dataset on computer vision tasks. Specifically, we propose a series of experiments using a transformer-based architecture with self-supervised learning for image classification and object detection. For image classification, we conduct experiments on CIFAR-10C, CIFAR-100C, SVHN, and ImageNet, using the standard CIFAR-10C dataset with and without a self-supervision step. For object detection, we use the ResNet-50 architecture with and without a self-supervision step. Our results show that using a transformer-based architecture and a large self-supervised dataset leads to significant performance gains for a variety of computer vision tasks. This code is available at https://github.com/hkaalab/request . In addition, we provide the related papers, which can serve as a benchmark for the research community to benchmark their own work on the proposed task. To summarize, the contributions of this paper are: • We propose a study that demonstrates the benefits of using a transformer-based and large-scale dataset on computer vision tasks. • We provide the code for the experiments with the related papers. This paper is available at https://github.com/hkaalab/request . \n\n### Introduction:\n2. Related Work This section introduces the related work, including recent studies on using transformer architectures for computer vision tasks and the use of large-scale datasets for learning with self-supervision. Related Work on Using Transformers for Computer Vision. Transformers have been used in computer vision for a long time, for example, in image classification and object detection. For image classification, the earliest work was done by Vapnik [53] who introduced the use of a transformer architecture for feature extraction in his famous paper on statistical learning theory. His approach was later extended to include self-supervision and achieved significant"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Foundation Model Test**"
      ],
      "metadata": {
        "id": "hfz_EbEvz2Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "model_bloomz = AutoModelForCausalLM.from_pretrained(\n",
        "    \"bigscience/bloomz-3b\",\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_8bit=True,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "tokenizer_bloomz = AutoTokenizer.from_pretrained(\"bigscience/bloomz-3b\")"
      ],
      "metadata": {
        "id": "cI_EGvLZw7rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_bloomz)"
      ],
      "metadata": {
        "id": "F4dgxiKr0A2s",
        "outputId": "054e3c45-c617-449c-8643-4addd5d53eb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BloomForCausalLM(\n",
            "  (transformer): BloomModel(\n",
            "    (word_embeddings): Embedding(250880, 2560)\n",
            "    (word_embeddings_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "    (h): ModuleList(\n",
            "      (0-29): 30 x BloomBlock(\n",
            "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attention): BloomAttention(\n",
            "          (query_key_value): Linear8bitLt(in_features=2560, out_features=7680, bias=True)\n",
            "          (dense): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
            "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): BloomMLP(\n",
            "          (dense_h_to_4h): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n",
            "          (gelu_impl): BloomGelu()\n",
            "          (dense_4h_to_h): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2560, out_features=250880, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_inference_bloomz(title, abstract, tokens_len=1000):\n",
        "  batch = tokenizer(f\"### INSTRUCTION\\nBelow is a paper's title and abstract, please write a introduction for this paper.\\n\\n### Title:\\n{title}\\n### Abstract:\\n{abstract}\\n\\n### Introduction:\\n\", return_tensors='pt')\n",
        "\n",
        "  with torch.cuda.amp.autocast():\n",
        "    output_tokens = model_bloomz.generate(**batch, max_new_tokens=tokens_len)\n",
        "\n",
        "  display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))"
      ],
      "metadata": {
        "id": "2SDVz6pN0QXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_here = 'Request for Information on Key Techniques and Titles of Papers Utilizing CVPR 2023 Published Research'\n",
        "abstract_here = 'We kindly request information from experts regarding the key techniques presented in the research paper unveiled at CVPR 2023. Specifically, we are interested in understanding the innovative methodologies proposed in the paper, such as transformer-based and self-supervised approaches, that are expected to enhance various computer vision tasks. Moreover, we seek details on any other research papers that have utilized or built upon the aforementioned key techniques to achieve significant advancements in the field of computer vision. These papers titles and related information would be invaluable in gauging the widespread impact and applicability of the novel methodologies introduced in the CVPR 2023 paper. Our intent is to gain a comprehensive understanding of the state-of-the-art approaches in computer vision research, enabling us to identify potential areas of collaboration and further exploration. We sincerely appreciate any assistance from experts in sharing insights and references to relevant research work.'\n",
        "\n",
        "make_inference_bloomz(title_here, abstract_here)"
      ],
      "metadata": {
        "id": "s5ko5P-e0uFk",
        "outputId": "4605d103-ef92-4837-b2ff-b0a805cc5c05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1494: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### INSTRUCTION\nBelow is a paper's title and abstract, please write a introduction for this paper.\n\n### Title:\nRequest for Information on Key Techniques and Titles of Papers Utilizing CVPR 2023 Published Research\n### Abstract:\nWe kindly request information from experts regarding the key techniques presented in the research paper unveiled at CVPR 2023. Specifically, we are interested in understanding the innovative methodologies proposed in the paper, such as transformer-based and self-supervised approaches, that are expected to enhance various computer vision tasks. Moreover, we seek details on any other research papers that have utilized or built upon the aforementioned key techniques to achieve significant advancements in the field of computer vision. These papers titles and related information would be invaluable in gauging the widespread impact and applicability of the novel methodologies introduced in the CVPR 2023 paper. Our intent is to gain a comprehensive understanding of the state-of-the-art approaches in computer vision research, enabling us to identify potential areas of collaboration and further exploration. We sincerely appreciate any assistance from experts in sharing insights and references to relevant research work.\n\n### Introduction:\nCVPR 2023 is a conference on computer vision and pattern recognition. The conference is held annually in June in San Francisco, California. The conference is organized by the IEEE Computer Society."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title_here = 'Request for Presentation on Key Technologies at CVPR 2023'\n",
        "abstract_here = 'We are in the latest developments in the field of computer vision and pattern recognition. As the computer vision landscape continues to rapidly evolve, novel techniques and algorithms are continuously advancing the boundaries of whats possible. In light of this, we are particularly interested in CVPR 2023 one of the most esteemed academic conferences in the field, which offers an excellent platform for presenting and sharing cutting-edge technologies and research findings. With the aim to gain insightful perspectives on the latest trends and research breakthroughs. Hence, we kindly request your valuable time to deliver a brief presentation on the key technologies showcased at CVPR2023. We look forward to learning about state-of-the-art advancements, novel methodologies, application insights, etc. Additionally, we are eager to explore the innovative approaches and technical nuances led by your institution/organization. Thank you for considering our request, and we anticipate an enlightening presentation from you.'\n",
        "\n",
        "make_inference_bloomz(title_here, abstract_here)"
      ],
      "metadata": {
        "id": "1WJCQ5Xs0xq2",
        "outputId": "1b4938b1-8977-4e0b-c82a-2c291747e9a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### INSTRUCTION\nBelow is a paper's title and abstract, please write a introduction for this paper.\n\n### Title:\nRequest for Presentation on Key Technologies at CVPR 2023\n### Abstract:\nWe are in the latest developments in the field of computer vision and pattern recognition. As the computer vision landscape continues to rapidly evolve, novel techniques and algorithms are continuously advancing the boundaries of whats possible. In light of this, we are particularly interested in CVPR 2023 one of the most esteemed academic conferences in the field, which offers an excellent platform for presenting and sharing cutting-edge technologies and research findings. With the aim to gain insightful perspectives on the latest trends and research breakthroughs. Hence, we kindly request your valuable time to deliver a brief presentation on the key technologies showcased at CVPR2023. We look forward to learning about state-of-the-art advancements, novel methodologies, application insights, etc. Additionally, we are eager to explore the innovative approaches and technical nuances led by your institution/organization. Thank you for considering our request, and we anticipate an enlightening presentation from you.\n\n### Introduction:\nCVPR 2023 is a prestigious academic conference in the field of computer vision and pattern recognition. It is one of the most esteemed conferences in the field. It offers an excellent platform for presenting and sharing cutting-edge technologies and research findings."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title_here = 'Request for Information on Key Techniques and Titles of Papers Utilizing CVPR 2023 Published Research'\n",
        "abstract_here = 'We kindly request information from experts regarding the key techniques presented in the research paper unveiled at CVPR 2023. Specifically, we are interested in understanding the innovative methodologies proposed in the paper, such as transformer-based large data learning methods and their applications, advancements in generative models, and 3D image generation utilizing nerf (Neural Radiance Fields). These methodologies are expected to significantly enhance various computer vision tasks. Furthermore, we seek detailed information and titles of other research papers that have utilized or built upon these key techniques. Knowing the titles of such papers would be crucial in gauging the widespread impact and applicability of the innovative methodologies introduced in the CVPR 2023 paper. Our intent is to gain a comprehensive understanding of the state-of-the-art approaches in computer vision research, enabling us to identify potential areas of collaboration and further exploration. We sincerely appreciate any assistance from experts in sharing insights and references to relevant research work.'\n",
        "\n",
        "make_inference(title_here, abstract_here)"
      ],
      "metadata": {
        "id": "XVYGOslt31jC",
        "outputId": "d4d75ec3-183e-4257-b90a-86bda2d4cd71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### INSTRUCTION\nBelow is a paper's title and abstract, please write a introduction for this paper.\n\n### Title:\nRequest for Information on Key Techniques and Titles of Papers Utilizing CVPR 2023 Published Research\n### Abstract:\nWe kindly request information from experts regarding the key techniques presented in the research paper unveiled at CVPR 2023. Specifically, we are interested in understanding the innovative methodologies proposed in the paper, such as transformer-based large data learning methods and their applications, advancements in generative models, and 3D image generation utilizing nerf (Neural Radiance Fields). These methodologies are expected to significantly enhance various computer vision tasks. Furthermore, we seek detailed information and titles of other research papers that have utilized or built upon these key techniques. Knowing the titles of such papers would be crucial in gauging the widespread impact and applicability of the innovative methodologies introduced in the CVPR 2023 paper. Our intent is to gain a comprehensive understanding of the state-of-the-art approaches in computer vision research, enabling us to identify potential areas of collaboration and further exploration. We sincerely appreciate any assistance from experts in sharing insights and references to relevant research work.\n\n### Introduction:\n1. Introduction Vision-based computer vision systems are capable of performing fundamental tasks, such as detection, segmentation, and tracking. However, these tasks are often deficient in low-level visual features, especially in terms of spatial and semantic details. To address this issue, a number of researchers have endeavored to utilize deep learning techniques with large datasets to learn more discriminative visual representations. This has led to the development of powerful and versatile computer vision systems, which are now capable of performing various tasks in a single system, for instance, image detection, segmentation, and motion analysis at the same time. Despite the large advances in computer vision, there are still numerous tasks that remain underexplored. Specifically, we are interested in understanding the innovative methodologies proposed in the recent paper titled, “Large Data Large Models: A Transformer-based Large Data Learning Perspective ,” published in the International Journal of Computer Applications (IJC), on March 3, 2023. This paper unveils the use of a transformer-based large data learning method to explore large image collections in various computer vision tasks, such as image detection, segmentation, and tracking, as well as 3D image generation. This is expected to significantly enhance the performance of these computer vision tasks. However, there are currently no published research papers that detail the innovative methods used in these computer vision tasks. Therefore, we begin by establishing a database of these methods, followed by an introduction to the computer vision tasks that these methods are expected to improve the performance of. We will continue by soliciting feedback and suggestions from computer vision experts regarding the key techniques that are innovative and beneficial to the task at hand. Specifically, we are interested in understanding the innovative methodologies proposed in the paper, such as transformer-based large data learning methods and their applications, advancements in generative models, and 3D image generation, as well as detailed information and titles of other published research papers that utilize or build upon these key techniques. We begin by establishing a database of innovative techniques, followed by a brief introduction to the computer vision tasks they are expected to improve in terms of performance. We will continue by soliciting feedback and suggestions from computer vision experts regarding the key techniques that are innovative and beneficial to the task. Figure 1illustrates the proposed framework. Here, we first establish a database of innovative techniques for computer vision tasks. Then, we introduce the task of 3D image generation, which is one of the remaining tasks that are yet to be explored. Finally, we present the proposed use of large data learning to explore and exploit these innovative techniques in these computer vision tasks. We aim to stimulate further collaboration and research between researchers on these tasks and facilitate the adoption of these innovative technologies in downstream applications. Our project is hosted at http://vision.cs.uni-freiburg.de/detect/. \n| 1536 Figure 1. The proposed framework. Here, we first establish a database of innovative techniques for computer vision tasks. Then, we introduce the task of 3D image generation, which is one of the remaining tasks that are yet to be explored. Finally, we present the proposed use of large data learning to explore and exploit these innovative techniques in these computer vision tasks. We expect these key techniques to significantly enhance the performance of these computer vision tasks. Figure 2illustrates the proposed framework. Here, we first present an example image that is captured by a motion-pose-based camera. Then, we use image features from the past to guide the current image generation. This is one of the tasks that are expected to be improved by the proposed large data large model paradigm. We will continue by soliciting feedback and suggestions from computer vision experts regarding the key techniques that are innovative and beneficial to this task. Figure 3illustrates the proposed framework. Here, we first present an image that is captured by a motion-pose-based camera. Then, we use image features from the past to guide the current image generation. This is one of the tasks that are expected to be improved by the proposed large data large model paradigm. We will continue by soliciting feedback and suggestions from computer vision experts regarding the key techniques that are innovative and beneficial to this task. Figure 4illustrates the proposed framework. Here, we first present an image that is captured by a motion-pose-based camera. Then, we use image features from the past to guide the current image generation. This is one of the tasks that are expected to be improved by the proposed large data large model paradigm. We will continue by soliciting feedback and suggestions from computer vision experts regarding the key techniques that are innovative and beneficial to this task. Table 1shows the details of the tasks that are addressed by the proposed framework. For each task, we first summarize the relevant research papers, followed by a brief description of the computer vision tasks that are addressed by the proposed method. We then list the references of these papers. Finally, we introduce the key techniques used in these tasks. The tasks are as follows: 1) Image Detection:"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H4fe4Jf0Nm5q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}