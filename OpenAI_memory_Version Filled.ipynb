{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fXsYHTgvnCM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e28771-b0a2-492a-fab2-bb8e1990b275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install openai langchain -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZl9HMTr-8xO",
        "outputId": "e4100f09-33cf-4993-eae8-11c4cc8bef9f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
        "cvpr_path = \"./drive/MyDrive/cvpr_papers/text/part1\"\n",
        "dir_loader = DirectoryLoader(cvpr_path, loader_cls=TextLoader)\n",
        "cvpr_data = dir_loader.load()"
      ],
      "metadata": {
        "id": "iKw7UIKI5D9I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "documents = text_splitter.split_documents(cvpr_data)"
      ],
      "metadata": {
        "id": "5VkoHS_e5dMh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D3iDCsQakJx",
        "outputId": "273d3d1b-e2a2-4c6e-edec-4b227099437e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Minimizing the Accumulated Trajectory Error to Improve Dataset Distillation\\nJiawei Du1,5,2†,Yidi Jiang2†,Vincent Y. F. Tan3,2,Joey Tianyi Zhou1,5*,Haizhou Li4,2\\n1Centre for Frontier AI Research (CFAR), Agency for Science, Technology and Research (A*STAR), Singapore\\n2Department of Electrical and Computer Engineering, National University of Singapore\\n3Department of Mathematics, National University of Singapore\\n4SRIBD, School of Data Science, The Chinese University of Hong Kong, Shenzhen, China\\n5Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR), Singapore\\n{dujiawei,yidi jiang }@u.nus.edu, vtan@nus.edu.sg, Joey.tianyi.zhou@gmail.com\\nAbstract\\nModel-based deep learning has achieved astounding suc-\\ncesses due in part to the availability of large-scale real-world\\ndata. However, processing such massive amounts of data\\ncomes at a considerable cost in terms of computations, stor-\\nage, training and the search for good neural architectures.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Dataset distillation has thus recently come to the fore. This\\nparadigm involves distilling information from large real-\\nworld datasets into tiny and compact synthetic datasets such\\nthat processing the latter ideally yields similar performances\\nas the former. State-of-the-art methods primarily rely on\\nlearning the synthetic dataset by matching the gradients ob-\\ntained during training between the real and synthetic data.\\nHowever, these gradient-matching methods suffer from the\\nso-called accumulated trajectory error caused by the discrep-\\nancy between the distillation and subsequent evaluation. To\\nmitigate the adverse impact of this accumulated trajectory\\nerror, we propose a novel approach that encourages the op-\\ntimization algorithm to seek a flat trajectory. We show that\\nthe weights trained on synthetic data are robust against the\\naccumulated errors perturbations with the regularization\\ntowards the flat trajectory. Our method, called Flat Trajec-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tory Distillation (FTD) , is shown to boost the performance\\nof gradient-matching methods by up to 4.7% on a subset\\nof images of the ImageNet dataset with higher resolution\\nimages. We also validate the effectiveness and generalizabil-\\nity of our method with datasets of different resolutions and\\ndemonstrate its applicability to neural architecture search.\\nCode is available at .https://github.com/AngusDujw/FTD-\\ndistillation.\\n1. Introduction\\nModern deep learning has achieved astounding successes\\nin achieving ever better performances in a wide range of\\n*Corresponding Author.†Equal Contribution.\\n0 10 20 30 40 50\\nEpoch0.00.10.20.30.40.50.6 LTest(f)LTest(f*)\\nConvNet on CIFAR 100, IPC=10\\nMTT in Distillation\\nMTT in Evaluation\\nOurs in Evaluation\\n2.53.03.54.04.5\\n LTest(f)\\nFigure 1. The change of the loss difference LTTest(fθ)−LTTest(fθ∗),\\nin which θandθ∗denote the weights optimized by synthetic dataset S\\nand real dataset T, respectively. The gray line represents LTTest(fθ∗)and', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='is associated with the gray y-axis of the plot with two y-axes. The lines\\nindicated by “Evaluation” represent the networks that are initialized at epoch\\n0and trained with the synthetic dataset for 50epochs. The line indicated by\\n“Distillation” represents the network that is initialized at epochs 2,4, . . . , 48\\nand trained with the synthetic dataset for 2epochs. The former lines have\\nmuch higher loss difference compared to the latter; this is caused by the\\naccumulated trajectory error. And we try to minimize it in the evaluation\\nphase, so that the loss difference line of our method is lower and tends to\\nconverge than that of MTT [1].\\nfields by exploiting large-scale real-world data and well-\\nconstructed Deep Neural Networks (DNN) [4,5,9]. Unfortu-\\nnately, these achievements have come at a prohibitively high\\ncost in terms of computation, particularly when it relates to\\nthe tasks of data storage, network training, hyperparameter\\ntuning, and architectural search.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='A series of model distillation studies [2, 14, 16, 22] has\\nthus been proposed to condense the scale of models by dis-\\ntilling the knowledge from a large-scale teacher model into\\na compact student one. Recently, a similar but distinct task,\\ndataset distillation [1, 3, 26, 35, 36, 42 –45, 47, 49, 50] has\\nbeen considered to condense the size of real-world datasets.\\nThis task aims to synthesize a large-scale real-world dataset\\ninto a tiny synthetic one, such that a model trained with the\\nsynthetic dataset is comparable to the one trained with the\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n3749\\nreal dataset. Dataset distillation can expedite model training\\nand reduce cost. It plays an important role in some machine\\nlearning tasks such as continual learning [39, 48 –50], neural', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='architecture search [19,37,38,48,50], and privacy-preserving\\ntasks [8, 13, 27], etc.\\nWang et al. [45] was the first to formally study dataset\\ndistillation. The authors proposed a method DD that models\\na regular optimizer as the function that treats the synthetic\\ndataset as the inputs, and uses an additional optimizer to\\nupdate the synthetic dataset pixel-by-pixel. Although the\\nperformance of DD degrades significantly compared to train-\\ning on the real dataset, [45] revealed a promising solution for\\ncondensing datasets. In contrast to conventional methods,\\nthey introduced an evaluation standard for synthetic datasets\\nthat uses the learned distilled set to train randomly initialized\\nneural networks and the authors evaluate their performance\\non the real test set. Following that, Such et al. [41] employed\\na generative model to generate the synthetic dataset. Nguyen\\net al. [34] reformulated the inner regular optimization of\\nDD into a kernel-ridge regression problem, which admits', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='closed-form solution.\\nIn particular, Zhao and Bilen [50] pioneered a gradient-\\nmatching approach DC, which learns the synthetic dataset\\nby minimizing the distance between two segments of gra-\\ndients calculated from the real dataset T, and the synthetic\\ndataset S. Instead of learning a synthetic dataset through a\\nbi-level optimization as DD does, DC [50] optimizes the syn-\\nthetic dataset explicitly and yields much better performance\\ncompared to DD. Along the lines of DC [50], more gradient-\\nmatching methods have been proposed to further enhance\\nDC from the perspectives of data augmentation [48], feature\\nalignment [44], and long-range trajectory matching [1].\\nHowever, these follow-up studies on gradient-matching\\nmethods fail to address a serious weakness that results from\\nthe discrepancy between training and testing phases. In\\nthe training phase, the trajectory of the weights generated\\nbySis optimized to reproduce the trajectory of Twhich', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='commenced from a set of weights that were progressively\\nupdated by T. However, in the testing phase, the weights\\nare no longer initialized by the weights with respect to T,\\nbut the weights that are continually updated by Sin previous\\niterations. The discrepancy of the starting points of the train-\\ning and testing phases results in an error on the converged\\nweights. Such inaccuracies will accumulate and have an ad-\\nverse impact on the starting weight for subsequent iterations.\\nAs demonstrated in Figure 1, we observe the loss difference\\nbetween the weights updated by SandT. We refer to the\\nerror as the accumulated trajectory error , because it grows\\nas the optimization algorithm progresses along its iterations.\\nThe synthetic dataset Soptimized by the gradient-\\nmatching methods is able to generalize to various starting\\nweights, but is not sufficiently robust to mitigate the per-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='turbation caused by the accumulated trajectory error of thestarting weights. To minimize this source of error, the most\\nstraightforward approach is to employ robust learning, which\\nadds perturbations to the starting weights intentionally dur-\\ning training to make Srobust to errors. However, such a\\nrobust learning procedure will increase the amount of infor-\\nmation of the real dataset to be distilled. Given a fixed size\\nofS, the distillation from the increased information results\\nin convergence issues and will degrade the final performance.\\nWe demonstrate this via empirical studies in subsection 3.2.\\nIn this paper, we propose a novel approach to minimize\\nthe accumulated trajectory error that results in improved\\nperformance. Specifically, we regularize the training on the\\nreal dataset to a flat trajectory that is robust to the pertur-\\nbation of the weights. Without increasing the information\\nto be distilled in the real dataset, the synthetic dataset will', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='enhance its robustness to the accumulated trajectory error at\\nno cost. Thanks to the improved tolerance to the perturbation\\nof the starting weights, the synthetic dataset is also able to\\nameliorate the accumulation of inaccuracies and improves\\nthe generalization during the testing phase. It can also be ap-\\nplied to cross-architecture scenarios. Our proposed method\\nis compatible with the gradient-matching methods and boost\\ntheir performances. Extensive experiments demonstrate that\\nour solution minimizes the accumulated error and outper-\\nforms the vanilla trajectory matching method on various\\ndatasets, including CIFAR-10, CIFAR-100, subsets of the\\nTinyImageNet, and ImageNet. For example, we achieve per-\\nformance accuracies of 43.2% with only 10images per class\\nand50.7%with50images per class on CIFAR-100, com-\\npared to the previous state-of-the-art work from [1] (which\\nyields accuracies of only 40.1% and 47.7% respectively). In\\nparticular, we significantly improve the performance on a', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='subset of the ImageNet dataset which contains higher resolu-\\ntion images by more than 4%.\\n2. Preliminaries and Related Work\\nProblem Statement. We start by briefly overviewing the\\nproblem statement of Dataset Distillation. We are given a\\nreal dataset T={(xi, yi)}|T |\\ni=1, where the examples xi∈Rd\\nand the class labels yi∈ Y={0,1, . . . , C −1}andCis the\\nnumber of classes. Dataset Distillation refers to the problem\\nof synthesizing a new dataset Swhose size is much smaller\\nthan that of T(i.e., it contains much fewer pairs of synthetic\\nexamples and their class labels), such that a model ftrained\\non the synthetic dataset Sis able to achieve a comparable\\nperformance over the real data distribution PDas the model\\nftrained with the original dataset T.\\nWe denote the synthetic dataset Sas{(si, yi)}|S|\\ni=1where\\nsi∈Rdandyi∈ Y. Each class of Scontains ipc (images\\nper class) examples. In this case, |S|=ipc×Cand\\n|S| ≪ |T | . We denote the optimized weight parameters', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='obtained by minimizing an empirical loss term over the\\n3750\\nsynthetic training set Sas\\nθS= arg min\\nθX\\n(si,yi)∈Sℓ(fθ, si, yi),\\nwhere ℓcan be an arbitrary loss function which is taken to be\\nthe cross entropy loss in this paper. Dataset Distillation aims\\nat synthesizing a synthetic dataset Sto be an approximate\\nsolution of the following optimization problem\\nSDD= arg min\\nS⊂Rd×Y,|S|=ipc×CLTTest(fθS). (1)\\nWang et al. [45] proposed DD to solve Sby optimizing\\nEquation 1 after replacing TTestwithT, i.e., minimizing\\nLT(fθS)directly because TTestis inaccessible.\\nGradient-Matching Methods. Unfortunately, DD’s [45]\\nperformance is poor because optimizing Equation 1 only\\nprovides limited information for distilling the real dataset T\\ninto the synthetic dataset S. This motivated Zhao et al. [50]\\nto propose a so-called gradient-matching method DC to\\nmatch the informative gradients calculated by TandSat\\neach iteration to enhance the overall performance. Namely,\\nthey considered solving\\nSDC= arg min', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='SDC= arg min\\nS⊂Rd×Y\\n|S|=ipc×CE\\nθ0∼Pθ0\\x14MX\\nm=1L(S)\\x15\\n,where (2)\\nL(S) =D\\x00\\n∇θmLS(fθm),∇θmLT(fθm)\\x01\\n. (3)\\nIn the definition of L(S),θmcontains the weights updated\\nfrom the initialization θ0withTat iteration m. The ini-\\ntial set of weights θ0is randomly sampled from an initial-\\nization distribution PθandMin Equation 2 is the total\\nnumber of update steps. Finally, D(·,·)in Equation 3 de-\\nnotes a (cosine similarity-based) distance function measur-\\ning the discrepancy between two matrices and is defined as\\nD(X, Y) =PI\\ni=1\\x10\\n1−⟨Xi,Yi⟩\\n∥Xi∥∥Yi∥\\x11\\n,where X, Y∈RI×J\\nandXi, Yi∈RJare the ithcolumns of XandYrespectively.\\nAt each distillation (training) iteration, DC [50] minimizes\\ntheL(S)as defined in Equation 3. The Gradient-Matching\\nMethod regularizes the distillation of Sby matching the gra-\\ndients of single-step (DC [50]), or multiple-step (MTT [1])\\nfor improved performance. More related works can be found\\nin Appendix B.\\n3. Methodology\\nThe gradient-matching methods as discussed in section 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='constitute a reliable and state-of-the-art approach for dataset\\ndistillation. These methods match a short range of gradi-\\nents with respect to a sets of the weights trained with the\\nreal dataset in the distillation (training) phase. However,\\nthe gradients calculated in the evaluation (testing) phase\\n(a) Bufferθ*\\n0, 0θ*\\n0, mθ*\\n1, mθ*\\nM − 1, m\\n(b) Distillationθ*\\nt, m\\n̂\\nθ\\nt, 0= θ*\\nt, 0̂\\nθ\\nt, n\\nδ\\n̂\\nθ\\n0, 0\\n(c) E aluationε\\ntε\\nt \\ue235\\nt+1δε\\nt+ 1T eacher T rajectory\\nStudent T rajectoryFigure 2. Illustration of trajectory matching: (a) A teacher trajectory is\\nobtained by recording the intermediate network parameters at every epoch\\ntrained on the real dataset Tin the buffer phase. (b) The synthetic dataset S\\nis optimized to match the segments of the student trajectory with the teacher\\ntrajectory in the distillation phase. (c) The entire student trajectory and the\\naccumulated trajectory error ϵtin the evaluation phase is shown. We aim to\\nminimize this accumulated trajectory error.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='are with respect to the recurrent weights from previous it-\\nerations, instead of the exact weights from the teacher’s\\ntrajectory. Unfortunately, this discrepancy between the dis-\\ntillation (training) and evaluation (testing) phases result in\\na so-called accumulated trajectory error . We take MTT [1]\\nas an instance of a gradient-matching method to explain the\\nexistence of such an error in subsection 3.2. We then propose\\na novel and effective method to mitigate the accumulated\\ntrajectory error in subsection 3.3.\\n3.1. Matching Training Trajectories (MTT)\\nIn contrast to DC [50], MTT [1] matches the accumu-\\nlated gradients over several steps (i.e., over a segment of the\\ntrajectory of updated weights), to further improve the overall\\nperformance. Therefore, MTT [1] solves for Sas follows\\nSMTT = arg min\\nS⊂Rd×Y,|S|=ipc×CE\\nθ0∼Pθ0[∆A],where (4)\\n∆A=', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='A[∇θLS(fθ0), n]− A[∇θLT(fθ0), m]', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='2\\n2.(5)\\nIn Equation 5, the algorithm A, which is the first-order opti-\\nmizer sans momentum used in MTT, outputs the difference\\nof the parameter vectors at the nthiteration and at initializa-\\ntion, i.e.,\\nA[∇θLS(fθ0), n] =θn−θ0.\\nWe model Aas a function with input being the gradient\\n∇θLS(fθ0), which is run over a number of iterations n, and\\nwhose output is the accumulated change of weights after n\\niterations. Note that n, m are set so that n < m because\\n|S| ≪ |T | . Equation 4 particularizes to Equation 2 when\\nn=m= 1.\\n3751\\nIntuitively, MTT [1] learns an informative synthetic\\ndataset Sso that it can provide sufficiently reliable informa-\\ntion to the optimizer A. Then, Autilizes the information\\nfromSto map the weights θ0sampled from its (initializa-\\ntion) distribution Pθ0into an approximately-optimal param-\\neter space W={θ|LTTest(fθ)≤Ltol}, where Ltol>0\\ndenotes an “tolerable minimum value”.\\nIn the actual implementation, the ground truth trajecto-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ries, also known as the teacher trajectories , are prerecorded\\nin the buffer phase as (θ∗\\n0,0, . . . , θ∗\\n0,m, θ∗\\n1,0, . . . , θ∗\\nM−1,m).\\nAs illustrated in Figure 2(a), the teacher trajectories are\\ntrained until convergence on the real dataset Twith a ran-\\ndom initialization θ∗\\n0,0. The long teacher trajectories are then\\npartitioned into Msegments {Θ∗\\nt}M−1\\nt=0and each segment\\nΘ∗\\nt= (θ∗\\nt,0, θ∗\\nt,1, . . . , θ∗\\nt,m). Note that θ∗\\nt,0=θ∗\\nt−1,msince\\nthe last set of weights of the segment will be used to initialize\\nthe first set of weights of the next one.\\nAs shown in Figure 2(b), in the distillation phase, a\\nsegment of the weights Θ∗\\ntis randomly sampled from\\n{Θ∗\\nt}M−1\\nt=0 and used to initialize the student trajectory\\n(ˆθt,0,ˆθt,1, . . . , ˆθt,n)which satisfies ˆθt,0=θ∗\\nt,0. In summary,\\nθ∗\\nt,m=θ∗\\nt,0+A[∇θLT(fθ∗\\nt,0), m],and\\nˆθt,n=ˆθt,0+A[∇θLS(fˆθt,0), n].\\nSubsequently, MTT [1] solves Equation 4 by minimizing,\\nat each distillation iteration, the following loss over S:\\nL(S) =∥ˆθt,n−θ∗', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='L(S) =∥ˆθt,n−θ∗\\nt,m∥2\\n2\\n∥θ∗\\nt,0−θ∗\\nt,m∥2\\n2\\n=∥θ∗\\nt,0+A[∇θLS(fθ∗\\nt,0), n]−θ∗\\nt,m∥2\\n2\\n∥θ∗\\nt,0−θ∗\\nt,m∥2\\n2\\n=∥A[∇θLS(fθ∗\\nt,0), n]− A[∇ΘLT(fθ∗\\nt,0), m]∥2\\n2\\n∥θ∗\\nt,0−θ∗\\nt,m∥2\\n2.\\nThe synthetic dataset Sis obtained by minimizing L(S)\\nto be informative to guide the optimizer to update weights\\ninitialized at θ∗\\nt,0to eventually reach the target weights θ∗\\nt,m.\\n3.2. Accumulated Trajectory Error\\nThe student trajectory, to be matched in the distillation\\nphase, is only one segment from ˆθt,0toˆθt,ninitialized from\\na precise θ∗\\nt,0from the teacher trajectory, i.e., ˆθt,0=θ∗\\nt,0. In\\nthe distillation phase, the matching error is defined as\\nδt=A[∇θLS(fθ∗\\nt−1,m), n]− A[∇θLT(fθ∗\\nt−1,m), m].(6)\\nandδtcan be minimized in the distillation phase. However,\\nin the actual evaluation phase, the optimization procedure of\\nstudent trajectory is extended, and each segment is no longer\\ninitialized from the teacher trajectory but rather the last setof weights in the previous segments, i.e., ˆθt,0=ˆθt−1,n. This', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='discrepancy will result in a so-called accumulated trajectory\\nerror , which is the difference between the weights from the\\nteacher and student trajectory in tthsegment, i.e.,\\nϵt=ˆθt+1,0−θ∗\\nt+1,0=ˆθt,n−θ∗\\nt,m\\nThe initialization discrepancy between the distillation phase\\nand the evaluation phase will incur an initialization error\\nIt=I(θ∗\\nt,0, ϵt), representing the difference in accumulated\\ngradients. It can be represented mathematically as:\\nIt=A[∇θLS(fθ∗\\nt,0+ϵt), n]− A[∇θLS(fθ∗\\nt,0), n],(7)\\nIn the next segment, ϵt+1can be derived as follows\\nϵt+1=ˆθt+2,0−θ∗\\nt+2,0=ˆθt+1,n−θ∗\\nt+1,m\\n= (ˆθt,n+A[∇θLS(fˆθt,n), n])\\n−(θ∗\\nt,m+A[∇θLT(fθ∗\\nt,m), m])\\n= (A[∇θLS(fˆθt,n), n]− A[∇θLT(fθ∗\\nt,m), m])\\n+ (ˆθt,n−θ∗\\nt,m)\\n= (A[∇θLS(fθ∗\\nt,m+ϵt), n]− A[∇θLS(fθ∗\\nt,m), n])\\n+ (A[∇θLS(fθ∗\\nt,m), n]− A[∇θLT(fθ∗\\nt,m), m] +ϵt)\\n=ϵt+I(θ∗\\nt,m, ϵt) +δt+1. (8)\\nThe accumulated trajectory error ϵt+1continues to accumu-\\nlate the initialization error I(θ∗\\nt,m, ϵt), the matching error ϵt,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and the ϵtin previous segment. It also impacts the accumula-\\ntion of errors in subsequent segments, and thereby degrading\\nthe final performance. This is illustrated in Figure 2(c). We\\nconduct experiments to verify the existence of the accumu-\\nlated trajectory error, which are demonstrated in Figure 1,\\nmore exploring about accumulated trajectory error can be\\nfound in Appendix A.1.\\n3.3. Flat Trajectory helps reduce the accumulated\\ntrajectory error\\nFrom Equation 8, we seek to minimize ∆ϵt+1=ϵt+1−\\nϵt=It+δt+1where δt+1is the matching error of gradient-\\nmatching methods, which has been optimized to a small\\nvalue in the distillation phase. However, the initialization\\nerrorItis not optimized in the distillation phase. The ex-\\nistence of Itresults from the gap between the distillation\\nand evaluation phases. To minimize it, a straightforward\\napproach is to design the synthetic dataset Swhich is robust\\nto the perturbation ϵin the distillation phase. This is done', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='by adding random noise to initialize the weights, i.e.,\\nS= arg min\\nS⊂Rd×Y,\\n|S|=ipc×CE\\nθ0∼Pθ0,\\nϵ∼N(0,σ2I)[L(S, θ0, ϵ)],where\\nL(S, θ0, ϵ)=', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='A[LS(fθ0+ϵ), n]−A[LT(fθ0), m]', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='2\\n2,(9)\\n3752\\nandN(0, σ2I)is a Gaussian with mean 0and covariance\\nσ2I. However, we find that solving Equation 9 results in a\\ndegradation of the final performance when the number of\\nimages per class of Sis not large (e.g., ipc∈ {1,10}). It\\nonly can improve the final performance when ipc = 50 .\\nThese experimental results are reported in Table 1 and la-\\nbelled as “Robust Learning”. A plausible explanation is\\nthat adding random noise to the initialized weights θ0+ϵ\\nin the distillation phase is equivalent to mapping a more\\ndispersed (spread out) distribution Pθ0+ϵinto the parame-\\nter space W={θ|LTTest(fθ)≤Ltol}, which necessitates\\nmore information per class (i.e., larger ipc) from Sin or-\\nder to ensure convergence, hence degrading the distilling\\neffectiveness when ipc∈ {1,10}is relatively small.\\nWe thus propose an alternative approach to regularize\\nthe teacher trajectory to a Flat Trajectory for Distillation\\n(FTD). Our goal is to distill a synthetic dataset whose stan-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='dard training trajectory is flat; in other words, it is robust\\nto the weight perturbations with the guidance of the teacher\\ntrajectory. Without exceeding the capacity of information\\nper class ( ipc), FTD improves the buffer phase to make the\\nteacher trajectory robust to weight perturbation. As such,\\nthe flat teacher trajectory will guide the distillation gradi-\\nent update to synthesize a dataset with the flat trajectory\\ncharacteristic in a standard optimization procedure.\\nWe aim to minimize Itto ameliorate the adverse effect\\ncaused by ϵt. Assuming that ∥ϵt∥2\\n2is small, we can first\\nrewrite the accumulated trajectory error Equation 8 using a\\nfirst-order Taylor series approximation as It=I(θ∗\\nt, ϵt) =\\n∂A\\n∂ϵt, ϵt\\x0b\\n+O(∥ϵt∥2)1(where 1is the all-ones vector). To\\nsolve for θ∗\\ntthat approximately minimizes the ℓ2norm of\\nI(θ∗\\nt, ϵt)in the buffer phase, we note that\\nθ∗\\nt=arg min\\nθt∥I(θ∗\\nt, ϵt)∥2\\n2≈arg min\\nθt', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='∂A\\n∂ϵt\\n\\n\\n\\n2\\n2\\n=arg min\\nθt\\n\\n\\n\\n∂A\\n∂∇θLS(fθ∗\\nt)·∂∇θLS(fθ∗\\nt)\\n∂θ·∂θ\\n∂ϵt\\n\\n\\n\\n2\\n2.(10)\\nSinceAis the first-order optimizer sans momentum, which\\nhas been modeled as a function as discussed after Equation 4.\\nTherefore,∂A\\n∂∇θLS(fθ∗\\nt)=η, where ηis the learning rate used\\ninA. Because θ=θ∗\\nt+ϵ, we have∂θ\\n∂ϵ= 1. Substituting\\nthese derivatives into Equation 10, we obtain\\narg min\\nθt∥I(θ∗\\nt, ϵt)∥2\\n2≈arg min\\nθt\\n\\n\\n∂∇θLS(fθ∗\\nt)\\n∂θ\\n\\n\\n2\\n2\\n= arg min\\nθt\\n\\n∇2\\nθLS(fθ∗\\nt)', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='2\\n2.(11)\\nMinimizing ∥∇2\\nθLS(fθ∗\\nt)∥2\\n2is obviously equivalent to min-\\nimizing the largest eigenvalue of the Hessian ∇2\\nθLS(fθ∗\\nt).\\nUnfortunately, the computation of the largest eigenvalue is\\nexpensive. Fortunately, the largest eigenvalue of ∇2\\nθLS(fθ∗\\nt)has also be regarded as the sharpness of the loss land-\\nscape, which has been well-studied by many works such\\nas SAM [11] and GSAM [51]. In our work, we employ\\nGSAM to help solve Equation 11 to find a teacher trajec-\\ntory that is as flat as possible. The sharpness S(θ), can be\\nquantified using\\nS(θ)≜max\\nϵ∈Ψ\\x02\\nLT(fθ+ϵ)−LT(fθ)\\x03\\n(12)\\nwhere Ψ ={ϵ:∥ϵ∥2≤ρ}andρ >0is a given constant that\\ndetermines the permissible norm of ϵ. Then, θ∗is obtained\\nin the buffer phase by solving a minimax problem as follows,\\nθ∗= arg min\\nθ\\x08\\nLT(fθ) +α S(θ)\\t\\n, (13)\\nwhere αis the coefficient that balances the robustness of θ∗\\nto the perturbation. From the above derivation, we see that\\na different teacher trajectory is proposed. This trajectory', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='is robust to the perturbation of the weights in the buffer\\nphase so as to reduce the accumulated trajectory error in the\\nevaluation phase. The details about our algorithm and the\\noptimization of θ∗can be found in Appendix A.3.2.\\n4. Experiments\\nIn this section, we verify the effectiveness of FTD through\\nextensive experiments. We conduct experiments to com-\\npare FTD to state-of-the-art baseline methods evaluated on\\ndatasets with different resolutions. We emphasize the cross-\\narchitecture performance and generalization capabilities of\\nthe generated synthetic datasets. We also conduct exten-\\nsive ablation studies to exhibit the enhanced performance\\nand study the influence of hyperparameters. Finally, we ap-\\nply our synthetic dataset to neural architecture search and\\ndemonstrate its reliability in performing this important task.\\n4.1. Experimental Setup\\nWe follow up the conventional procedure used in the\\nliterature on dataset distillation. Every experiment involves', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='two phases—distillation and evaluation. First, we synthesize\\na small synthetic set (e.g., 10images per class) from a given\\nlarge real training set. We investigate three settings ipc=\\n1,10,50, which means that the distilled set contains 1,10or\\n50images per class respectively. Second, in the evaluation\\nphase on the synthetic data, we utilize the learnt synthetic set\\nto train randomly initialized neural networks and test their\\nperformance on the real test set. For each synthetic set, we\\nuse it to train five networks with random initializations and\\nreport the mean accuracy and its standard deviation for 1000\\niterations with a standard training procedure.\\nDatasets. We evaluate our method on various resolution\\ndatasets. We consider the CIFAR10 and CIFAR100 [23]\\ndatasets which consist of tiny colored natural images with\\n3753\\nTable 1. Comparison of the performances trained with ConvNet [12] to other distillation methods on the CIFAR [23] and Tiny ImageNet [25]', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='datasets. We reproduce the results of MTT [1]. We cite the reported results of other baselines from Cazenavette et al. [1]. We only provide\\nour reproduced results of DC and MTT on the Tiny ImageNet dataset as previous works did not report their results on this dataset.\\nCIFAR-10 CIFAR-100 Tiny ImageNet\\nipc 1 10 50 1 10 50 1 10\\nreal dataset 84.8±0.1 56.2±0.3 37.6±0.4\\nDC [50] 28.3±0.5 44.9 ±0.5 53.9 ±0.5 12.8±0.3 25.2 ±0.3 - - -\\nDM [49] 26.0±0.8 48.9 ±0.6 63.0 ±0.4 11.4±0.3 29.7 ±0.3 43.6 ±0.4 3.9±0.2 12.9 ±0.4\\nDSA [48] 28.8±0.7 52.1 ±0.5 60.6 ±0.5 13.9±0.3 32.3 ±0.3 42.8 ±0.4 - -\\nCAFE [44] 30.3±1.1 46.3 ±0.6 55.5 ±0.6 12.9±0.3 27.8 ±0.3 37.9 ±0.3 - -\\nCAFE+DSA 31.6±0.8 50.9 ±0.5 62.3 ±0.4 14.0±0.3 31.5 ±0.2 42.9 ±0.2 - -\\nPP [28] 46.4±0.6 65.5 ±0.3 71.9 ±0.2 24.6±0.1 43.1 ±0.3 48.4 ±0.3 - -\\nMTT [1] 46.2±0.8 65.4 ±0.7 71.6 ±0.2 24.3±0.3 39.7 ±0.4 47.7 ±0.2 8.8±0.3 23.2 ±0.2\\nMTT+Robust Learning 45.8±0.7 63.2 ±0.7 72.7 ±0.2 24.1±0.3 39.4 ±0.4 47.9 ±0.2 - -', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='FTD 46.8±0.3 66.6 ±0.3 73.8 ±0.2 25.2±0.2 43.4 ±0.3 50.7 ±0.3 10.4±0.3 24.5 ±0.2\\nTable 2. The performance comparison trained with ConvNet on the 128×128resolution ImageNet subset. We only cite the results of\\nMTT [1], which is the only and first distillation method among the baselines to apply their method on the high-resolution ImageNet subsets.\\nImageNette ImageWoof ImageFruit ImageMeow\\nipc 1 10 1 10 1 10 1 10\\nReal dataset 87.4±1.0 67.0±1.3 63.9±2.0 66.7±1.1\\nMTT 47.7±0.9 63.0 ±1.3 28.6±0.8 35.8 ±1.8 26.6±0.8 40.3 ±1.3 30.7±1.6 40.4 ±2.2\\nFTD 52.2±1.0 67.7±0.7 30.1±1.0 38.8±1.4 29.1±0.9 44.9±1.5 33.8±1.5 43.3±0.6\\nthe resolution of 32×32from 10 and 100 categories, respec-\\ntively. We conduct experiments on the Tiny ImageNet [25]\\ndataset with the resolution of 64×64. We also evaluate our\\nproposed FTD on the ImageNet subsets with the resolution\\nof128×128. These subsets are selected 10 categories by\\nCazenavette et al. [1] from the ImageNet dataset [4].', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Baselines and Models. We compare our method to a\\nseries of baselines including Dataset Condensation [50]\\n(DC), Differentiable Siamese Augmentation [48] (DSA),\\nand gradient-matching methods Distribution Matching [49]\\n(DM), Aligning Features [44] (CAFE), Parameter Prun-\\ning [28] (PP), and trajectory matching method [1] (MTT).\\nFollowing the settings of Cazenavette et al. [1], we dis-\\ntill and evaluate the synthetic set corresponding to CIFAR-\\n10 and CIFAR-100 using 3-layer convolutional networks\\n(ConvNet-3) while we move up to a depth- 4ConvNet for\\nthe images with a higher resolution ( 64×64) for the Tiny\\nImageNet dataset and a depth- 5ConvNet for the ImageNet\\nsubsets ( 128×128). We evaluate the cross-architecture\\nclassification performance of distilled images on four stan-\\ndard deep network architectures: ConvNet ( 3-layer) [12],\\nResNet [15], VGG [40] and AlexNet [24].\\nImplementation Details. We use ρ= 0.01, α= 1as the\\ndefault values while implementing FTD. The same suite of', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='differentiable augmentations [48] has been implemented asin previous studies [1, 49]. We use the Exponential Moving\\nAverage (EMA) [46] for faster convergence in the distilla-\\ntion phase for the synthetic image optimization procedure.\\nThe details of the hyperparameters used in buffer phase,\\ndistillation phase of each setting (real epochs per iteration,\\nsynthetic updates per iteration, image learning rate, etc.) are\\nreported in Appendix A.3.3. Our experiments were run on\\ntwo RTX3090 and four Tesla V100 GPUs.\\n4.2. Results\\nCIFAR and Tiny ImageNet. As demonstrated in Ta-\\nble 1, FTD surpasses all baselines among the CIFAR-10/100\\nand Tiny ImageNet dataset. In particular, our proposed FTD\\nachieves significant improvement with ipc = 10 ,50on\\nthe CIFAR-10/100 datasets. For example, our method im-\\nproves MTT [1] by 2.2%on the CIFAR-10 dataset with\\nipc= 50 , and achieves 3.5%improvement on the CIFAR-\\n100 dataset with ipc = 10 . Besides, the results under', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='“MTT+Robust learning” are obtained by using Equation 9\\nas the objective function of MTT during the distillation\\nphase. “MTT+Robust learning” boosts the performance\\nof MTT by 1.1%and0.2%withipc= 50 on the CIFAR-\\n10/100 datasets, respectively; However, it will incur a perfor-\\nmance degradation with ipc= 1,10. We have introduced\\n“MTT+Robust learning” in subsection 3.3.\\n3754\\nWe visualize part of the synthetic sets for ipc= 1,10of\\nthe CIFAR-100 and Tiny ImageNet datasets in Figure 3. Our\\nimages look easily identifiable and highly realistic, which\\nare akin to combinations of semantic features. We provide\\nmore additional visualizations in Appendix A.3.5 .\\nImageNet Subsets. The ImageNet subsets are signifi-\\ncantly more challenging than the CIFAR-10/100 [23] and\\nTiny ImageNet [25] datasets, because their resolutions are\\nmuch higher. This characteristic of the images makes it\\ndifficult for the distillation procedure to converge. In addi-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tion, the majority of the existing dataset distillation methods\\nmay result an out-of-memory issue when distilling high-\\nresolution data. The ImageNet subsets contains 10 cate-\\ngories selected from ImageNet-1k [4] following the setting\\nof MTT [1], which is the first distillation method which is\\ncapable of distilling higher-resolution ( 128×128) images.\\nThese subsets include ImageNette (assorted objects), Image-\\nWoof (dog breeds), ImageFruits (fruits), and ImageMeow\\n(cats) in conjunction with a depth-5 ConvNet.\\nAs shown in Table 2, FTD outperforms MTT in every\\nsubset with a significant improvement.1For example, we\\nsignificantly improve the performance on the ImageNette\\nsubset when ipc= 1,10by more than 4.5%.\\nCross-Architecture Generalization The ability to gen-\\neralize well across different architectures of the synthetic\\ndataset is crucial in the real application of dataset distillation.\\nHowever, the existing dataset distillation methods suffer', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='from a performance degradation when the synthetic dataset\\nis trained by the network with a different architecture than\\nthe one used in distillation [1, 44].\\nHere, we study the cross-architecture performance of\\nFTD, compare it with three baselines, and report the results\\nin Table 3. We evaluate FTD on CIFAR-10 with ipc= 50 .\\nWe use three more different neural network architectures\\nfor evaluation: ResNet [15], VGG [40] and AlexNet [24].\\nThe synthetic dataset is distilled with ConvNet ( 3-layer) [12].\\nThe results show that synthetic images learned with FTD per-\\nform and generalize well to different convolutional networks.\\nThe performances of synthetic data on architectures distinct\\nfrom the one used to distill should be utilized to validate that\\nthe distillation method is able to identify essential features\\nfor learning, other than merely the matching of parameters.\\n4.3. Ablation and Parameter Studies\\nExploring the Flat Trajectory Many studies [6, 17,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='18, 21, 29] have revealed that DNNs with flat minima can\\ngeneralize better than ones with sharp minima. Although\\nFTD encourages dataset distillation to seek a flat trajectory\\nwhich terminates a flat minimum, the progress along a flat\\nteacher trajectory, which minimizes the accumulated trajec-\\ntory error, contributes primarily to the performance gain\\n1The results of ImageNet subsets are cited exactly from [1].Table 3. Cross-Architecture Results trained with ConvNet on\\nCIFAR-10 with ipc = 50 . We reproduce the results of MTT,\\nand cite the results of DC and CAFE reported in Wang et al. [44].\\nEvaluation Model\\nMethod ConvNet ResNet18 VGG11 AlexNet\\nDC 53.9±0.5 20.8 ±1.0 38.8 ±1.1 28.7 ±0.7\\nCAFE 55.5±0.4 25.3 ±0.9 40.5 ±0.8 34.0 ±0.6\\nMTT 71.6±0.2 61.9 ±0.7 55.4 ±0.8 48.2 ±1.0\\nFTD 73.8±0.2 65.7±0.3 58.4±1.6 53.8±0.9\\nof FTD. To verify this, we design experiments to demon-\\nstrate that the attainment of a flat minimum does not en-\\nhance the accuracy of the synthetic dataset. We implement', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Sharpness-Aware Minimization (SAM) [11] to bias the train-\\ning over the synthetic dataset obtained from MTT to con-\\nverge at a flat minimum. We term this as “MTT + Flat\\nMinimum” and compare the results to FTD. A set values\\nofρ∈ {0.005,0.01,0.03,0.05,0.1}is tested for a thorough\\ncomparison. We report the comparison in Figure 4. It can\\nbe seen that a flatter minimum does not help the synthetic\\ndataset to generalize well. We provide more theoretical\\nexplanation about it in Appendix A.2. Therefore, FTD’s\\nchief advantage lies in the suppression of the accumulated\\ntrajectory error to improve dataset distillation.\\nEffect of EMA. We implement the Exponential Moving\\nAverage (EMA) with β= 0.999in the distillation phase of\\nFTD for enhanced convergence. While EMA contributes to\\nthe improvement, it is not the primary driver. The results of\\nour proposed approach with and without EMA are presented\\nin Table 4. We observe that EMA enhances the evaluation', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='accuracies. However, our proposed regularization in the\\nbuffer phase for a flatter teacher trajectory contributes most\\nsignificantly to the performance improvement.\\nWe have also conducted a parameter study on the coeffi-\\ncient ρand observed that ρ= 0.01is the optimal value for\\neach dataset considered. See Appendix A.3.1.\\nTable 4. Ablation study of FTD. FTD without EMA still signifi-\\ncantly surpasses MTT.\\nCIFAR-100 Tiny ImageNet\\nipc 10 50 1 10\\nMTT 39.7±0.4 47.7 ±0.2 8.8±0.3 23.2 ±0.2\\nFTD (w.o. EMA) 43.4±0.3 49.8 ±0.3 9.8±0.2 24.1 ±0.3\\nFTD 43.2±0.3 50.7±0.3 10.0±0.2 24.5±0.2\\n4.4. Neural Architecture Search (NAS)\\nTo better demonstrate the substantial practical benefits of\\nour proposed method FTD, we evaluate our method in neural\\narchitecture search (NAS). NAS is one of the important\\ndown-stream task of dataset distillation. It aims to find\\nthe best network architecture for a given dataset among a\\nvariety of architecture candidates. Dataset distillation uses\\n3755', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='3755\\nFigure 3. Visualization example of synthetic images distilled from 32×32CIFAR-100 ( ipc = 10 ), and 64×64Tiny ImageNet ( ipc = 1).\\n0.005 0.01 0.03 0.05 0.1\\nρ  394041424344Acc.(%)MTT\\nMTT+Flat Minimum\\nFTD\\nFigure 4. We apply SAM with different values of ρon the synthetic dataset\\nobtained from MTT to train the networks, which is termed as “MTT + Flat\\nMinimum”. “MTT” and “FTD” represent the standard results of MTT and\\nFTD on CIFAR-100 with ipc=10 , respectively. A “flat” minimum does not\\nhelp the synthetic dataset to generalize better.\\nthe synthetic dataset as the proxy to efficiently search for\\nthe optimal architecture, which reduces the computational\\ncost in a linear fashion. We show that FTD can synthesize\\na better and practical proxy dataset, which has a stronger\\ncorrelation with the real dataset.\\nFollowing [50], we implement NAS on the CIFAR-10\\ndataset on a search space of 720 ConvNets that differ in\\nnetwork depth, width, activation, normalization, and pooling', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='layers. More details can be found in Appendix A.3.4. We\\ntrain these different architecture models on the MTT syn-\\nthetic dataset, our synthetic dataset, and the real CIFAR-10\\ndataset for 200 epochs. Additionally, the accuracy on the\\ntest set of real data determines the overall architecture. The\\nSpearman’s rank correlation between the searched rankings\\nof the synthetic dataset and the real dataset training is used\\nas the evaluation metric. Since the top-ranking architectures\\nare more essential, only the rankings of the top 5, 10 and 20\\narchitectures will be used for evaluation, respectively.\\nOur results are displayed in Table 5. FTD achieves much\\nhigher rank correlation than MTT in every top- kranking.\\nIn particular, FTD achieves a 0.87correlation in the top-5\\nranking, which is very close to the value of 1.0in real dataset,\\nwhile MTT’s correlation is 0.41. FTD is thus able to obtain\\na reliable synthetic dataset, which generalizes well for NAS.\\n5. Conclusion and Future Work', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='We studied a flat trajectory distillation technique, that is\\nable to effectively mitigate the adverse effect of the accumu-Table 5. We implement NAS on CIFAR-10 with a search over 720\\nConvNets. We present the Spearman’s rank correlation ( 1.00is the\\nbest) of the top 5, 10, and 20 architectures between the rankings\\nsearched by the synthetic and real datasets. The Time column\\nrecords the entire time to search for each dataset.\\nTop 5 Top 10 Top 20 Time(min) Images No.\\nReal 1.00 1.00 1.00 6,804 50,000\\nMTT 0.41 0.36 -0.04 360 500\\nFTD 0.87 0.68 0.54 360 500\\nlated trajectory error leading to significant performance gain.\\nThe cross-architecture and NAS experiments also confirmed\\nFTD’s ability to generalize well across different architectures\\nand downstream tasks of dataset distillation.\\nWe note that the performance of the teacher trajectories\\nin the existing gradient-matching methods doesn’t represent\\nthe state-of-the-art. This is because the optimization of the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='teacher trajectories has to be simplified to improve the con-\\nvergence of distillation. The accumulation of the trajectory\\nerror, for instance, is a possible reason to limit the total num-\\nber of training epochs of the teacher trajectories, that calls\\nfor further research.\\nAcknowledgements\\nThis work is support by Joey Tianyi Zhou’s A*STAR\\nSERC Central Research Fund (Use-inspired Basic Research)\\nand the Singapore Government’s Research, Innovation and\\nEnterprise 2020 Plan (Advanced Manufacturing and Engi-\\nneering domain) under Grant A18A1b0045.\\nThis work is also supported by 1) National Natural Sci-\\nence Foundation of China (Grant No. 62271432); 2) Guang-\\ndong Provincial Key Laboratory of Big Data Computing,\\nThe Chinese University of Hong Kong, Shenzhen (Grant\\nNo. B10120210117-KP02); 3) Human-Robot Collaborative\\nAI for Advanced Manufacturing and Engineering (Grant\\nNo. A18A2b0046), Agency of Science, Technology and\\nResearch (A*STAR), Singapore; 4) Advanced Research and', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Technology Innovation Centre (ARTIC), the National Uni-\\nversity of Singapore (project number: A-0005947-21-00);\\nand 5) the Singapore Ministry of Education (Tier 2 grant:\\nA-8000423-00-00).\\n3756\\nReferences\\n[1]George Cazenavette, Tongzhou Wang, Antonio Torralba,\\nAlexei A Efros, and Jun-Yan Zhu. Dataset distillation\\nby matching training trajectories. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 4750–4759, 2022. 1, 2, 3, 4, 6, 7, 13\\n[2]Jang Hyun Cho and Bharath Hariharan. On the efficacy of\\nknowledge distillation. In Proceedings of the IEEE/CVF\\ninternational conference on computer vision , pages 4794–\\n4802, 2019. 1\\n[3]Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Dc-\\nbench: Dataset condensation benchmark. arXiv preprint\\narXiv:2207.09639 , 2022. 1\\n[4]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\\nFei-Fei. Imagenet: A large-scale hierarchical image database.\\nIn2009 IEEE conference on computer vision and pattern', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='recognition , pages 248–255. Ieee, 2009. 1, 6, 7\\n[5]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\\nToutanova. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. arXiv preprint\\narXiv:1810.04805 , 2018. 1\\n[6]Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua\\nBengio. Sharp minima can generalize for deep nets. In\\nInternational Conference on Machine Learning , pages 1019–\\n1028. PMLR, 2017. 7\\n[7]Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua\\nBengio. Sharp minima can generalize for deep nets. In\\nInternational Conference on Machine Learning , pages 1019–\\n1028. PMLR, 2017. 13\\n[8]Tian Dong, Bo Zhao, and Lingjuan Lyu. Privacy for free:\\nHow does dataset condensation help privacy? arXiv preprint\\narXiv:2206.00240 , 2022. 2\\n[9]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='formers for image recognition at scale. arXiv preprint\\narXiv:2010.11929 , 2020. 1\\n[10] Jiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Lian-\\ngli Zhen, Rick Siow Mong Goh, and Vincent YF Tan. Effi-\\ncient sharpness-aware minimization for improved training of\\nneural networks. arXiv preprint arXiv:2110.03141 , 2021. 13\\n[11] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam\\nNeyshabur. Sharpness-aware minimization for efficiently\\nimproving generalization. In International Conference on\\nLearning Representations , 2020. 5, 7, 11, 12, 13\\n[12] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot\\nvisual learning without forgetting. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition , pages\\n4367–4375, 2018. 6, 7, 11\\n[13] Jack Goetz and Ambuj Tewari. Federated learning via syn-\\nthetic data. arXiv preprint arXiv:2008.04489 , 2020. 2\\n[14] Jianping Gou, Baosheng Yu, Stephen J Maybank, and\\nDacheng Tao. Knowledge distillation: A survey. Interna-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tional Journal of Computer Vision , 129(6):1789–1819, 2021.\\n1[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In Proceed-\\nings of the IEEE conference on computer vision and pattern\\nrecognition , pages 770–778, 2016. 6, 7\\n[16] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distill-\\ning the knowledge in a neural network. arXiv preprint\\narXiv:1503.02531 , 2(7), 2015. 1\\n[17] Sepp Hochreiter and J ¨urgen Schmidhuber. Simplifying neural\\nnets by discovering flat minima. In Proceedings of the 8th\\nInternational Conference on Neural Information Processing\\nSystems , pages 529–536, 1995. 7, 13\\n[18] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip\\nKrishnan, and Samy Bengio. Fantastic generalization mea-\\nsures and where to find them. In International Conference on\\nLearning Representations , 2019. 7, 13\\n[19] Haifeng Jin, Qingquan Song, and Xia Hu. Auto-keras: An\\nefficient neural architecture search system. In Proceedings of', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the 25th ACM SIGKDD international conference on knowl-\\nedge discovery & data mining , pages 1946–1956, 2019. 2\\n[20] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal,\\nMikhail Smelyanskiy, and Ping Tak Peter Tang. On large-\\nbatch training for deep learning: Generalization gap and sharp\\nminima. arXiv preprint arXiv:1609.04836 , 2016. 13\\n[21] Nitish Shirish Keskar, Jorge Nocedal, Ping Tak Peter Tang,\\nDheevatsa Mudigere, and Mikhail Smelyanskiy. On large-\\nbatch training for deep learning: Generalization gap and sharp\\nminima. In International Conference on Learning Represen-\\ntations , 2017. 7\\n[22] Yoon Kim and Alexander M Rush. Sequence-level knowledge\\ndistillation. arXiv preprint arXiv:1606.07947 , 2016. 1\\n[23] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-\\n10 and CIFAR-100 datasets. URl: https://www. cs. toronto.\\nedu/kriz/cifar. html , 6(1):1, 2009. 5, 6, 7\\n[24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Im-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='agenet classification with deep convolutional neural networks.\\nAdvances in neural information processing systems , 25, 2012.\\n6, 7\\n[25] Ya Le and Xuan Yang. Tiny imagenet visual recognition\\nchallenge. CS 231N , 7(7):3, 2015. 6, 7\\n[26] Shiye Lei and Dacheng Tao. A comprehensive survey to\\ndataset distillation. arXiv preprint arXiv:2301.05603 , 2023.\\n1\\n[27] Guang Li, Ren Togo, Takahiro Ogawa, and Miki Haseyama.\\nSoft-label anonymous gastric x-ray image distillation. In 2020\\nIEEE International Conference on Image Processing (ICIP) ,\\npages 305–309. IEEE, 2020. 2\\n[28] Guang Li, Ren Togo, Takahiro Ogawa, and Miki Haseyama.\\nDataset distillation using parameter pruning. arXiv preprint\\narXiv:2209.14609 , 2022. 6\\n[29] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom\\nGoldstein. Visualizing the loss landscape of neural nets. Pro-\\nceedings of the 32nd International Conference on Neural\\nInformation Processing Systems , 31, 2018. 7\\n[30] Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='James Stokes. Fisher–Rao metric, geometry, and complexity\\nof neural networks. In The 22nd International Conference on\\nArtificial Intelligence and Statistics , pages 888–896. PMLR,\\n2019. 13\\n3757\\n[31] Chen Liu, Mathieu Salzmann, Tao Lin, Ryota Tomioka, and\\nSabine S ¨usstrunk. On the loss landscape of adversarial train-\\ning: Identifying challenges and how to overcome them. Pro-\\nceedings of the 34th International Conference on Neural In-\\nformation Processing Systems , 33:21476–21487, 2020. 13\\n[32] Dougal Maclaurin, David Duvenaud, and Ryan Adams.\\nGradient-based hyperparameter optimization through re-\\nversible learning. In International conference on machine\\nlearning , pages 2113–2122. PMLR, 2015. 13\\n[33] David A McAllester. Pac-bayesian model averaging. In Pro-\\nceedings of the twelfth annual conference on Computational\\nlearning theory , pages 164–170, 1999. 11\\n[34] Timothy Nguyen, Zhourong Chen, and Jaehoon Lee. Dataset\\nmeta-learning from kernel ridge-regression. arXiv preprint', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='arXiv:2011.00050 , 2020. 2, 13\\n[35] Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon\\nLee. Dataset distillation with infinitely wide convolutional\\nnetworks. Advances in Neural Information Processing Sys-\\ntems, 34:5186–5198, 2021. 1\\n[36] Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon\\nLee. Dataset distillation with infinitely wide convolutional\\nnetworks. Advances in Neural Information Processing Sys-\\ntems, 34:5186–5198, 2021. 1, 13\\n[37] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff\\nDean. Efficient neural architecture search via parameters\\nsharing. In International conference on machine learning ,\\npages 4095–4104. PMLR, 2018. 2\\n[38] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang,\\nZhihui Li, Xiaojiang Chen, and Xin Wang. A comprehensive\\nsurvey of neural architecture search: Challenges and solutions.\\nACM Computing Surveys (CSUR) , 54(4):1–34, 2021. 2\\n[39] Andrea Rosasco, Antonio Carta, Andrea Cossu, Vincenzo\\nLomonaco, and Davide Bacciu. Distilled replay: Overcom-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ing forgetting through synthetic samples. arXiv preprint\\narXiv:2103.15851 , 2021. 2\\n[40] Karen Simonyan and Andrew Zisserman. Very deep convo-\\nlutional networks for large-scale image recognition. arXiv\\npreprint arXiv:1409.1556 , 2014. 6, 7\\n[41] Felipe Petroski Such, Aditya Rawal, Joel Lehman, Kenneth\\nStanley, and Jeffrey Clune. Generative teaching networks:\\nAccelerating neural architecture search by learning to gener-\\nate synthetic training data. In International Conference on\\nMachine Learning , pages 9206–9216. PMLR, 2020. 2\\n[42] Ilia Sucholutsky and Matthias Schonlau. Soft-label dataset\\ndistillation and text dataset distillation. In 2021 International\\nJoint Conference on Neural Networks (IJCNN) , pages 1–8.\\nIEEE, 2021. 1\\n[43] Paul Vicol, Jonathan P Lorraine, Fabian Pedregosa, David\\nDuvenaud, and Roger B Grosse. On implicit bias in overpa-\\nrameterized bilevel optimization. In International Conference\\non Machine Learning , pages 22234–22259. PMLR, 2022. 1', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[44] Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang,\\nShuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, and\\nYang You. Cafe: Learning to condense dataset by align-\\ning features. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pages 12196–\\n12205, 2022. 1, 2, 6, 7[45] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and\\nAlexei A Efros. Dataset distillation. arXiv preprint\\narXiv:1811.10959 , 2018. 1, 2, 3, 13\\n[46] Ross Wightman. Pytorch image models. https :\\n/ / github . com / rwightman / pytorch - image -\\nmodels , 2019. 6\\n[47] Ruonan Yu, Songhua Liu, and Xinchao Wang. Dataset\\ndistillation: A comprehensive review. arXiv preprint\\narXiv:2301.07014 , 2023. 1\\n[48] Bo Zhao and Hakan Bilen. Dataset condensation with differ-\\nentiable siamese augmentation. In International Conference\\non Machine Learning , pages 12674–12685. PMLR, 2021. 2,\\n6, 13\\n[49] Bo Zhao and Hakan Bilen. Dataset condensation with distri-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='bution matching. arXiv preprint arXiv:2110.04181 , 2021. 1,\\n2, 6\\n[50] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset\\ncondensation with gradient matching. ICLR , 1(2):3, 2021. 1,\\n2, 3, 6, 8, 13\\n[51] Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui,\\nHartwig Adam, Nicha Dvornek, Sekhar Tatikonda, James\\nDuncan, and Ting Liu. Surrogate gap minimization improves\\nsharpness-aware training. arXiv preprint arXiv:2203.08065 ,\\n2022. 5, 13\\n3758', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving\\nBen Agro*, Quinlan Sykora∗, Sergio Casas∗, Raquel Urtasun\\nWaabi, University of Toronto\\n{bagro, qsykora, sergio, urtasun }@waabi.ai\\nAbstract\\nA self-driving vehicle (SDV) must be able to perceive its\\nsurroundings and predict the future behavior of other traf-\\nfic participants. Existing works either perform object de-\\ntection followed by trajectory forecasting of the detected\\nobjects, or predict dense occupancy and flow grids for the\\nwhole scene. The former poses a safety concern as the num-\\nber of detections needs to be kept low for efficiency rea-\\nsons, sacrificing object recall. The latter is computation-\\nally expensive due to the high-dimensionality of the out-\\nput grid, and suffers from the limited receptive field inher-\\nent to fully convolutional networks. Furthermore, both ap-\\nproaches employ many computational resources predicting\\nareas or objects that might never be queried by the motion', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='planner. This motivates our unified approach to percep-\\ntion and future prediction that implicitly represents occu-\\npancy and flow over time with a single neural network. Our\\nmethod avoids unnecessary computation, as it can be di-\\nrectly queried by the motion planner at continuous spatio-\\ntemporal locations. Moreover, we design an architecture\\nthat overcomes the limited receptive field of previous ex-\\nplicit occupancy prediction methods by adding an efficient\\nyet effective global attention mechanism. Through exten-\\nsive experiments in both urban and highway settings, we\\ndemonstrate that our implicit model outperforms the cur-\\nrent state-of-the-art. For more information, visit the project\\nwebsite: https://waabi.ai/research/implicito.\\n1. Introduction\\nThe goal of a self-driving vehicle is to take sensor ob-\\nservations of the environment and offline evidence such as\\nhigh-definition (HD) maps and execute a safe and comfort-\\nable plan towards its destination. Meanwhile, it is important', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='to produce interpretable representations that explain why\\nthe vehicle performed a certain maneuver, particularly if a\\ndangerous event were to occur. To satisfy this, traditional\\nautonomy stacks [2, 6, 9, 14, 15, 20, 32, 38, 39] break down\\nthe problem into 3 tasks: perception, motion forecasting and\\nmotion planning. Perception leverages sensor data to local-\\nize the traffic participants in the scene. Motion forecasting\\n*Denotes equal contribution\\nFigure 1. Left: Explicit approaches predict whole-scene occu-\\npancy and flow on a spatio-temporal grid. Right: Our implicit\\napproach only predicts occupancy and flow at queried continuous\\npoints, focusing on what matters for downstream planning.\\noutputs the distribution of their future motion, which is typ-\\nically multimodal. Finally, motion planning is tasked with\\ndeciding which maneuver the SDV should execute.\\nMost autonomy systems are object-based , which in-\\nvolves detecting the objects of interest in the scene. To do', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='so, object detectors threshold predicted confidence scores\\nto determine which objects in the scene, a trade off be-\\ntween precision and recall. Furthermore, object-based mo-\\ntion forecasting methods are limited to predict only a hand-\\nful of sample trajectories or parametric distributions with\\nclosed-form likelihood for tractability, as they scale linearly\\nwith the number of objects and must run online in the vehi-\\ncle. This causes information loss that could result in unsafe\\nsituations [30], e.g., if a solid object is below the detection\\nthreshold, or the future behavior of the object is not captured\\nby the simplistic future trajectory estimates.\\nIn recent years, object-free approaches [3, 12, 29, 30]\\nthat model the presence, location and future behavior of all\\nagents in the scene via a non-parametric distribution have\\nemerged to address the shortcomings of object-based mod-\\nels. Object-free approaches predict occupancy probability', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and motion for each cell in a spatio-temporal grid, directly\\nfrom sensor data. More concretely, the spatio-temporal grid\\nis a 3-dimensional dense grid with two spatial dimensions\\nrepresenting the bird’s-eye view, and a temporal dimension\\nfrom the current observation time to a future horizon of\\nchoice. All dimensions are quantized at regular intervals.\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n1379\\nIn this paradigm, no detection confidence thresholding is re-\\nquired and the distribution over future motion is much more\\nexpressive, enabling the downstream motion planner to plan\\nwith consideration of low-probability objects and futures.\\nUnfortunately, object-free approaches are computationally\\nexpensive as the grid must be very high-dimensional to mit-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='igate quantization errors. However, most of the computa-\\ntion and memory employed in object-free methods is un-\\nnecessary, as motion planners only need to cost a set of\\nspatio-temporal points around candidate trajectories, and\\nnot a dense region of interest (RoI). We refer the reader to\\nFig. 1 for an illustration.\\nThis motivates our approach, I MPLICIT O, which utilizes\\nan implicit representation to predict both occupancy proba-\\nbility and flow over time directly from raw sensor data and\\nHD maps. This enables downstream tasks such as motion\\nplanning to efficiently evaluate a large collection of spatio-\\ntemporal query points in parallel, focusing on areas of in-\\nterest where there are potential interactions with the self-\\ndriving vehicle. We design an architecture that overcomes\\nthe limited receptive field of fully convolutional explicit ar-\\nchitectures [12, 24, 29, 30] by adding an efficient yet effec-\\ntive global attention mechanism. In particular, we leverage', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='deformable convolutions [8] and cross attention [37] to fo-\\ncus on a compact set of distant regions per query, giving\\nthe predictions a global context. This is useful as dynamic\\nobjects can move at very high speeds, particularly on the\\nhighway. For instance, when predicting in-lane occupancy\\n3 seconds into the future on a road where the speed limit is\\n30 m/s, the attention can look approximately 90 meters back\\nalong the lane to find the corresponding sensor evidence.\\nExtensive experiments in both urban and highway scenar-\\nios show that our object-free implicit approach outperforms\\nthe two prevalent paradigms in the literature on the task of\\noccupancy-flow prediction: (i) object-based methods that\\nfirst perform object detection to localize a finite set of ob-\\njects in the scene, and then predict their future trajectory dis-\\ntribution (ii) object-free explicit methods that predict dense\\nspatio-temporal grids of occupancy and motion.\\n2. Related Work', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='2. Related Work\\nIn this section we discuss traditional object-based per-\\nception and prediction as well as object-free perception and\\nprediction. We also outline literature in implicit geometric\\nreconstruction , which inspired our approach.\\nObject-based Perception and Motion Forecasting: The\\nmajority of previous works have adopted object-based rea-\\nsoning with a 2-stage pipeline, where first object detec-\\ntion [17, 42] and tracking [33, 38] are performed, followed\\nby trajectory prediction from past tracks [4, 28, 35, 44].\\nAs there are multiple possible futures, these methods ei-\\nther generate a fixed number of modes with probabilities\\nand/or draw samples to characterize the trajectory distribu-tion. This paradigm has three main issues [30, 36]: (1) un-\\ncertainty is not propagated from perception to downstream\\nprediction, (2) the predicted future distributions must be\\noverly compact in practice, as their size grows linearly with', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the number of objects, (3) thresholded decisions in percep-\\ntion make the planner blind to undetected objects. Several\\nworks [1, 2, 20, 23] tackle (1) by optimizing jointly through\\nthe multiple stages. However, (2) and (3) are fundamentally\\nhard to address in this object-based paradigm as it implies\\na finite set of objects that can be large in crowded scenes.\\nIn contrast, our model is agnostic to the number of objects\\nin the scene since it predicts the occupancy probability and\\nflow vectors at desired spatio-temporal points.\\nObject-Free Perception and Prediction: These methods\\nforecast future occupancy and motion from sensor data such\\nas LiDAR [3, 30] and camera [12, 13, 29], without consid-\\nering individual actors . P3 [30] first introduced temporal\\nsemantic occupancy grids as an interpretable intermediate\\nrepresentation for motion planning. MP3 [3] enhanced this\\nrepresentation by predicting an initial occupancy grid and\\nwarping it into the future with a spatio-temporal grid of', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='multi-modal flow predictions. Compared to fully convolu-\\ntional architectures, this flow-warping increases the effec-\\ntive receptive field for occupancy prediction, and imposes\\nprior on how occupancy can evolve over time. However,\\nforward flow struggles with dispersing occupancy over time\\nwhen uncertainty increases, as pointed out in [24]. FIERY\\n[12] added instance reasoning to the object-free paradigm\\nas a postprocessing, improving interpretability. O CCFLOW\\n[24] introduced backwards flow as a representation that can\\ncapture multi-modal forward motions with just one flow\\nvector prediction per grid cell. However, O CCFLOW iso-\\nlates the occupancy and flow forecasting problem by assum-\\ning input features (e.g., position, velocity, extent etc.) from\\na detection and tracking module, instead of raw sensor data.\\nWhile our work belongs to the category of object-\\nfree methods, our model only predicts occupancy-flow at\\nselect query points instead of outputting spatio-temporal', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='occupancy-flow grids with fully convolutional networks.\\nWe achieve this with an efficient and effective global at-\\ntention mechanism. This makes the model more expressive\\nwhile improving efficiency by reducing the computation to\\nonly that which matters to downstream tasks.\\nImplicit Geometric Reconstruction: Geometric recon-\\nstruction refers to the task of predicting the 3D shape of an\\nobject given some incomplete representation of it, e.g., im-\\nages, LiDAR, voxels. Implicit neural geometric reconstruc-\\ntion methods [5, 25, 26] have been shown to outperform ex-\\nplicit counterparts, which represent the 3D shape as a grid,\\nset of points, voxels or a mesh. In contrast, implicit methods\\ntrain a neural network to predict a continuous field that as-\\nsigns a value to each point in 3D space, so that a shape can\\nbe extracted as an iso-surface. More concretely, this net-\\n1380\\nwork can predict non-linear binary occupancy [5, 25] over\\n3D space f(x) :R3→[0,1], or a signed distance func-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tion to the surface [26]. Our work is motivated by these\\nideas, and we explore their application to the task of occu-\\npancy and flow prediction for self-driving. Particularly, the\\narchitecture of our implicit prediction decoder is inspired by\\nConvolutional Occupancy Networks [27], which proposed\\na translation equivariant approach to accurately reconstruct\\nlarge scale scenes.\\n3. Implicit Perception and Future Prediction\\nUnderstanding the temporal occupancy and motion of\\ntraffic participants in the scene is critical for motion plan-\\nning, allowing the self-driving vehicle (SDV) to avoid colli-\\nsions, maintain safety buffers and keep a safe headway [3].\\nPrevious methods [3, 12, 16, 24, 30] represent occupancy\\nand motion in bird’s-eye view (BEV) explicitly with a dis-\\ncrete spatio-temporal grid. This approach is resource inef-\\nficient, because it uses computational resources to predict\\nin regions that are irrelevant to the SDV . In this section, we', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='introduce I MPLICIT O, an implicit neural network that can\\nbe queried for both scene occupancy and motion at any 3-\\ndimensional continuous point ( x,y,t). Here, xandyare\\nspatial coordinates in BEV and t=¯t+ ∆tis the time into\\nthe future, where ¯trefers to the current timestep at which\\nwe are making the predictions, and ∆t≥0is an offset\\nfrom the current timestep into the future. This enables the\\nmotion planner to request the computation only at points\\naround the candidate trajectories that are being considered.\\nIn the remainder of this section, we first describe the task\\nparametrization, then the network architecture, and finally\\nhow to train our approach.\\n3.1. Task Parameterization\\nWe discuss the task by defining its inputs and outputs.\\nInput parameterization: Our model takes as input a\\nvoxelized LiDAR representation ( L) as well as a raster\\nof the HD map ( M). For the LiDAR, let S¯t=\\n{s¯t−Thistory +1, . . . , s¯t}be the sequence of the most recent', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Thistory = 5 sweeps. More precisely, st′∈RPt′×3is\\nthe LiDAR sweep ending at timestep t′containing a set\\nofPt′points, each of which described by three features:\\n(px, py, ph).pxandpyare the location of the point relative\\nto the SDV’s reference frame at the current timestep ¯t—\\ncentered at the SDV’s current position and with the x-axis\\npointing along the direction of the its heading. phcorre-\\nsponds to the height of the point above the ground. Finally,\\nL=V oxelize (St)∈RThistory D×H×W, where we follow\\nthe multi-sweep BEV voxelization proposed in [41] with\\na discretization of Ddepth channels normal to the BEV\\nplane, Hheight pixels and Wwidth pixels. For the raster\\nmap, we take the lane centerlines Crepresented as polylinesfrom the high-definition map and rasterize them on a sin-\\ngle channel M=Raster (C)∈R1×H×Wwith the same\\nspatial dimensions.\\nOutput parameterization: Letq= (x, y, t )∈R3be a\\nspatio-temporal point in BEV , at a future time t. The task is', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='to predict the probability of occupancy o:R3→[0,1],and\\nthe flow vector f:R3→R2specifying the BEV motion of\\nany agent that occupies that location. We model the back-\\nwards flow [24] for the flow vector f, as it can capture multi-\\nmodal forward motions with a single reverse flow vector per\\ngrid cell. More concretely, backwards flow describes the\\nmotion at time tand location (x, y)as the translation vector\\nat that location from t−1tot, should there be an object\\noccupying it:\\nf(x, y, t ) = (x′, y′)t−1−(x, y)t, (1)\\nwhere (x′, y′)denotes the BEV location at time t−1of the\\npoint occupying (x, y)at time t.\\n3.2. Network Architecture\\nWe parameterize the predicted occupancy ˆoand flow ˆf\\nwith a multi-head neural network ψ. This network takes as\\ninput the voxelized LiDAR L, raster map M, and a mini-\\nbatchQcontaining |Q|spatio-temporal query points q, and\\nestimates the occupancy ˆO={ˆo(q)}q∈Qand flow ˆF=\\n{ˆf(q)}q∈Qfor the mini-batch in parallel:\\nˆO,ˆF=ψ(L, M,Q) (2)', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ˆO,ˆF=ψ(L, M,Q) (2)\\nThe network ψis divided into a convolutional encoder that\\ncomputes scene features, and an implicit decoder that out-\\nputs the occupancy-flow estimates, as shown in Fig. 2.\\nInspired by [42], our encoder consists of two convolu-\\ntional stems that process the BEV LiDAR and map raster,\\na ResNet [11] that takes the concatenation of the LiDAR\\nand map raster features and outputs multi-resolution fea-\\nture planes, and a lightweight Feature Pyramid Network\\n(FPN) [21] that processes these feature planes. This results\\nin a BEV feature map at half the resolution of the inputs,\\ni.e.,Z∈RC×H\\n2×W\\n2, that contains contextual features cap-\\nturing the geometry, semantics, and motion of the scene. It\\nis worth noting that every spatial location (feature vector)\\nin the feature map Zcontains spatial information about its\\nneighborhood (the size of the receptive field of the encoder),\\nas well as temporal information over the past Thistory sec-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='onds. In other words, each feature vector in Zmay contain\\nimportant cues regarding the motion, the local road geome-\\ntry, and neighboring agents.\\nWe design an implicit occupancy and flow decoder mo-\\ntivated by the intuition that the occupancy at query point\\nq= (x, y, t )∈ Q might be caused by a distant object mov-\\ning at a fast speed prior to time t. Thus, we would like to use\\nthe local features around the spatio-temporal query location\\n1381\\nFigure 2. An overview of our model, I MPLICIT O. V oxelized LiDAR and an HD map raster are encoded by a two-stream CNN. The\\nresulting feature map Zis used by the decoder to obtain relevant features for the query points Qand eventually predict occupancy ˆOand\\nreverse flow ˆF. We illustrate the attention for a single query point q, but the inference is fully parallel across query points Q.\\nto suggest where to look next. For instance, there might be\\nmore expressive features about an object around its origi-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='nal position (at times {(¯t−Thistory + 1), . . . ,¯t}) since that\\nis where the LiDAR evidence is. Neighboring traffic par-\\nticipants that might interact with the object occupying the\\nquery point at time tare also relevant to look for (e.g., lead\\nvehicle, another vehicle arriving at a merging point at a sim-\\nilar time).\\nTo implement these intuitions, we first bi-linearly inter-\\npolate the feature map Zat the query BEV location qx,y=\\n(x, y)to obtain the feature vector zq=Interp (Z, x, y)∈\\nRCthat contains local information around the query. We\\nthen predict Kreference points {r1, . . . , rK}by offseting\\nthe initial query point rk=q+ ∆qk, where the offsets ∆q\\nare computed by employing the fully connected ResNet-\\nbased architecture proposed by Convolutional Occupancy\\nNetworks [27]. For all offsets we then obtain the corre-\\nsponding features zrk=Interp (Z,rk). This can be seen\\nas a form of deformable convolution [8]; a layer that pre-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='dicts and adds 2D offsets to the regular grid sampling loca-\\ntions of a convolution, and bi-linearly interpolates for fea-\\nture vectors at those offset locations. To aggregate the infor-\\nmation from the deformed sample locations, we use cross\\nattention between learned linear projections of zq∈R1×C\\nandZr={zr1, . . . , zrk} ∈RK×C. The result is the ag-\\ngregated feature vector z. See Fig. 2 for a visualization of\\nthis feature aggregation procedure. Finally, zandzqare\\nconcatenated, which, along with q, is processed by another\\nfully connected ResNet-based architecture with two linear\\nlayer heads to predict occupancy logits and flow. Please see\\nadditional implementation details and a full computational\\ngraph of our model in the supplementary.\\n3.3. Training\\nWe train our implicit network by minimizing a linear\\ncombination of an occupancy loss and a flow loss\\nL=Lo+λfLf. (3)Occupancy is supervised with binary cross entropy loss H\\nbetween the predicted and the ground truth occupancy at', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='each query point q∈ Q,\\nLo=1\\n|Q|X\\nq∈QH(o(q),ˆo(q)), (4)\\nwhere o(q)andˆo(q)are ground truth and predicted occu-\\npancy and query point q, respectively. The ground truth\\nlabels are generated by directly calculating whether or not\\nthe query point lies within one of the bounding boxes in the\\nscene. We supervised the flow only for query points that\\nbelong to foreground, i.e., points that are occupied. By do-\\ning so, the model learns to predict the motion of a query\\nlocation should it be occupied. We use the ℓ2error, where\\nthe labels are backwards flow targets from ttot−1com-\\nputed as rigid transformations between consecutive object\\nbox annotations:\\nLf=1\\n|Q|X\\nq∈Qo(q)||f(q)−ˆf(q)||2. (5)\\nWe train with a batch of continuous query points Q,\\nas opposed to points on a regular grid as previously pro-\\nposed. More concretely, for each example we sample |Q|\\nquery points uniformly across the spatio-temporal volume\\n[0, H]×[0, W]×[0, T], where H∈RandW∈Rare the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='height and width of a rectangular region of interest (RoI) in\\nBEV surrounding the SDV , and T∈Ris the future horizon\\nwe are forecasting.\\n4. Experiments\\nIn this section, we introduce the datasets and metrics\\nused to benchmark occupancy-flow perception and predic-\\ntion, and show that I MPLICIT O outperforms the state-of-\\nthe-art in both urban and highway settings. Further, we\\nconduct two ablations studies to understand the effect of\\nour contributions to the decoder architecture, and an analy-\\n1382\\nsis of the inference time of our implicit decoder compared\\nto explicit alternatives.\\nDatasets: We conduct our experiments using two\\ndatasets: Argoverse 2 Sensor [40] (urban), and High-\\nwaySim (highway). The Argoverse 2 Sensor (A V2) dataset\\nis collected in U.S. cities and consists of 850 fifteen-second\\nsequences with sensor data from two 32-beam LiDARs at\\na frequency of 10 Hz, high-definition maps with lane-graph\\nand ground-height data, and bounding box annotations. We', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='split the set into 700 sequences for training and 150 for val-\\nidation, and break the sequences into examples that include\\n5 frames of LiDAR history and a prediction time horizon of\\n5 seconds. In our experiments, we only consider the occu-\\npancy and flow of vehicles, which we define as the union of\\nthe following A V2 annotation classes: regular vehicle, large\\nvehicle, wheeled device, box truck, truck, vehicular trailer,\\ntruck cab, school bus, articulated bus, message-board trailer\\nand railed vehicle. Query points are labeled with occupancy\\nby checking if they intersect with the annotated bounding\\nboxes. Occupied query points are labeled with flow vec-\\ntors using finite differences between the current query point\\nand where that point was in the previous frame. Incom-\\nplete tracks caused by missing annotations were filled-in us-\\ning the constant turn rate and acceleration (CTRA) motion\\nmodel [18], so the models learn the prior that occupancy is', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='persistent. HighwaySim (HwySim) is a dataset generated\\nwith a state-of-the-art simulator, containing realistic high-\\nway traffic scenarios including on-ramps, off-ramps, and\\ncurved roads. A Pandar64 LiDAR is realistically simulated,\\nand maps with lane-graph and ground-height are provided.\\n700 sequences of around 15 seconds each are split 80/20\\ninto training/validation. Sequences are cut into examples,\\neach with a history of 5 past LiDAR frames and a 5 s future\\nhorizon.\\nMetrics: To be fair with the baselines, we evaluate all\\nmodels with query points on a regular spatio-temporal grid.\\nTemporally, we evaluate a prediction horizon of 5 seconds\\nwith a resolution of 0.5 seconds for both datasets. In A V2,\\nwe employ a rectangular RoI of 80 by 80 meters centered\\naround the SDV position at time ¯twith a spatial grid res-\\nolution of 0.2 m. In HwySim, we use an asymmetric ROI\\nwith 200 meters ahead and 40 meters back and to the sides\\nof the SDV at time ¯twith a grid resolution of 0.4 m. This is', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='to evaluate on highway vehicles moving fast (up to 30 m/s)\\nin the direction of the SDV over the full prediction horizon.\\nFor simplicity, we refer to the grid cell centroids as “query-\\npoints”. We evaluate the ability of the models to recover the\\nground-truth occupancy-flow. In particular, we utilize met-\\nrics to measure the precision, recall, accuracy and calibra-\\ntion of the occupancy, the flow errors, and the consistency\\nbetween the occupancy and flow predictions.\\nMean average precision (mAP): mAP captures if the\\nmodel correctly predicts a higher occupancy probability inoccupied regions relative to unoccupied regions, i.e., an ac-\\ncurate ranking of occupancy probability. mAP is computed\\nas the area under the precision recall curve averaged across\\nall timesteps in the prediction horizon.\\nSoft-IoU: We follow prior works [16, 24, 34] in the use\\nofsoft intersection over union for assessing occupancy pre-\\ndictions:\\nSoft-IoU =P\\nq∈Qo(q)ˆo(q)P\\nq∈Q(o(q) + ˆo(q)−o(q)ˆo(q)).(6)', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Unlike mAP, Soft-IoU also considers the magnitude of pre-\\ndicted occupancy probability instead of just the predicted\\nprobability ranking.\\nExpected Calibration Error (ECE): ECE measures the\\nexpected difference between model confidence and accu-\\nracy. This is desirable because the occupancy outputs may\\nbe used by downstream planners in a probabilistic way —\\ne.g., to compute the expected collision cost [3]. Thus, we\\nneed to understand if the predicted probabilities are poorly\\ncalibrated, i.e., suffering from over-confidence or under-\\nconfidence [10, 22].\\nForeground mean end-point-error (EPE): This metric\\nmeasures the average L2 flow error at each occupied query\\npoint:\\nEPE=1P\\nq∈Qo(q)X\\nq∈Qo(q)||f(q)−ˆf(q)||2.(7)\\nFlow Grounded Metrics: LetOt,ˆOt, and ˆFtdenote the\\noccupancy labels, predicted occupancy, and predicted flow\\non a spatio-temporal grid at time t, respectively. The flow\\ngrounded occupancy grid at timestep t >¯t, is obtained by', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='warping the ground truth occupancy grid at the previous\\ntimestep Ot−1with the predicted flow field ˆFt, and mul-\\ntiplying it element-wise with the predicted occupancy at the\\ncurrent timestep ˆOt[34]. We report flow-grounded Soft-\\nIoU and mAP by comparing the flow-grounded occupancy\\nto the occupancy ground truth. The flow grounded metrics\\nare useful for evaluating the consistency between the occu-\\npancy and flow predictions, as you can only achieve a high\\nscore if (1) the flow is accurate and (2) the warped ground-\\ntruth occupancy aligns well with the predicted occupancy\\nfor the next time step.\\nInference Time: When measuring inference time, all\\nmethods were implemented with vanilla PyTorch code (no\\ncustom kernels) and run on a single Nvidia GeForce GTX\\n1080 Ti. This metric is sensitive to implementation, hard-\\nware, and optimizations, and thus should not be compared\\nacross different works.\\nBaselines: We compare against five baselines that cover', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the different perception and prediction paradigms outlined\\nin the Sec. 1. M ULTI PATH [4], L ANEGCN [19], and\\nGORELA [7] are object-based trajectory prediction models.\\n1383\\nA V2 HwySim\\nmAP↑Soft-IoU ↑ECE↓EPE↓ Flow Grounded mAP ↑Soft-IoU ↑ECE↓EPE↓ Flow Grounded\\nmAP↑Soft-IoU ↓ mAP↑Soft-IoU ↓\\nMULTI PATH [4] 0.625 0.398 0.916 0.982 0.803 0.321 0.299 0.231 0.433 4.227 0.463 0.154\\nLANEGCN [19] 0.620 0.449 1.138 0.709 0.778 0.350 0.472 0.283 0.337 2.951 0.636 0.194\\nGORELA [7] 0.609 0.453 1.161 0.671 0.813 0.355 0.548 0.259 0.288 2.206 0.722 0.166\\nOCCFLOW [24] 0.675 0.356 0.348 0.390 0.886 0.493 0.597 0.370 0.260 0.842 0.841 0.330\\nMP3 [3] 0.774 0.422 0.201 0.472 0.902 0.466 0.637 0.246 0.208 1.172 0.833 0.193\\nIMPLICIT O 0.799 0.480 0.193 0.267 0.936 0.597 0.716 0.415 0.076 0.510 0.886 0.492\\nTable 1. Comparing our proposed model I MPLICIT O to state-of-the-art perception and prediction models on A V2 and HwySim. The first\\nthree rows are object-based models, while the others are object-free.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Ground Truth GORELA OCCFLOW MP3 IMPLICIT O\\nScene 1 Scene 2 Scene 3 Scene 4\\n0s 5s \\nFigure 3. Occupancy predictions of various models (columns) across four scenes (rows) in A V2. Opacity denotes occupancy probability,\\nand the colormap indicates prediction time ∆t(from current to future horizon, as shown on the right). Failure modes are highlighted with\\ncolored boxes: occupancy hallucination ,fading/missing occupancy ,inconsistent with map ,inconsistent with actors ,miss-detection .\\nFollowing [16, 22], to evaluate these object-based models\\non the task of occupancy-flow prediction, we rasterize the\\ntrajectory predictions to generate occupancy and flow fields.\\nFor occupancy, we rasterize the multi-modal trajectory pre-\\ndictions weighted by the mode probabilities. For flow, we\\ngenerate a multi-modal spatio-temporal flow field, where\\nfor each mode, a grid cell predicted to be occupied by an\\nobject contains the forward-flow rigid-transformations de-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='fined by the trajectory of that object. O CCFLOW uses the\\noccupancy-flow prediction architecture and flow-traced lossfrom Mahjourian et al. [24], using input features from a pre-\\ntrained detection and tracking module. More information\\non the detector can be found in the supplementary. MP3 [3]\\nis an end-to-end trained perception and prediction method\\nthat predicts multi-modal forward-flow vectors and associ-\\nated probabilities, and uses these to warp a predicted ini-\\ntial occupancy grid forward in time. We compute EPE on\\nthe expected motion vector (the probability-weighted sum\\nof modes) when evaluating MP3.\\n1384\\nBackwards Flow Attention OffsetsScene 1 Scene 2\\nFigure 4. Visualizations of the backwards flow field predictions\\nand attention offset predictions of I MPLICIT O at the last timestep\\nof the prediction horizon on Scene 1 and Scene 2 from Fig. 3.\\nBenchmark against state-of-the-art: Tab. 1 presents our\\nresults on A V2 and HwySim against the state-of-the-art', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='baselines described above. For this experiment, our model\\nIMPLICIT O predicts K= 1 attention offset. Our method\\noutperforms all others across all metrics and both datasets,\\nshowing the suitability of I MPLICIT O in both urban and\\nhighway settings. Fig. 3 displays qualitative results of these\\nmodels on A V2. Notice that all the object-based mod-\\nels generally under-perform relative to the object-free ap-\\nproaches. This is likely because these models are not op-\\ntimized for occupancy-flow directly, rather they are trained\\nto predict accurate trajectories. The qualitative results of\\nGORELA in Fig. 3 show that thresholding to produce in-\\nstances can result in missed detections (Scene 4). Further,\\nthe trajectory parameterization results in rasterized occu-\\npancy that is more often inconsistent with the map (Scenes\\n1 and 2), or inconsistent due to apparent collisions with\\nother actors (Scenes 1 and 3). This agrees with the re-\\nsults from [16], and reaffirms the utility of the object-free', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='parameterization. Interestingly, on A V2, the object-based\\napproaches have a high Soft-IoU despite their inaccurate\\noccupancy ranking. We find this is because these mod-\\nels are overconfident (reflected in their high ECE), which\\nis heavily rewarded by Soft-IoU on the many “easy” ex-\\namples in A V2 with stationary vehicles (in the evaluation\\nset, 64.4% of actors are static within the prediction hori-\\nzon). This is supported by the worse relative Soft-IoU of\\nthese object-based models on HwySim, which has a much\\nhigher proportion of dynamic actors. Interestingly MP3\\noutperforms O CCFLOW in the joint perception and predic-\\ntion setting, contrary to the results under perfect perception\\nassumption reported by [24]. We hypothesize this is be-\\ncause MP3 is trained end-to-end and does not have the in-mAP↑Soft-IoU ↑ECE↓EPE↓ Flow Grounded\\nmAP↑Soft-IoU ↑A V2MP3 [3] 0.774 0.422 0.201 0.472 0.902 0.466\\nCONV NET 0.796 0.466 0.135 0.312 0.929 0.581\\nCONV NETFT [24] 0.796 0.475 0.198 0.302 0.929 0.582', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='IMPLICIT O 0.799 0.480 0.193 0.267 0.936 0.597HwySimMP3 [3] 0.637 0.246 0.208 1.172 0.833 0.193\\nCONV NET 0.648 0.344 0.024 0.657 0.859 0.408\\nCONV NETFT [24] 0.654 0.351 0.024 0.657 0.860 0.416\\nIMPLICIT O 0.716 0.415 0.076 0.510 0.886 0.492\\nTable 2. Comparing the performance of occupancy-flow decoders,\\ntrained end-to-end, with the same encoder.\\ntermediate object-based detection representation. We can\\nsee in Fig. 3 that O CCFLOW hallucinates occupancy at the\\ninitial timestep (Scenes 1 and 4), and misses the detection of\\na vehicle in Scene 4, both of which are artifacts of training\\nwith input from a pre-trained detection model.\\nFlow and Attention visualization: Fig. 4 plots the re-\\nverse flow vectors as well as the attention offsets on two of\\nthe scenes from Fig. 3 (the middle two rows). The first ob-\\nservation is that the flow vectors and attention offsets rely\\nvery heavily on the map raster, as expected. The second\\nobservation is that the direction of the backward flow vec-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tors and the attention offsets are very heavily correlated.\\nThis shows that the model has learned to “look backwards”\\nalong the lanes to gather relevant features despite the offsets\\nbeing unsupervised. We hypothesize that I MPLICIT O out-\\nperforms the others because of its larger effective receptive\\nfield. Fig. 3 shows that I MPLICIT O maintains occupancy\\ninto the future more accurately than MP3. We attribute this\\nto the attention offsets being a more general and expressive\\nmechanism than MP3’s forward flow warping. To illustrate\\nthis further, in the supplementary we plot occupancy met-\\nrics as a function of prediction time ∆t.\\nInfluence of the decoder architecture: In this section,\\nwe compare various occupancy-flow decoders, all trained\\nend-to-end from LiDAR input with the same encoder ar-\\nchitecture as I MPLICIT O (described in Sec. 3.2). This al-\\nlows us to isolate the effect of our implicit decoder archi-\\ntecture design. C ONV NETimplements the decoder from', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Mahjourian et al. [24], but it takes as input a feature map\\nfrom the encoder, instead of hand-crafted detection fea-\\ntures. C ONV NETFT denotes this same decoder architecture\\ntrained with the auxiliary supervision of flow trace loss [24].\\nNote that MP3 and I MPLICIT O from Tab. 1 already use this\\nencoder and are trained end-to-end, so the same results are\\npresented for this ablation study. As shown in Tab. 2 our\\nimplicit decoder I MPLICIT O outperforms all the other de-\\ncoders across all metrics except for ECE, on both HwySim\\nand A V2. Notice that C ONV NETand C ONV NETFT outper-\\nform their detection-based counterpart O CCFLOW in Tab. 1\\nby a significant margin. This highlights the utility of end-to-\\n1385\\nNum. offsets mAP ↑Soft-IoU ↑ECE↓EPE↓ Flow Grounded\\nmAP↑Soft-IoU ↑A V2K= 0 0.790 0.456 0.128 0.300 0.930 0.583\\nK= 1 0.799 0.480 0.193 0.267 0.936 0.597\\nK= 4 0.797 0.478 0.257 0.252 0.936 0.570HwySimK= 0 0.649 0.359 0.052 0.686 0.857 0.421\\nK= 1 0.716 0.415 0.076 0.510 0.886 0.492', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='K= 4 0.714 0.404 0.051 0.509 0.890 0.487\\nTable 3. Ablation study on the effect of the number of predicted\\nattention offsets on the performance of I MPLICIT O.\\nend training in the object-free paradigm for occupancy-flow\\nprediction. Evidently, thresholding to produce detections\\nand hand-crafted features limits the information available\\nfor occupancy-flow perception and prediction. Again, we\\nhypothesize that I MPLICIT O outperforms the others due to\\nits offset mechanism increasing the effective receptive field.\\nEven with the powerful encoder and flow warping mecha-\\nnism, C ONV NETand MP3 fail to match this. This is sup-\\nported by the relatively close performance of C ONV NETto\\nIMPLICIT O on A V2, but not HwySim. On HwySim most\\nvehicles travel larger fraction of the ROI, so a larger effec-\\ntive receptive field is necessary. On A V2 more vehicles are\\nstationary or move slowly, so a large receptive field is less\\nimportant for occupancy-flow prediction.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Influence of the number of offsets ( K):Based on the\\nattention offset visualizations in Fig. 4, we have conjec-\\ntured that the predicted attention offsets of our implicit de-\\ncoder are responsible for its state-of-the-art performance.\\nIn this section, we ablate the number of predicted offsets\\nof I MPLICIT O to investigate this further. Tab. 3 reports re-\\nsults for implicit decoders with a different number of atten-\\ntion offsets. K= 0 denotes no attention offset, predict-\\ning occupancy-flow from zqalone without cross-attention\\n(see Fig. 2). We first note that K= 1 clearly outperforms\\nK= 0, particularly on HwySim. This aligns with our in-\\ntuition that the main function of the attention offsets is to\\nexpand the receptive field of a query point. Since vehicles\\ntravel at much lower speeds in urban than highway, A V2 has\\na lower effective receptive field requirement than HwySim\\nand thus the improvements are not as pronounced. We ob-\\nserve fairly close and mixed results between one and four', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='attention offsets. K= 1has the best occupancy prediction\\nmetrics, while K= 4is the best in some flow metrics. Un-\\nder the assumption that the predicted offsets look back to\\nwhere occupancy could have come from in the past, K >1\\nwould only improve performance over K= 1 when occu-\\npancy could come from more than one past location (e.g.,\\ncomplex intersections, Scene 2 of Fig. 3). These examples\\nare rare in the training and evaluation datasets, and having\\nredundant offsets in the simple cases where one offset suf-\\nfices could introduce noise, explaining why K= 4 does\\nnot outperform K= 1. See the supplementary for visual-0 0.5 1 1.5 2\\n·1050102030\\nNumber of Query Points |Q|Inference Time (ms)MP3\\nOCCFLOW\\nIMPLICIT O\\nFigure 5. Decoder inference time as a function of the number of\\nquery points for the object-free decoders presented in Tab. 1 on\\nHwySim. I MPLICIT O uses K= 1.\\nizations of the attention offsets when K= 4.\\nInference Time Comparison: In this section we compare', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the decoder inference time of explicit object-free methods\\nin the literature (from Tab. 1) against the decoder of I M-\\nPLICIT O with K= 1. Fig. 5 presents the inference time\\nas a function of the number of query points. For the ex-\\nplicit models (MP3, O CCFLOW ), this includes the time to\\nbi-linearly interpolate occupancy probability at the contin-\\nuous query points. The plot evaluates on query points in a\\nrange (1,2·105), that most planners will operate within. For\\ninstance, 2,000candidate trajectories ×10timesteps per\\ntrajectory is well aligned with the literature [3, 29–31, 43].\\nWith 200,000 trajectories, this allows for 10queries per\\ntimestep to integrate occupancy over the volume of the SDV ,\\nwhich should provide a good estimation of collision proba-\\nbility. We notice that for ≲20,000query points, I MPLIC -\\nITO has a constant inference time because it is batched over\\nquery points. Once the GPU is saturated, the operations are', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='run sequentially so inference time increases approximately\\nlinearly. The explicit decoders have approximately constant\\ninference times (the only increase is due to bilinear interpo-\\nlation), but are significantly slower than I MPLICIT O in this\\n“planner-relevant” range.\\n5. Conclusion\\nIn this paper, we have proposed a unified approach to\\njoint perception and prediction for self-driving that implic-\\nitly represents occupancy and flow over time with a neural\\nnetwork. This queryable implicit representation can provide\\ninformation to a downstream motion planner more effec-\\ntively and efficiently. We showcased that our implicit archi-\\ntecture predicts occupancy and flow more accurately than\\ncontemporary explicit approaches in both urban and high-\\nway settings. Further, this approach outperforms more tra-\\nditional object-based perception and prediction paradigms.\\nIn the future, we plan to assess the impact of our improve-\\nments on the downstream task of motion planning.\\n1386\\nReferences', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='1386\\nReferences\\n[1] Sergio Casas, Cole Gulino, Simon Suo, Katie Luo, Renjie\\nLiao, and Raquel Urtasun. Implicit latent variable model for\\nscene-consistent motion forecasting. In ECCV , 2020. 2\\n[2] Sergio Casas, Wenjie Luo, and Raquel Urtasun. Intentnet:\\nLearning to predict intention from raw sensor data. In CoRL ,\\n2018. 1, 2\\n[3] Sergio Casas, Abbas Sadat, and Raquel Urtasun. Mp3: A\\nunified model to map, perceive, predict and plan. In CVPR ,\\n2021. 1, 2, 3, 5, 6, 7, 8\\n[4] Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir\\nAnguelov. Multipath: Multiple probabilistic anchor trajec-\\ntory hypotheses for behavior prediction. In CoRL , 2019. 2,\\n5, 6\\n[5] Zhiqin Chen and Hao Zhang. Learning implicit fields for\\ngenerative shape modeling. In CVPR , 2019. 2, 3\\n[6] Alexander Cui, Sergio Casas, Abbas Sadat, Renjie Liao, and\\nRaquel Urtasun. Lookout: Diverse multi-future prediction\\nand planning for self-driving. In ICCV , 2021. 1\\n[7] Alexander Cui, Sergio Casas, Kelvin Wong, Simon Suo, and', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Raquel Urtasun. Gorela: Go relative for viewpoint-invariant\\nmotion forecasting. arXiv preprint arXiv:2211.02545 , 2022.\\n5, 6\\n[8] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\\nnetworks. In ICCV , 2017. 2, 4\\n[9] Junru Gu, Chen Sun, and Hang Zhao. Densetnt: End-to-end\\ntrajectory prediction from dense goal sets. In ICCV , 2021. 1\\n[10] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger.\\nOn calibration of modern neural networks. In ICML , 2017.\\n5\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In CVPR ,\\n2016. 3\\n[12] Anthony Hu, Zak Murez, Nikhil Mohan, Sof ´ıa Dudas, Jef-\\nfrey Hawke, Vijay Badrinarayanan, Roberto Cipolla, and\\nAlex Kendall. Fiery: Future instance prediction in bird’s-\\neye view from surround monocular cameras. In ICCV , 2021.\\n1, 2, 3\\n[13] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi\\nYan, and Dacheng Tao. St-p3: End-to-end vision-based au-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tonomous driving via spatial-temporal feature learning. In\\nECCV , 2022. 2\\n[14] Boris Ivanovic, Karen Leung, Edward Schmerling, and\\nMarco Pavone. Multimodal deep generative models for tra-\\njectory prediction: A conditional variational autoencoder ap-\\nproach. RA-L , 2020. 1\\n[15] Boris Ivanovic and Marco Pavone. The trajectron: Proba-\\nbilistic multi-agent trajectory modeling with dynamic spa-\\ntiotemporal graphs. In ICCV , 2019. 1\\n[16] Jinkyu Kim, Reza Mahjourian, Scott Ettinger, Mayank\\nBansal, Brandyn White, Ben Sapp, and Dragomir Anguelov.\\nStopnet: Scalable trajectory and occupancy prediction for ur-\\nban autonomous driving. In ICRA , 2022. 3, 5, 6, 7\\n[17] Alex H Lang, Sourabh V ora, Holger Caesar, Lubing Zhou,\\nJiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders\\nfor object detection from point clouds. In CVPR , 2019. 2[18] St ´ephanie Lef `evre, Dizan Vasquez, and Christian Laugier. A\\nsurvey on motion prediction and risk assessment for intelli-\\ngent vehicles. ROBOMECH , 2014. 5', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[19] Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song\\nFeng, and Raquel Urtasun. Learning lane graph representa-\\ntions for motion forecasting. In ECCV , 2020. 5, 6\\n[20] Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu,\\nSergio Casas, and Raquel Urtasun. Pnpnet: End-to-end per-\\nception and prediction with tracking in the loop. In CVPR ,\\n2020. 1, 2\\n[21] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\\nBharath Hariharan, and Serge Belongie. Feature pyramid\\nnetworks for object detection. In CVPR , 2017. 3\\n[22] Katie Luo, Sergio Casas, Renjie Liao, Xinchen Yan, Yuwen\\nXiong, Wenyuan Zeng, and Raquel Urtasun. Safety-oriented\\npedestrian motion and scene occupancy forecasting. 2021.\\n5, 6\\n[23] Wenjie Luo, Bin Yang, and Raquel Urtasun. Fast and furi-\\nous: Real time end-to-end 3d detection, tracking and motion\\nforecasting with a single convolutional net. In CVPR , 2018.\\n2\\n[24] Reza Mahjourian, Jinkyu Kim, Yuning Chai, Mingxing Tan,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Ben Sapp, and Dragomir Anguelov. Occupancy flow fields\\nfor motion forecasting in autonomous driving. RA-L , 2022.\\n2, 3, 5, 6, 7\\n[25] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\\nLearning 3d reconstruction in function space. In CVPR ,\\n2019. 2, 3\\n[26] Jeong Joon Park, Peter Florence, Julian Straub, Richard\\nNewcombe, and Steven Lovegrove. Deepsdf: Learning con-\\ntinuous signed distance functions for shape representation.\\nInCVPR , 2019. 2, 3\\n[27] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc\\nPollefeys, and Andreas Geiger. Convolutional occupancy\\nnetworks. In ECCV , 2020. 3, 4\\n[28] Tung Phan-Minh, Elena Corina Grigore, Freddy A Boulton,\\nOscar Beijbom, and Eric M Wolff. Covernet: Multimodal\\nbehavior prediction using trajectory sets. In CVPR , 2020. 2\\n[29] Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding\\nimages from arbitrary camera rigs by implicitly unprojecting\\nto 3d. In ECCV , 2020. 1, 2, 8', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[30] Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu,\\nPranaab Dhawan, and Raquel Urtasun. Perceive, predict, and\\nplan: Safe motion planning through interpretable semantic\\nrepresentations. In ECCV , 2020. 1, 2, 3, 8\\n[31] Abbas Sadat, Mengye Ren, Andrei Pokrovsky, Yen-Chen\\nLin, Ersin Yumer, and Raquel Urtasun. Jointly learnable be-\\nhavior and trajectory planning for self-driving vehicles. In\\nIROS , 2018. 8\\n[32] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and\\nMarco Pavone. Trajectron++: Dynamically-feasible trajec-\\ntory forecasting with heterogeneous data. In ECCV , 2020.\\n1\\n[33] Sarthak Sharma, Junaid Ahmed Ansari, J Krishna Murthy,\\nand K Madhava Krishna. Beyond pixels: Leveraging ge-\\nometry and shape cues for online multi-object tracking. In\\nICRA , 2018. 2\\n1387\\n[34] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien\\nChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,\\nYuning Chai, Benjamin Caine, et al. Scalability in perception', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='for autonomous driving: Waymo open dataset. In CVPR ,\\n2020. 5\\n[35] Charlie Tang and Russ R Salakhutdinov. Multiple futures\\nprediction. In Advances in Neural Information Processing\\nSystems , 2019. 2\\n[36] Ameni Trabelsi, Ross J Beveridge, and Nathaniel Blanchard.\\nDrowned out by the noise: Evidence for tracking-free motion\\nprediction. arXiv preprint arXiv:2104.08368 , 2021. 2\\n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. NeurIPS , 2017. 2\\n[38] Xinshuo Weng, Jianren Wang, David Held, and Kris Kitani.\\n3d multi-object tracking: A baseline and new evaluation met-\\nrics. In IROS , 2020. 1, 2\\n[39] Xinshuo Weng, Ye Yuan, and Kris Kitani. Ptp: Parallelized\\ntracking and prediction with graph neural networks and di-\\nversity sampling. RA-L , 2021. 1\\n[40] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lam-\\nbert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Rat-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='nesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes,\\nDeva Ramanan, Peter Carr, and James Hays. Argoverse 2:\\nNext generation datasets for self-driving perception and fore-\\ncasting. In NeurIPS , 2021. 5\\n[41] Bin Yang, Ming Liang, and Raquel Urtasun. Hdnet: Exploit-\\ning hd maps for 3d object detection. In CoRL , 2018. 3\\n[42] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-\\ntime 3d object detection from point clouds. In CVPR , 2018.\\n2, 3\\n[43] Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin\\nYang, Sergio Casas, and Raquel Urtasun. End-to-end inter-\\npretable neural motion planner. In CVPR , 2019. 8\\n[44] Tianyang Zhao, Yifei Xu, Mathew Monfort, Wongun Choi,\\nChris Baker, Yibiao Zhao, Yizhou Wang, and Ying Nian Wu.\\nMulti-agent tensor fusion for contextual trajectory predic-\\ntion. In CVPR , 2019. 2\\n1388', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='CCuantuMM: Cycle-Consistent Quantum-Hybrid Matching of Multiple Shapes\\nHarshil Bhatia1,2Edith Tretschk2Zorah L ¨ahner3Marcel Seelbach Benkner3\\nMichael Moeller3Christian Theobalt2Vladislav Golyanik2\\n1Indian Institute of Technology, Jodhpur2MPI for Informatics, SIC3Universit ¨at Siegen\\nAbstract\\nJointly matching multiple, non-rigidly deformed 3D\\nshapes is a challenging, NP-hard problem. A perfect\\nmatching is necessarily cycle-consistent: Following the\\npairwise point correspondences along several shapes must\\nend up at the starting vertex of the original shape. Unfor-\\ntunately, existing quantum shape-matching methods do not\\nsupport multiple shapes and even less cycle consistency.\\nThis paper addresses the open challenges and introduces\\nthe first quantum-hybrid approach for 3D shape multi-\\nmatching; in addition, it is also cycle-consistent. Its itera-\\ntive formulation is admissible to modern adiabatic quantum\\nhardware and scales linearly with the total number of input', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='shapes. Both these characteristics are achieved by reduc-\\ning the N-shape case to a sequence of three-shape match-\\nings, the derivation of which is our main technical contribu-\\ntion. Thanks to quantum annealing, high-quality solutions\\nwith low energy are retrieved for the intermediate NP-\\nhard objectives. On benchmark datasets, the proposed ap-\\nproach significantly outperforms extensions to multi-shape\\nmatching of a previous quantum-hybrid two-shape match-\\ning method and is on-par with classical multi-matching\\nmethods. Our source code is available at 4dqv.mpi-\\ninf.mpg.de/CCuantuMM/ .\\n1. Introduction\\nRecently, there has been a growing interest in applying\\nquantum computers in computer vision [3, 20, 32]. Such\\nquantum computer vision methods rely on quantum anneal-\\ning (QA) that allows to solve NP-hard quadratic uncon-\\nstrained binary optimisation problems (QUBOs). While\\nhaving to formulate a problem as a QUBO is rather inflexi-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ble, QA is, in the future, widely expected to solve QUBOs at\\nspeeds not achievable with classical hardware. Thus, cast-\\ning a problem as a QUBO promises to outperform more un-\\nrestricted formulations in terms of tractable problem sizes\\nand attainable accuracy through sheer speed.\\nA recent example for such a problem is shape match-\\ning, where the goal is to estimate correspondences between\\nFigure 1. Our quantum-hybrid method matches all 100shapes\\nof the FAUST collection [4] with guaranteed cycle consistency\\n(white arrows). Here, we visualise the matchings via texture\\ntransfer between all shapes. Our method scales linearly in the\\nnumber of shapes. See the full figure in the supplement.\\ntwo shapes. Accurate shape matching is a core element of\\nmany computer vision and graphics applications ( i.e.,tex-\\nture transfer and statistical shape modelling). If non-rigid\\ndeformations are allowed, even pairwise matching is NP-\\nhard, leading to a wide area of research that approximates', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='this problem, as a recent survey shows [15]. Matching two\\nshapes is one of the problems that was shown to benefit\\nfrom quantum hardware: Q-Match [39] iteratively updates\\na subset of point correspondences using QA. Specifically,\\nits cyclic α-expansion allows to parametrise changes to per-\\nmutation matrices without relaxations.\\nThe question we ask in this work is: How can we design\\namulti-shape matching algorithm in the style of Q-Match\\nthat has the same benefits? As we show in the experiments,\\nwhere we introduce several na ¨ıve multi-shape extensions of\\nQ-Match, this is a highly non-trivial task. Despite tweaking\\nthem, our proposed method significantly outperforms them.\\nIfN>2shapes have to be matched, the computational\\ncomplexity of na ¨ıve exhaustive pairwise matching increases\\nquadratically with N, which does not scale to large N. Fur-\\nthermore, these pairwise matchings can easily turn out to be\\ninconsistent with each other, thereby violating cycle con-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n1296\\nFigure 2. We match Nshapes by iteratively matching triplets.\\nsistency. For example, chaining the matchings PXYfrom\\nshapeXtoYandPYZfromYtoZcan give very differ-\\nent correspondences between XandZthan the direct, pair-\\nwise matching PXZofXandZ:PXZ̸=PXYPYZ. (We\\napply the permutation matrix PXYto the one-hot vertex in-\\ndex vector x∈ X asx⊤PXY=y∈ Y.) Thus, how can\\nwe achieve cycle consistency by design? A simple solution\\nwould be to match a few pairs in the collection to create a\\nspanning tree covering all shapes and infer the remaining\\ncorrespondences by chaining along the tree. Despite a high\\naccuracy of methods for matching two shapes, this corre-\\nspondence aggregation policy is prone to error accumula-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tion [36]. A special case of this policy is pairwise matching\\nagainst a single anchor shape, which also guarantees cycle-\\nconsistent solutions by construction [19]. We build on this\\nlast option in our method as it avoids error accumulation.\\nThis paper, in contrast to purely classical methods, lever-\\nages the advantages of quantum computing for multi-shape\\nmatching and introduces a new method for simultaneous\\nalignment of multiple meshes with guaranteed cycle con-\\nsistency; see Fig. 1. It makes a significant step forward\\ncompared to Q-Match and other methods utilising adiabatic\\nquantum computing (AQC), the basis for QA. Our c ycle-\\nconsistent quantu m-hybrid m ulti-shape m atching (CCuan-\\ntuMM; pronounced “quantum”) approach relies on the\\ncomputational power of modern quantum hardware. Thus,\\nour main challenge lies in casting our problem in QUBO\\nform, which is necessary for compatibility with AQC. To\\nthat end, two design choices are crucial: (1) Our method', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='reduces the N-shapes case to a series of three-shape match-\\nings; see Fig. 2. Thus, CCuantuMM is iterative and hy-\\nbrid, i.e.,it alternates in every iteration between preparing\\na QUBO problem on the CPU and sampling a QUBO so-\\nlution on the AQC. (2) It discards negligible higher-order\\nterms, which makes mapping the three-shape objective to\\nquantum hardware possible. In summary, the core technical\\ncontributions of this paper are as follows:\\n• CCuantuMM, i.e.,a new quantum-hybrid method for\\nshape multi-matching relying on cyclic α-expansion.\\nCCuantuMM produces cycle-consistent matchings andscales linearly with the number of shapes N.\\n• A new formulation of the optimisation objective for the\\nthree-shapes case that is mappable to modern QA.\\n• A new policy in shape multi-matching to address the\\nN-shape case relying on a three-shapes formulation\\nand adaptive choice of an anchor shape.\\nOur experiments show that CCuantuMM significantly\\noutperforms several variants of the previous quantum-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='hybrid method Q-Match [39]. It is even competitive with\\nseveral non-learning-based classical state-of-the-art shape\\nmethods [19, 33] and can match more shapes than them. In\\na broader sense, this paper demonstrates the very high po-\\ntential of applying (currently available and future) quantum\\nhardware in computer vision.\\n2. Related Work\\nQuantum Computer Vision (QCV). Several algorithms\\nfor computer vision relying on quantum hardware were pro-\\nposed over the last three years for such problems as shape\\nmatching [20, 32, 39], object tracking [29, 44], fundamental\\nmatrix estimation, point triangulation [17] and motion seg-\\nmentation [1], among others. The majority of them address\\nvarious types of alignment problems, i.e.,transformation es-\\ntimation [20,32], point set [20,34] and mesh alignment [39],\\ngraph matching [3,38] and permutation synchronisation [3].\\nOnly one of them, QSync [3], can operate on more than\\ntwo inputs and ensure cycle consistency for the underlying', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='matchings. In contrast to QSync, we can align inputs with\\nsubstantially larger (by two orders of magnitude) shapes in\\nthe number of vertices. Furthermore, we address a different\\nproblem, i.e.,mesh alignment, for which an algorithm for\\ntwo-mesh alignment with the help of AQC exists, namely\\nQ-Match [39], as we discuss in the introduction.\\nTo maintain the valid structure of permutation matrices,\\nQuantum Graph Matching, QGM [38] and Q-Sync [3] im-\\npose linear constraints. However, this requires that the cor-\\nresponding penalty parameter is carefully chosen. If the pa-\\nrameter is chosen too big and the linear constraints are en-\\nforced too strongly, this severely limits QGM and Q-Sync’s\\nability to handle large sets of vertices. On the other hand,\\nif the linear constraints are enforced too weakly, there is no\\nguarantee to obtain valid permutations as solutions. As dis-\\ncussed in the introduction, our approach follows Q-Match\\nto ensure valid permutation matrices by construction.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Multi-Shape Matching. We focus this section on multi-\\nshape and non-learning methods as CCuantuMM falls in\\nthis category. As our approach is not learning-based, it\\ntrivially generalises to unknown object categories without\\na need for training data. For a general survey of recent ad-\\nvances in shape matching, see Sahillioglu [37].\\nMatching shape pairs is a classical problem in geome-\\ntry processing [33]. When more than two shapes of the\\n1297\\nsame class exist, stronger geometric cues can be leveraged\\nto improve results by matching all of them simultaneously.\\nUnfortunately, the already very high problem complexity\\nincreases even further the more shapes are used. Hence,\\nexisting multi-shape matching methods limit the total num-\\nber of shapes and their resolution [11, 19], work in spectral\\nspace [23], or relax the permutation constraints [25]. Early\\nmulti-matching methods computed pair-wise matchings and\\nsubsequently used permutation synchronisation to establish', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='cycle consistency [30, 35, 40]. Still, permutation synchro-\\nnisation requires the eigendecomposition of a matrix with\\nquadratically increasing dimensions [35].\\nHiPPI [2] is a computationally efficient method that\\ntakes geometric relations into account while generalis-\\ning permutation synchronization but is still limited in\\nresolution. Instead of looking at permutations directly,\\nZoomOut [33] reduces the dimensionality of the problem\\nby projecting it onto the spectral decomposition. This idea\\nhas been extended to take cycle consistency within the spec-\\ntral space into account [22], which does not guarantee a\\npoint-wise consistent matching. To circumvent this issue,\\nIsoMuSh [19] jointly optimises point and functional corre-\\nspondences. The method detangles the optimisation into\\nsmaller subproblems by using a so-called universe shape\\nthat all shapes are mapped to instead of each other, as Cao\\nand Bernard do [10]. Using a universe is similar to requir-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ing a template shape, as many learning-based approaches\\ndo [16, 21, 41]: Both synchronise all correspondences by\\nmatching them through a unified space. This is similar to\\nthe concept of anchor shape we use but inherently less flex-\\nible because the universe size or template have to be given\\na priori . Our anchor is chosen from the given collection\\nas part of the method. Although using an anchor slightly\\nimproves our results, we note that our method does not nec-\\nessarily require one for operation. Hence, a random shape\\ncould be picked instead in each iteration without an increase\\nin complexity if using an anchor is not feasible or does not\\nrepresent the shape collection well.\\n3. Background\\n3.1. Adiabatic Quantum Computing (AQC)\\nAQC is a model of computation that leverages quan-\\ntum effects to obtain high-quality solutions to the NP-hard\\nproblem class of Quadratic Unconstrained Binary Optimi-\\nsation (QUBO) problems: minx∈{0,1}kxTQx, fork∈N', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and a QUBO weight matrix Q∈Rk×k. Each entry of x\\ncorresponds to its own logical qubit, the quantum equiva-\\nlent of a classical bit. The diagonal of Qconsists of lin-\\near terms, while the off-diagonals are inter-qubit coupling\\nweights . A QUBO can be classically tackled with simu-\\nlated annealing (SA) [42] or a variety of other discrete op-\\ntimisation techniques [18, 28], which, for large k, typicallyyield only approximate solutions as QUBOs are in general\\nNP-hard. AQC holds the potential to systematically out-\\nperform classical approaches such as SA, see [14,27] for an\\nexample. AQC exploits the adiabatic theorem of quantum\\nmechanics [5]: If, when starting from an equal superposi-\\ntion state of the qubits (where all solutions {0,1}khave the\\nsame probability of being measured) and imposing external\\ninfluences corresponding to the QUBO matrix on the qubits\\nsufficiently slowly (called annealing ), they will end up in a\\nquantum state that, when measured, yields a minimizer xof', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the QUBO. Not all physical qubits on a real quantum pro-\\ncessing unit (QPU) can be connected (coupled) with each\\nother. Thus, a minor embedding of the logical-qubit graph\\n(defined by non-zero entries of the QUBO matrix) into the\\nphysical-qubit graph (defined by the hardware) is required\\n[9]. This can lead to a chain of multiple physical qubits\\nrepresenting a single logical qubit. For details of quantum\\nannealing on D-Wave machines, we recommend [31].\\n3.2. Shape Matching\\nThe problem of finding a matching for non-rigidly de-\\nformed shapes having nvertices can be formulated as an\\nNP-hard Quadratic Assignment Problem (QAP) [8, 25]:\\nmin\\nP∈PnpTWp, (1)\\nwhere p=vec(P)∈ {0,1}n2is a flattened permutation\\nmatrix, and W∈Rn2×n2is an energy matrix describing\\nhow well certain pairwise properties are conserved between\\ntwo pairs of matches. If two shapes X,Yare discretised\\nwithnvertices each, Wis often chosen as [25]:\\nWx1·n+y1,x2·n+y2=∥dg\\nX(x1, x2)−dg\\nY(y1, y2)∥,(2)', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Y(y1, y2)∥,(2)\\nwhere x1, x2are vertices on X;y1, y2are vertices on Y; and\\ndg\\nI(·,·)is the geodesic distance on the shape I. Therefore,\\nWx1·n+y1,x2·n+y2represents how well the geodesic dis-\\ntance is preserved between corresponding pairs of vertices\\non the two shapes. Instead of pure geodesics, Gaussian-\\nfiltered geodesics are also a popular choice for W[43]:\\ngX(x1, x2) =1\\nρ√\\n2πexp \\n−1\\n2\\x12dg\\nX(x1, x2)\\nρ\\x132!\\n.(3)\\ngIcan be used to directly replace dg\\nIin (2). A small value\\nofρfocuses the energy on a local neighbourhood around the\\nvertex, while a large value increases the receptive field. Us-\\ning Gaussian kernels in Wplaces more emphasis on local\\ngeometry whereas geodesics have higher values far away\\nfrom the source vertex. Thus, geodesics work well for\\nglobal alignment and Gaussians for local fine-tuning.\\n1298\\n3.3. Cyclic α-Expansion (CAE)\\nCCuantuMM represents matchings as permutation ma-\\ntrices. In order to update them, we build on Seelbach et', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='al.’s CAE algorithm [39] (similar to a fusion move [24]),\\nwhich we describe here. A permutation matrix Pis called\\nanr-cycle , if there exist rdisjoint indices i1, . . . , i rsuch\\nthatPiji(j+1)% r= 1 for all j∈ {1, . . . , r }andPl,l= 1\\nfor all l /∈ {i1, . . . , i r}, in which case P= (i1i2. . . ir)is\\na common notation. Two cycles, i.e.two permutation ma-\\ntrices, are disjoint if these indices are pairwise disjoint. We\\nknow that disjoint cycles commute, which allows us to rep-\\nresent any permutation PasP=\\x10Qk\\ni=1ci\\x11\\x10Ql\\ni=1˜ci\\x11\\n,\\nwhere {ci}iand{˜ci}ieach are sets of disjoint 2-cycles.\\nGiven a set {ci}k\\ni=1ofkdisjoint 2-cycles, an update ,\\nor modification, of Pcan therefore be parameterised as:\\nP(α) =Qk\\ni=1cαi\\niP,where α∈ {0,1}kis a binary de-\\ncision vector determining the update. (Note that αiincαi\\ni\\nis an exponent, not an index.) Crucially, to make this pa-\\nrameterisation compatible with QUBOs, we need to make\\nit linear in α. To this end, CAE uses the following equality:\\nP(α) =P+kX', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='P(α) =P+kX\\ni=1αi(ci−I)P. (4)\\n4. Our CCuantuMM Method\\nPrevious adiabatic quantum computing methods [38,39]\\ncan only match two shapes. We present a method for match-\\ningNshapes. To ensure cycle consistency on Nshapes, it\\nis sufficient that all triplets of shapes are matched cycle con-\\nsistently [22]. CCuantuMM iteratively solves three-shape\\nproblems, which preserve cycle consistency by construc-\\ntion and fit on existing quantum annealers with limited re-\\nsources. We introduce our formulation for matching three\\nshapes in Sec. 4.1 and then extend it to Nshapes in Sec. 4.2.\\n4.1. Matching Three Shapes\\nConsider the problem of matching three non-rigidly de-\\nformed shapes S={X,Y,Z}ofnvertices each, while\\npreserving cycle consistency. We formulate this as an en-\\nergy minimisation with respect to P={PIJ∈Pn|I,J ∈\\nS}, the set of permutations between all pairs in S:\\nmin\\nPX\\nI,J∈S ;I̸=Jvec(PIJ)⊤WIJvec(PIJ),\\ns.t. PXZ=PXYPYZ,(5)\\nwhere WIJ∈Rn2×n2is the energy matrix describing', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='how well certain pairwise properties are conserved between\\nshapes IandJ(see Sec. 3.2), and PXZ=PXYPYZen-\\nforces cyclic consistency. An overview of the algorithm for\\nthree shapes is shown in Alg. 1.4.1.1 QUBO Derivation\\nTo perform optimisation on the quantum annealer, we need\\nto transform (5) into a QUBO problem. We adapt the CAE\\nformulation from [39] (see Sec. 3.3) and iteratively update\\nthe permutations to decrease the value of (5). Given a set\\nC={ci}k\\ni=1ofkdisjoint 2-cycles and binary decision\\nvariables α, we can parameterise our permutation matri-\\nces as PIJ(α) =PIJ+Pk\\ni=1αi(ci−I)PIJ. However,\\nCAE alone is not sufficient to transform (5) into a QUBO\\nas cyclic consistency is still missing. A simple solution\\nwould be to encourage cyclic consistency as a quadratic soft\\npenalty, but then there are no guarantees on the solution. In-\\nstead, we enforce cyclic consistency by construction:\\nPXZ(α, β) = (6)\\n(PXY+kX\\ni=1αi(ci−I)PXY)·(PYZ+kX\\nj=1βj(˜cj−I)PYZ),', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='j=1βj(˜cj−I)PYZ),\\nwhere {ci}i({˜cj}j)are cycles and α(β) are decision vari-\\nables for the updates to PXY(PYZ). For brevity, we write\\nCi= (ci−I)PXYand ˜Cj= (˜cj−I)PYZ. We explain\\nhow we construct the cycles in Secs. 4.1.2-4.1.3. Thus, we\\niteratively solve (5) via a sequence of problems of the form:\\nmin\\nα,β∈{0,1}kEXY(PXY(α)) +EYZ(PYZ(β)) +EXZ(PXZ(α, β)),\\n(7)\\nwhere EIJ(P, Q) = vec( P)⊤WIJvec(Q)and\\nEIJ(P) =EIJ(P, P). While the first two terms are in\\nQUBO form, the third term contains cubic and bi-quadratic\\nterms (see the supplement for details) which are not\\ncompatible with current quantum annealer architectures.\\nHigher-Order Terms. All of these higher-order terms\\ncome from PXZ(α, β), specifically from the term H=P\\niP\\njαiβjCi˜Cj. As we only consider 2-cycles, Ciand\\n˜Cjeach have only four non-zero elements. Due to this ex-\\ntreme sparsity, most summands of Hbecome 0.\\nWe could tackle these undesirable terms by decompos-\\ning them into quadratic terms by using ancilla variables and', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='adding penalty terms [13]. This gives exact solutions for\\nsufficiently high weights of the penalty terms. However,\\nmultiple reasons speak against this: (1) the QUBO matrix\\nis already dense (a clique) under the current formulation (as\\nwe will see in (9)) and adding ancilla qubits scales quadrat-\\nically in k, (2) adding penalties makes the problem harder\\nto solve, and (3) His sparsely non-zero and in practise we\\nobserve no drastic influence on the quality of the solution.\\nAlternatively, we could assume H=0. However, this\\nis unnecessarily strong since (1) Halso contributes to\\nquadratic terms ( E(H,·)), and (2) higher-order terms op-\\nerating on the same decision variable trivially reduce to\\n1299\\nquadratic terms: αi·αi=αifor binary αi. We thus keep\\nthose two types of terms and merely assume all truly cubic\\nand bi-quadratic terms to be zero.\\nCycle-Consistent CAE. After eliminating the higher-\\norder terms and ignoring constants from (7), we obtain\\n(with the same colour coding):\\nmin\\nα,βkX', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='min\\nα,βkX\\ni=1αi\\x12\\nFXY(PXY, Ci) +FXZ(PXZ, CiPYZ)\\x13\\n+kX\\nj=1βj\\x12\\nFYZ(PYZ,˜Cj) +FXZ(PXZ, PXY˜Cj)\\x13\\n+kX\\ni=1kX\\nl=1αiαl\\x12\\nEXY(Ci, Cl) +EXZ(CiPYZ, ClPYZ)\\x13\\n+kX\\nj=1kX\\nl=1βjβl\\x12\\nEYZ(˜Cj,˜Cl) +EXZ(PXY˜Cj, PXY˜Cl)\\x13\\n+kX\\ni=1kX\\nj=1αiβj\\x12\\nFXZ(PXY˜Cj, CiPYZ) +FXZ(Kij, PXZ)\\n+FXZ(Kij, PXY˜Cj) +FXZ(Kij, CiPYZ) +EXZ(Kij, Kij)\\x13\\n,\\n(8)\\nwhere PXZ=PXZ(0,0) =PXYPYZ, and we use the\\nshorthands FIJ(A, B) =EIJ(A, B) +EIJ(B, A)and\\nKij=Ci˜Cj. Denoting αk+j=βjfor an expanded\\nα∈ {0,1}2k, (8) can be written in the form:\\nmin\\nα∈{0,1}2kα⊤˜Wα. (9)\\nThe full formula for ˜Wis provided in the supplement. (9) is\\nfinally in QUBO form and we can optimise it classically or\\non real quantum hardware (see Sec. 3.1).\\n4.1.2 Choosing Vertices\\nThe question of how to choose the sets of cycles\\n{ci}i,{˜cj}jis still open. We first choose a subset of ver-\\ntices using the “worst vertices” criterion introduced in [39]\\nbased on the relative inconsistency IXYof a vertex x∈ X\\nunder the current permutation:\\nIXY(x) =X\\nw∈XWx·n+x⊤PXY,w·n+w⊤PXY, (10)', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='where we treat the one-hot vector x⊤PXYas a vertex in-\\ndex on Y. A high value indicates that xis inconsistent with\\nmany other matches under PXYand swapping it will likely\\nimprove the matching. We denote the set of the m=2kver-\\ntices with the highest IXY(·)asVX. Finally, we follow the\\npermutations to get VY={x⊤PXY|x∈VX} ⊂ Y .\\nIn practice, we observe a systematic improvement in the\\nmatchings when considering all three possibilities (using\\nIXY,IYZ, orIXZas the starting point). We thus use three\\n“sub”-iterations per iteration, one for each possibility.ux vxux wxux tx\\nwx txvx txvx wx\\nuy vy\\nuy wy\\nuy tywy ty\\nvy ty\\nvy wyCall\\nX\\nCall\\nYuy vy\\nuy wy\\nuy tywy ty\\nvy ty\\nvy wyux vxux wxux tx\\nwx txvx txvx wxC0\\nX\\nC1\\nX\\nC2\\nX\\nC2\\nYC1\\nYC0\\nYVX\\nuxvxwx\\ntx\\nVYuy\\nvy\\nwytyPXY\\nFigure 3. We depict the sub-iteration that starts from IXY, from\\nwhich we construct VX, thenCall\\nX, and finally CX={C0\\nX,C1\\nX,\\nC2\\nX}. We also build VYfrom VXand construct CYanalogously.\\nMatching each element of CXwith one from CY(visualised via', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='matching colours) leads to three sub-sub-iterations.\\nXα0 α1ux wx\\nvx txPXY(α)Yβ0 β1uy vy\\nwy tyPYZ(β)\\nZ\\nuz wzvz tz\\nPXZ=PXY(α)PYZ(β)\\nFigure 4. We depict the sub-sub-iteration for C2\\nX={(ux, vx),\\n(wx, tx)}andC1\\nY={(uy, wy),(vy, ty)}from Fig. 3.\\n4.1.3 Choosing Cycles\\nGiven the worst vertices VXandVYof any sub-iteration,\\nwe construct the cycles {ci}i,{˜cj}jfrom them. Fig. 3\\nvisualises this process. Focusing on VXfor the moment,\\nwe want to use all possible 2-cycles Call\\nX={(uv)|u, v∈\\nVX, u̸=v}in each sub-iteration. We cannot use all of\\nthese cycles at once since they are not disjoint, as CAE re-\\nquires. Instead, we next construct a set CXby partitioning\\nCall\\nXintom−1sets of cycles with each containing m/2=k\\ndisjoint cycles. An analogous methodology is used for CY.\\nWe now have CXandCY. Since we want to consider\\neach cycle of Call\\nXandCall\\nYonce, we need several “sub-sub”\\niterations. Thus, we next need to pick one set of cycles\\nfrom each CXandCYfor each sub-sub-iteration. There', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='are(m−1)2possible pairs between elements of CXandCY.\\nConsidering all possible pairs is redundant, does not provide\\nsignificant performance advantage, and increases the com-\\nputational complexity quadratically. Hence, we randomly\\npair each element of CXwith one element of CY(without\\nreplacement). This leads to m−1sub-sub-iterations, with\\neach one solving (9) with its respective cycles; see Fig. 4.\\n4.2. Matching NShapes\\nIn this section, we extend our model to matching a shape\\ncollection SwithNelements by iteratively matching three\\nshapes while still guaranteeing cycle consistency. Similar\\n1300\\nAlgorithm 1 Hybrid Three-Shape Matching\\nInput: Pi,S\\nOutput: Pi+1\\n1:forI∈ {IXY, IYZ, IXZ}do ▷sub-iterations\\n2: construct VX, VY, VZ(see Sec. 4.1.2)\\n3: construct CX,CY,CZ(see Sec. 4.1.3)\\n4: forl=1tom−1do ▷sub-sub-iterations\\n5: compute ˜W\\n6: optimise QUBO (9) ▷quantum\\n7: end for\\n8: PXY=Qk\\ni=1cαi\\niPXY\\n9: PYZ=Qk\\nj=1˜cαm+j\\njPYZ\\n10: PXZ=PXY·PYZ\\n11:end for\\n12:return Pi+1={PXY, PYZ, PXZ}', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='to the three-shape case (5), this can be formulated as an\\nenergy minimisation problem with respect to the set of per-\\nmutations P, except Snow has cardinality N:\\nmin\\nPX\\nI,J∈S ;I̸=JEIJ(PIJ),\\ns.t. PIK=PIJPJK∀I,J,K ∈ S .(11)\\nThe energy contains summands for each possible pair of\\nshapes. Solving all of them jointly would be computation-\\nally expensive and even more complicated than (8). This is\\nthe reason most multi-shape matching methods apply relax-\\nations at this point or cannot scale to a large N. However,\\nthe cycle-consistency constraints still only span over three\\nshapes; triplets are sufficient for global consistency [22].\\nWe thus iteratively focus on a triplet X,Y,Z ∈ S and\\nits set of permutations P′={PXY, PXZ, PYZ}. We\\ncould then minimise (11) over P′, leading to a block-\\ncoordinate descent optimisation of (11) over P. This would\\nmake the problem tractable on current quantum hardware\\nsince it keeps the number of decision variables limited. It', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='would also formally guarantee that our iterative optimisa-\\ntion would never increase the total energy. However, each\\niteration would be linear in Ndue to the construction of the\\nQUBO matrix, preventing scaling to large Nin practice.\\nWe therefore instead restrict (11) to those terms that depend\\nonly on permutations from P′. This leads to the same en-\\nergy as for the three-shape case (5), where the minimisation\\nis now over P′. Importantly, the computational complexity\\nper triplet becomes independent of N, allowing to scale to\\nlarge N. While this foregoes the formal guarantee that the\\ntotal energy never increases, we crucially find that it still\\nonly rarely increases in practice; see the supplement.\\nBy iterating over different triples Xi,Yi,Zi, we cover\\nthe entire energy term and reduce it iteratively. Specifically,\\none iteration iof the N-shape algorithm runs Alg. 1 onP′={Pi\\nXiYi, Pi\\nYiZi, Pi\\nXiZi}. Here, Xi∈ Sis chosen ran-\\ndomly (we use stratified sampling to pick all shapes equally', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='often), the anchor Yi=A∈ Sis fixed, and Zi=Xi−1. In\\npractice, we saw slightly better results with this scheme in-\\nstead of choosing the triplet randomly; see the supplement.\\nWe note that we only need to explicitly keep track of per-\\nmutations into the anchor: Pi={Pi\\nIA}I∈S,I̸=A. We then\\ngetPi+1fromPiby replacing Pi\\nXiAandPi\\nZiAwith their\\nupdated versions from Alg. 1.\\nInitialisation. We compute an initial set of pairwise permu-\\ntations Pinitusing a descriptor-based similarity of the nor-\\nmalised heat-kernel-signatures (HKS) [7] extended by a di-\\nmension indicating whether a vertex lies on the left or right\\nside of a shape (a standard practice in the shape-matching\\nliterature [19]). Instead of using a random shape as anchor,\\nthe results improve when using the following shape:\\nA= arg min\\nA∈SX\\nI∈S ;I̸=AEIA(Pinit\\nIA), (12)\\nwhere Pinit\\nIA∈ Pinit. We thus have P0={Pinit\\nIA}I∈S,I̸=A.\\nTime Complexity. Our algorithm scales linearly with the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='number of shapes. Each iteration of Alg. 1 has worst-case\\ntime complexity O(nk3), as we discuss in the supplement.\\nEnergy Matrix Schedule. In practise, we first use pure\\ngeodesics for a coarse matching and then Gaussian-filtered\\ngeodesics to fine-tune. Specifically, for a shape collection\\nof three shapes, we use a schedule with 2Tgeodesics iter-\\nations followed by 2TGaussian iterations. For each addi-\\ntional shape in the shape collection, we add Titerations to\\nboth schedules. We exponentially decrease the variance of\\nthe Gaussians every N−1iterations to ρ(i) =c2exp(c1\\ni−T)\\nwhere c1andc2are chosen such that the variance decreases\\nfrom 25% to5%of the shape diameter over the iterations.\\nThus, allshapes undergo one iteration with the same spe-\\ncific variance. We refer to the supplement for more details.\\n5. Experimental Evaluation\\nWe compare against state-of-the-art multi-matching\\nmethods with a focus on quantum methods. We consider\\nclassical works for reference. All experiments use Python', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='3.9 on an Intel Core i7-8565U CPU with 8GB RAM and the\\nD-Wave Advantage System 4.1 (accessed via Leap 2). We\\nwill release our code, which is accelerated using Numba.\\nHyperparameters. We set T=11. We set the number of\\nworst vertices mto16% of the number of vertices n.\\nQuantum Comparisons. The closest quantum work, Q-\\nMatch [39], matches only two shapes. We consider two\\nadaptations to multi-matching: 1) Q-MatchV2-cc, similar to\\nour CCuantuMM, chooses an anchor and matches the other\\nshapes pairwise to it, implicitly enforcing cycle consis-\\ntency; and 2) Q-MatchV2-nc matches all pairs of shapes di-\\nrectly, without guaranteed cycle consistency. In both cases,\\n1301\\n0 5·10−2 0.1 0.15406080100\\nGeodesic Error% Correspondences\\n0 5·10−2 0.1 0.155060708090100\\nGeodesic Error% CorrespondencesQA one instance\\nSA one instance\\nQA another instance\\nSA another instance\\nFigure 5. PCK curves for (left) two three-shape and (right) two\\nten-shape instances using QA and SA. In each plot, we denote one', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='instance by normal lines and the other one by dotted lines.\\nwe use our faster implementation and adapt our energy ma-\\ntrix schedule, which gives significantly better results.\\nClassical Comparisons. For reference, we also compare\\nagainst the classical, non-learning-based multi-matching\\nstate of the art: IsoMuSh [19] and the synchronised version\\nof ZoomOut [33], which both guarantee vertex-wise cycle\\nconsistency across multiple shapes.\\nEvaluation Metric. We evaluate the correspondences using\\nthe Princeton benchmark protocol [26]. Given the ground-\\ntruth correspondences P∗\\nIJfor matching the shape ItoJ,\\nthe error of vertex v∈ Iunder our estimated matching PIJ\\nis given by the normalised geodesic distance:\\nev(PIJ) =dg\\nJ(v⊤PIJ, v⊤P∗\\nIJ)\\ndiam(J), (13)\\nwhere diam(·)is the shape diameter. We plot the fraction of\\nerrors that is below a threshold in a percentage-of-correct-\\nkeypoints (PCK) curve, where the threshold varies along the\\nx-axis. As a summary metric, we also report the area-under-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the-curve (AUC) of these PCK curves.\\nDatasets. The FAUST dataset [4] contains real scans of\\nten humans in different poses. We use the registration sub-\\nset with ten poses for each class and downsample to 500\\nvertices. TOSCA [6] has 76shapes from eight classes of\\nhumans and animals. We downsample to ∼1000 vertices.\\nSMAL [45] has scans of toy animals in arbitrary poses,\\nnamely 41non-isometric shapes from five classes registered\\nto the same template. ( E.g., the felidae (cats) class con-\\ntains scans of lions, cats, and tigers.) We downsample to\\n1000 vertices. We use the same number of vertices as Iso-\\nMuSh [19], except that they use 1000 vertices for FAUST.\\n5.1. Experiments on Real Quantum Annealer\\nWe run two three-shapes and two ten-shapes experiments\\nwith FAUST on a real QPU. However, since our QUBO ma-\\ntrices are dense, we effectively need to embed a clique on\\nthe QPU. (The supplement contains a detailed analysis of\\nthe minor embeddings and the solution quality.) Hence, we', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='test a reduced version of our method with 20 worst vertices\\nper shape (40 virtual qubits in total), as more would worsen\\nOurs\\nSource\\nIsoMuSh\\nFigure 6. Qualitative results on the TOSCA [6] cat class. We\\ncolour a source shape and transfer this colouring to target shapes\\nvia the matches estimated by our method or IsoMuSh [19].\\nresults significantly on current hardware. To compensate\\nfor this change, we use more iterations for the ten-shape\\nexperiments. We use 200 anneals per QUBO, the default\\nannealing path, and the default annealing time of 20µs. As\\nstandard chain strength, we choose 1.0001 times the largest\\nabsolute value of entries in Q. Each ten-shape experiment\\ntakes about 10 minutes of QPU time. In total, our results\\ntook about 30 minutes of QPU time for a total of 5.5·104\\nQUBOs. QA under these settings achieves a similar per-\\nformance as SA under the same settings (Fig. 5). As QPU\\ntime is expensive and since we have just shown that SA per-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='forms comparably to a QPU in terms of result quality, we\\nperform the remaining experiments with SA under our de-\\nfault settings, on classical hardware. This is common prac-\\ntice [1, 39, 44] since SA is conceptually close to QA. For\\nadditional results, including results on the new Zephyr hard-\\nware [12], we refer to the supplement.\\n5.2. Comparison to Quantum and Classical SoTA\\nOurs Q-MatchV2-cc Q-MatchV2-nc IsoMuSh ZoomOut HKS\\nFAUST 0.989 0.886 0.879 0.974 0.886 0.746\\nTOSCA 0.967 0.932 0.940 0.952 0.864 0.742\\nSMAL 0.866 0.771 0.813 0.926 0.851 0.544\\nTable 1. AUC averaged over all classes of each dataset. For refer-\\nence, we also include classical methods on the right.\\nFAUST. We outperform both quantum and classical prior\\nwork, as Fig. 7a and Tab. 1 show. Because we downsample\\nFAUST more, IsoMuSh’s results are better in our experi-\\nments than what Gao et al. [19] report.\\nMatching 100 Shapes. Next, we demonstrate that, unlike\\nIsoMuSh and ZoomOut, our approach can scale to match-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ing all 100 shapes of FAUST. Fig. 1 contains qualitative re-\\nsults. Tab. 2 compares the runtime of our method (using\\nSA) to others. Only ours and Q-MatchV2-cc scale well to\\n100shapes while ZoomOut and IsoMuSh cannot.\\nTOSCA. Fig. 7b and Tab. 1 show that our method achieves\\nstate-of-the-art results. While IsoMuSh’s PCK curve starts\\nhigher (better), the AUC in Tab. 1 suggests that our method\\nperforms better overall. Fig. 6 has qualitative examples.\\nSMAL. Our CCuantuMM outperforms the quantum base-\\nlines, both in terms of PCK (Fig. 7c) and AUC (Tab. 1).\\n1302\\n0 5·10−2 0.1 0.1560708090100\\nGeodesic Error% Correspondences\\n(a) FAUST [4]0 5·10−2 0.1 0.1560708090100\\nGeodesic Error% Correspondences\\n(b) TOSCA [6]0 5·10−2 0.1 0.1520406080100\\nGeodesic Error% CorrespondencesOurs\\nQ-MatchV2-cc\\nQ-MatchV2-nc\\nIsoMuSh\\nZoomOut\\nHKS\\n(c) SMAL [45]\\nFigure 7. Quantitative results on all three datasets. For each dataset, we match all shapes within a class and then plot the average PCK', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='curve across classes. We plot classical methods with dashed lines as they are only for reference. HKS is our initialisation (see Sec. 4.2).\\n# Shapes Ours Q-MatchV2-cc Q-MatchV2-nc IsoMuSh ZoomOut\\n10 97 16 81 (4+)0 .3 4\\n100 1137 175 ∼8000†OOM OOM\\nTable 2. Runtime (in min) for FAUST. IsoMuSh uses ZoomOut\\nfor initialisation. “OOM” (out of memory): memory requirements\\nare infeasible. “†” denotes an estimate.\\n0 5·10−2 0.1 0.15708090100\\nGeodesic Error% Correspondences With Gaussians\\nWithout Gaussians\\n0 5·10−2 0.1 0.1580859095100\\nGeodesic Error% Correspondences3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nFigure 8. We ablate (left) the usage of Gaussian kernels, and\\n(right) the large-scale multi-shape setting. Gaussian kernels im-\\nprove the results greatly. Matching more shapes improves results.\\nAt the same time, it achieves performance on par with\\nZoomOut and below IsoMuSh. SMAL is considered the\\nmost difficult of the three datasets due to the challenging\\nnon-isometric deformations of its shapes. All methods thus', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='show worse performance compared to FAUST and TOSCA.\\n5.3. Ablation Studies\\nWe perform an ablation study on FAUST to analyse how\\ndifferent components of our method affect the quality of the\\nmatchings. We refer to the supplement for more ablations.\\nGaussian Energy Schedule. Our schedule, which starts\\nwith geodesics and afterwards uses Gaussians, provides a\\nsignificant performance gain over using only geodesics, un-\\nder the same number of iterations, see Fig 8. That is because\\nGaussians better correct local errors in our approach.\\nDoes Using More Shapes Improve Results? We analyse\\nwhat effect increasing the number of shapes Nhas on the\\nmatchings’ quality. We first randomly select three shapes\\nand run our method on them, to obtain the baseline. Next,we run our method again and again from scratch, each time\\nadding one more shape to the previously used shapes. This\\nisolates the effect of using more shapes from all other fac-\\ntors. In Fig. 8, we plot the PCK curves for the three se-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='lected shapes . We repeat this experiment for several ran-\\ndomly sampled instances. Our results show that including\\nmore shapes improves the matchings noticeably overall.\\n5.4. Discussion and Limitations\\nOur method and all considered methods are based on\\nintrinsic properties like geodesic distances. Thus, without\\nleft-right labels for initialisation, they would produce par-\\ntial flips for inter-class instances in FAUST and intra-class\\ninstances in TOSCA and SMAL. For a large worst-vertices\\nset, contemporary quantum hardware leads to embeddings\\n(see Sec. 3.1) with long chains, which are unstable, degrad-\\ning the result quality. Finally, while our method is currently\\nslower in practice than SA, it would immediately benefit\\nfrom the widely expected quantum advantage in the future.\\n6. Conclusion\\nThe proposed method achieves our main goal: improv-\\ning mesh alignment w.r.t. the quantum state of the art. Fur-\\nthermore, it is even highly competitive among classical', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='state-of-the-art methods. This suggests that the proposed\\napproach can be used as a reference for comparisons and\\nextensions of classical mesh-alignment works in the future.\\n(For such cases, classical SA is a viable alternative when\\naccess to quantum computers is lacking.) Our results show\\nthat ignoring certain higher-order terms still allows for high-\\nquality matchings, which is promising for future quantum\\napproaches that could use similar approximations. Finally,\\nunlike classical work, we designed our method within the\\nconstraints of contemporary quantum hardware. We found\\nthat iteratively considering shape triplets is highly effective,\\nperhaps even for classical methods.\\nAcknowledgements . This work was partially supported by the\\nERC Consolidator Grant 4DReply (770784). ZL is funded by the\\nMinistry of Culture and Science of the State of NRW.\\n1303\\nReferences\\n[1] Federica Arrigoni, Willi Menapace, Marcel Seelbach\\nBenkner, Elisa Ricci, and Vladislav Golyanik. Quantum mo-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tion segmentation. In Eur. Conf. Comput. Vis. , 2022. 2, 7\\n[2] Florian Bernard, Johan Thunberg, Paul Swoboda, and Chris-\\ntian Theobalt. Hippi: Higher-order projected power itera-\\ntions for scalable multi-matching. In Int. Conf. Comput. Vis. ,\\n2019. 3\\n[3] Tolga Birdal, Vladislav Golyanik, Christian Theobalt, and\\nLeonidas Guibas. Quantum permutation synchronization. In\\nIEEE Conf. Comput. Vis. Pattern Recog. , 2021. 1, 2\\n[4] Federica Bogo, Javier Romero, Matthew Loper, and\\nMichael J. Black. FAUST: Dataset and evaluation for 3D\\nmesh registration. In IEEE Conf. Comput. Vis. Pattern\\nRecog. , 2014. 1, 7, 8\\n[5] Max Born and Vladimir Fock. Beweis des adiabatensatzes.\\nZeitschrift f ¨ur Physik , 51(3):165–180, 1928. 3\\n[6] Alexander M Bronstein, Michael M Bronstein, and Ron\\nKimmel. Numerical geometry of non-rigid shapes . Springer\\nScience & Business Media, 2008. 7, 8\\n[7] Michael M Bronstein and Iasonas Kokkinos. Scale-invariant\\nheat kernel signatures for non-rigid shape recognition. In', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='IEEE Conf. Comput. Vis. Pattern Recog. , 2010. 6\\n[8] O. Burghard and R. Klein. Efficient lifted relaxations of the\\nquadratic assignment problem. In Vision, Modeling and Vi-\\nsualization (VMV) , 2017. 3\\n[9] Jun Cai, William G Macready, and Aidan Roy. A prac-\\ntical heuristic for finding graph minors. arXiv preprint\\narXiv:1406.2741 , 2014. 3\\n[10] Dongliang Cao and Florian Bernard. Unsupervised deep\\nmulti-shape matching. In Eur. Conf. Comput. Vis. , 2022. 3\\n[11] Luca Cosmo, Emanuele Rodola, Andrea Albarelli, Facundo\\nM´emoli, and Daniel Cremers. Consistent partial matching of\\nshape collections via sparse modeling. In Computer Graph-\\nics Forum , volume 36, pages 209–221. Wiley Online Library,\\n2017. 3\\n[12] D-Wave. Zephyr topology of d-wave quantum proces-\\nsors (d-wave technical report series). https://www.\\ndwavesys . com / media / 2uznec4s / 14 - 1056a -\\na _ zephyr _ topology _ of _ d - wave _ quantum _\\nprocessors.pdf , 2021. 7\\n[13] Nike Dattani. Quadratization in discrete optimization and', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='quantum mechanics. arXiv preprint arXiv:1901.04405 ,\\n2019. 4\\n[14] Vasil S Denchev, Sergio Boixo, Sergei V Isakov, Nan Ding,\\nRyan Babbush, Vadim Smelyanskiy, John Martinis, and\\nHartmut Neven. What is the computational value of finite-\\nrange tunneling? Physical Review X , 6(3):031015, 2016. 3\\n[15] Bailin Deng, Yuxin Yao, Roberto M. Dyke, and Juyong\\nZhang. A survey of non-rigid 3d registration. Computer\\nGraphics Forum , 41(2):559–589, 2022. 1\\n[16] Theo Deprelle, Thibault Groueix, Matthew Fisher,\\nVladimir G Kim, Bryan C Russell, and Mathieu Aubry.\\nLearning elementary structures for 3d shape generation and\\nmatching. In Neurips , 2019. 3[17] Anh-Dzung Doan, Michele Sasdelli, David Suter, and Tat-\\nJun Chin. A hybrid quantum-classical algorithm for robust\\nfitting. In IEEE Conf. Comput. Vis. Pattern Recog. , 2022. 2\\n[18] Iain Dunning, Swati Gupta, and John Silberholz. What\\nworks best when? a systematic evaluation of heuristics\\nfor max-cut and qubo. INFORMS Journal on Computing ,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='30(3):608–624, 2018. 3\\n[19] Maolin Gao, Zorah Lahner, Johan Thunberg, Daniel Cre-\\nmers, and Florian Bernard. Isometric multi-shape matching.\\nInIEEE Conf. Comput. Vis. Pattern Recog. , 2021. 2, 3, 6, 7\\n[20] Vladislav Golyanik and Christian Theobalt. A quantum com-\\nputational approach to correspondence problems on point\\nsets. In IEEE Conf. Comput. Vis. Pattern Recog. , 2020. 1, 2\\n[21] Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan\\nRussell, and Mathieu Aubry. 3d-coded : 3d correspondences\\nby deep deformation. In Eur. Conf. Comput. Vis. , 2018. 3\\n[22] Qi-Xing Huang and Leonidas Guibas. Consistent shape\\nmaps via semidefinite programming. In Computer graphics\\nforum , volume 32, pages 177–186, 2013. 3, 4, 6\\n[23] Ruqi Huang, Jing Ren, Peter Wonka, and Maks Ovsjanikov.\\nConsistent zoomout: Efficient spectral map synchronization.\\nInComputer Graphics Forum , volume 39, pages 265–278,\\n2020. 3\\n[24] Lisa Hutschenreiter, Stefan Haller, Lorenz Feineis, Carsten', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Rother, Dagmar Kainm ¨uller, and Bogdan Savchynskyy. Fu-\\nsion moves for graph matching. In Int. Conf. Comput. Vis. ,\\n2021. 4\\n[25] Itay Kezurer, Shahar Z. Kovalsky, Ronen Basri, and Yaron\\nLipman. Tight relaxation of quadratic matching. In Sympo-\\nsium on Geometry Processing (SGP) , 2015. 3\\n[26] Vladimir G. Kim, Yaron Lipman, and Thomas Funkhouser.\\nBlended intrinsic maps. ACM Trans. Graph. , 2011. 7\\n[27] Andrew D King, Jack Raymond, Trevor Lanting, Sergei V\\nIsakov, Masoud Mohseni, Gabriel Poulin-Lamarre, Sara\\nEjtemaee, William Bernoudy, Isil Ozfidan, Anatoly Yu\\nSmirnov, et al. Scaling advantage over path-integral monte\\ncarlo in quantum simulation of geometrically frustrated mag-\\nnets. Nature communications , 12(1):1–6, 2021. 3\\n[28] Gary Kochenberger, Jin-Kao Hao, Fred Glover, Mark Lewis,\\nZhipeng L ¨u, Haibo Wang, and Yang Wang. The uncon-\\nstrained binary quadratic programming problem: a survey.\\nJournal of combinatorial optimization , 28(1):58–81, 2014.\\n3', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='3\\n[29] Junde Li and Swaroop Ghosh. Quantum-soft qubo suppres-\\nsion for accurate object detection. In Eur. Conf. Comput. Vis. ,\\n2020. 2\\n[30] Eleonora Maset, Federica Arrigoni, and Andrea Fusiello.\\nPractical and efficient multi-view matching. In Int. Conf.\\nComput. Vis. , 2017. 3\\n[31] Catherine C. McGeoch. Adiabatic quantum computation and\\nquantum annealing: Theory and practice. Synthesis Lectures\\non Quantum Computing , 5(2):1–93, 2014. 3\\n[32] Natacha Kuete Meli, Florian Mannel, and Jan Lellmann.\\nAn iterative quantum approach for transformation estimation\\nfrom point sets. In IEEE Conf. Comput. Vis. Pattern Recog. ,\\n2022. 1, 2\\n1304\\n[33] Simone Melzi, Jing Ren, Emanuele Rodola, Abhishek\\nSharma, Peter Wonka, and Maks Ovsjanikov. Zoomout:\\nSpectral upsampling for efficient shape correspondence.\\nACM Transactions on Graphics (Proc. SIGGRAPH Asia) ,\\n2019. 2, 3, 7\\n[34] Mohammadreza Noormandipour and Hanchen Wang.\\nMatching point sets with quantum circuit learning. In', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='International Conference on Acoustics, Speech and Signal\\nProcessing , 2022. 2\\n[35] Deepti Pachauri, Risi Kondor, and Vikas Singh. Solving the\\nmulti-way matching problem by permutation synchroniza-\\ntion. Adv. Neural Inform. Process. Syst. , 2013. 3\\n[36] Yusuf Sahilioglu and Y ¨ucel Yemez. Multiple shape corre-\\nspondence by dynamic programming. Computer Graphics\\nForum (CGF) , 33(7), 2014. 2\\n[37] Yusuf Sahillioglu. Recent advances in shape correspon-\\ndence. The Visual Computer , 36, 2020. 2\\n[38] Marcel Seelbach Benkner, Vladislav Golyanik, Christian\\nTheobalt, and Michael Moeller. Adiabatic quantum graph\\nmatching with permutation matrix constraints. In Int. Conf.\\n3D Vis. (3DV) , 2020. 2, 4\\n[39] Marcel Seelbach Benkner, Zorah L ¨ahner, Vladislav\\nGolyanik, Christof Wunderlich, Christian Theobalt, and\\nMichael Moeller. Q-match: Iterative shape matching viaquantum annealing. In Int. Conf. Comput. Vis. , 2021. 1,\\n2, 4, 5, 6, 7\\n[40] Yanyao Shen, Qixing Huang, Nati Srebro, and Sujay Sang-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='havi. Normalized spectral map synchronization. Adv. Neural\\nInform. Process. Syst. , 2016. 3\\n[41] Ramana Sundararaman, Gautam Pai, and Maks Ovsjanikov.\\nImplicit field supervision for robust non-rigid shape match-\\ning. In Eur. Conf. Comput. Vis. , 2022. 3\\n[42] Peter JM Van Laarhoven and Emile HL Aarts. Simulated\\nannealing. In Simulated annealing: Theory and applications ,\\npages 7–15. Springer, 1987. 3\\n[43] Matthias Vestner, Zorah L ¨ahner, Amit Boyarski, Or Litany,\\nRon Slossberg, Tal Remez, Emanuele Rodol `a, Alex M.\\nBronstein, Michael M. Bronstein, Ron Kimmel, and Daniel\\nCremers. Efficient deformable shape correspondence via\\nkernel matching. In Int. Conf. 3D Vis. (3DV) , 2017. 3\\n[44] Jan-Nico Zaech, Alexander Liniger, Martin Danelljan,\\nDengxin Dai, and Luc Van Gool. Adiabatic quantum com-\\nputing for multi object tracking. In IEEE Conf. Comput. Vis.\\nPattern Recog. , 2022. 2\\n[45] Silvia Zuffi, Angjoo Kanazawa, David Jacobs, and\\nMichael J. Black. 3D menagerie: Modeling the 3D shape', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and pose of animals. In IEEE Conf. Comput. Vis. Pattern\\nRecog. , 2017. 7, 8\\n1305', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='An Image Quality Assessment Dataset for Portraits\\nNicolas Chahine1,2Ana-Stefania Calarasanu1Davide Garcia-Civiero1\\nThÂeo Cayla1Sira Ferradans1Jean Ponce2,3\\n1DXOMARK2DÂepartement d’informatique de l’Ecole normale sup Âerieure (ENS-PSL, CNRS, Inria)\\n3Institute of Mathematical Sciences and Center for Data Science, New York University\\nAbstract\\nYear after year, the demand for ever-better smartphone\\nphotos continues to grow, in particular in the domain of\\nportrait photography. Manufacturers thus use perceptual\\nquality criteria throughout the development of smartphone\\ncameras. This costly procedure can be partially replaced\\nby automated learning-based methods for image quality as-\\nsessment (IQA). Due to its subjective nature, it is necessary\\nto estimate and guarantee the consistency of the IQA pro-\\ncess, a characteristic lacking in the mean opinion scores\\n(MOS) widely used for crowdsourcing IQA. In addition,\\nexisting blind IQA (BIQA) datasets pay little attention to', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the difficulty of cross-content assessment, which may de-\\ngrade the quality of annotations. This paper introduces\\nPIQ23, a portrait-specific IQA dataset of 5116 images of\\n50 predefined scenarios acquired by 100 smartphones, cov-\\nering a high variety of brands, models, and use cases. The\\ndataset includes individuals of various genders and ethnic-\\nities who have given explicit and informed consent for their\\nphotographs to be used in public research. It is annotated\\nby pairwise comparisons (PWC) collected from over 30 im-\\nage quality experts for three image attributes: face detail\\npreservation, face target exposure, and overall image qual-\\nity. An in-depth statistical analysis of these annotations\\nallows us to evaluate their consistency over PIQ23. Fi-\\nnally, we show through an extensive comparison with ex-\\nisting baselines that semantic information (image context)\\ncan be used to improve IQA predictions. The dataset along\\nwith the proposed statistical analysis and BIQA algorithms', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='are available: https://github.com/DXOMARK-\\nResearch/PIQ2023\\n1. Introduction\\nSocial media has made smartphones a vital tool for con-\\nnecting with people worldwide. Visual media, particularly\\nportrait photography, has become a crucial aspect of shar-\\ning content on these platforms. Portrait photography serves\\nnumerous applications ( e.g., advertisements, social media)and use cases ( e.g., anniversaries, weddings). Capturing\\na high-quality portrait is a complex exercise that demands\\ncareful consideration of multiple factors, such as scene se-\\nmantics, compositional rules, image quality, and other sub-\\njective properties [ 46].\\nSmartphone manufacturers strive to deliver the best vi-\\nsual quality while minimizing production costs to rival pro-\\nfessional photography. Achieving this requires implement-\\ning complex tuning and optimization protocols to calibrate\\nimage quality in smartphone cameras. These cameras intro-\\nduce sophisticated non-linear processing techniques such as', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='multi-image fusion or deep learning-based image enhance-\\nment [ 55], resulting in a combination of authentic (realis-\\ntic) camera distortions. This makes traditional objective\\nquality assessment [ 4,16,32,40] that models digital cam-\\neras as linear systems unreliable [ 9]. Therefore, in addi-\\ntion to objective measurements, the tuning process also in-\\ncludes perceptual evaluations where cameras are assessed\\nby image quality experts. This procedure requires shoot-\\ning and evaluating thousands of use cases, which can be\\ncostly, time-consuming, and challenging to reproduce. Au-\\ntomatic image quality assessment (IQA) methods that try to\\nmimic human perception of quality have been around for\\nmany years, in order to help in the tuning process [ 14,36,\\n37,39,48,57,60,64]. Blind IQA (BIQA), in particular, is\\na branch of IQA where image quality is evaluated without\\nthe need for undistorted reference images. Learning-based\\nBIQA methods [ 15,24,25,27,52,59,62,67,69] have shown', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='good performance on authentic camera distortion datasets\\n[9,13,21,56,61,70], annotated by subjective assessment\\nof image quality. Annotating these datasets is considered\\nan ill-posed problem, as the subjective opinions are not de-\\nterministic, making it challenging to use BIQA methods as\\naccurate quality measures. Therefore, there is a need to de-\\nvelop a quantitative and formal framework to evaluate and\\ncompare subjective judgments in an objective manner. In\\nthis paper, we rely on pairwise comparisons performed by\\nimage quality experts along a fixed and relevant set of at-\\ntributes.\\nMultiple attributes, including target exposure, dynamic\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n9968\\n(a) (b)', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='9968\\n(a) (b)\\nFigure 1. (a) Scenes from the PIQ23 dataset. (b) Examples of the region of interest (ROI) used for different attribute comparisons. Top:\\noverall quality; we use a resized version of the full image. Bottom: details & target exposure; we use an upscaled face area.\\nrange, color, sharpness, noise, and artifacts, define image\\nquality [ 3]. Portrait images require additional considera-\\ntions, such as skin tone, bokeh effect, face detail rendering,\\nand target exposure on the face, which fall under the scope\\nof portrait quality assessment (PQA) [ 40].\\nTo the best of our knowledge, the problem of assessing\\nthe quality of a portrait image has received limited atten-\\ntion. Most of the work on face IQA [ 49] has been directed\\ntowards improving face recognition systems and not as an\\nindependent topic. As far as we know, our paper introduces\\nthe first-of-its-kind, smartphone portrait quality dataset. We\\nhope to create a new domain of application for IQA and to', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='push forward smartphone portrait photography. Our contri-\\nbutions are the following:\\n• A new dataset, PIQ23, consisting of 5116 single por-\\ntrait images, taken using 100 smartphone devices from\\n14 brands, and distributed across 50 different natural\\nscenes ( scene = fixed visual content ). We have ad-\\ndressed the ethical challenges involved in creating such\\na dataset, by obtaining from each individual depicted\\nin the dataset a signed and informed agreement, mak-\\ning it the only IQA dataset with such legal and ethical\\ncharacteristics, as far as we know.\\n• A large IQA experiment controlled in a laboratory en-\\nvironment with fixed viewing conditions. Using pair-\\nwise comparisons (PWC) and following carefully de-\\nsigned guidelines, we gather opinions for each scene,\\nfrom over 30 image quality experts (professional pho-\\ntographers and image quality experts) on three at-\\ntributes related to portrait quality: face detail preser-\\nvation, face target exposure, and overall portrait image', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='quality.\\n• An in-depth statistical analysis method that allows us\\nto evaluate the precision and consistency of the labels\\nas well as the difficulty of the IQA task. This is par-\\nticularly important given the fact that image quality la-bels are heavily affected by subjectivity, disagreement\\nbetween observers, and the number of annotations.\\n• An extensive comparison between multiple BIQA\\nmodels and a simple new method combining scene se-\\nmantic information with quality features to strengthen\\nimage quality prediction on PIQ23.\\n2. Related work\\n2.1. BIQA\\nThe main goal of blind IQA (BIQA) is to predict image\\nquality without requiring a pristine reference image. We\\nreview the datasets that already exist in this domain as well\\nas the BIQA computational algorithms.\\nBIQA datasets. Early datasets like LIVE [ 50], CSIQ\\n[29], TID [ 44,45] and BAPPS [ 66] consist of noise-free\\nimages processed with several artificial distortions. These\\ndistortions aim to describe image compression or transmis-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='sion scenarios and most of them fail to capture the com-\\nplexity of modern smartphone camera systems, with non-\\nlinear processing pipelines. Recent ªin-the-wildº datasets\\nsuch as CLIVE [ 13], KonIQ10k [ 21] and PaQ-2-PiQ [ 61]\\nconsist of media-gathered images with more complex mix-\\ntures of distortions closer to real-world images. However,\\ndue to their wild nature and uncontrolled labeling environ-\\nment, they do not form a strong background to evaluate the\\nquality of digital cameras, which we are most interested in.\\nAs an early effort on this topic, Virtanen et al. [56] have\\ndeveloped a database (CID2013) that spans 8 visual scenes\\nwith 79 digital cameras. In recent work, Zhu et al . [70]\\nprovide a smartphone IQA dataset (SCPQD2020) of 1800\\nimages shot with 15 devices on 120 visual scenes. They an-\\nnotate the database in a well-controlled laboratory, by three\\nimage quality experts. Fang et al. published SPAQ [ 9], a\\nsmartphone IQA dataset with 11125 images shot with 66\\n9969', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='9969\\ndevices. Both datasets provide multiple attribute evalua-\\ntions and scene categories. They include generic visual con-\\ntent and do not deal with PQA. While SCPQD2020 lacks\\nin the number of observers, SPAQ relies on resized images\\nwhich heavily degrades the quality. All previously men-\\ntioned datasets, except TID2013 [ 44] and BAPPS [ 66], rely\\non rating systems (MOS), and do not pay close attention to\\nthe difficulty of cross-content observations. In PIQ23, we\\nprovide 50 scenes, each annotated independently. We col-\\nlect opinions from over 30 image quality experts by pair-\\nwise comparisons, which has been shown to be more con-\\nsistent in IQA experiments [ 34,43]. We also analyze the\\nuncertainty and consistency of our annotations through a\\nnew statistical analysis method.\\nBIQA methods. BIQA can be separated into classical and\\ndeep learning approaches. Early learning-based approaches\\n[14,36,39,48,60] use a combination of hand-crafted statisti-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='cal features (natural scene statistics) to train shallow regres-\\nsors ( e.g. SVR). Other approaches try to estimate the qual-\\nity without the need for training [ 37,57,64]. These methods\\nperform relatively poorly on modern IQA datasets, as they\\ndo not fully reflect the human perception of realistic distor-\\ntions [ 13,67]. Consequently, deep BIQA models have been\\nsurging in the last decade. Multiple convolutional neural\\nnetworks (CNN) based methods [ 24,27,67] have demon-\\nstrated solid performance on modern datasets. Zhang et al.\\n[69] address the problem of uncertainty in IQA and present\\na method to simultaneously train on multiple datasets us-\\ning image pairs as training samples. Su et al. [52] try to\\nseparate semantic features from image quality features by\\ntraining an adaptive hyper network that captures semantic\\ninformation. Recent works that adopt transformer architec-\\ntures [ 15,25,59,62] to extract global quality information,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='have shown impressive performances on IQA datasets. Be-\\ncause of the per-scene annotation structure of our dataset,\\nwe adopt a semantics-aware multitasking method to adapt\\nthe scale and features to the input scene.\\n2.2. PQA\\nDespite the lack of portrait quality assessment (PQA) re-\\nsearch, solely focusing on evaluating portrait quality, we\\nstill recognize the importance of face IQA (FIQA). FIQA\\naims to assess the quality of face images to boost the per-\\nformance of face recognition algorithms [ 1,17,20,31,41,47,\\n49,58]. The closest FIQA work to PIQ23 is that of Zhang et\\nal. [65], where they have developed a dataset to objectively\\nevaluate the illumination quality of a face image. Redi et\\nal. [46] define a set of attributes to evaluate the ªbeautyº of\\nthe portrait. Kanafusa et al. [23] propose a method to de-\\nfine a standard portrait image, which can be later used to\\nevaluate color rendering and other attributes between cross-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='media. In this work, we can see a first attempt to use a stan-dard portrait as a subject for IQA. Chahine et al. [40] pro-\\nposed the first approach to evaluate specific face attributes\\nas a separate metric for PQA on realistic mannequins. Fi-\\nnally, Liang et al. [30] have developed a large-scale portrait\\nphoto retouching dataset, with multiple use cases and cam-\\neras. To the best of our knowledge, PIQ23 is the first smart-\\nphone PQA dataset, with a variety of visual scenes, legal\\nvalidation, and expert annotations.\\n2.3. Domain shift\\nThe annotation strategies and image content can vary\\nwidely between different IQA datasets. Hence, their re-\\nspective quality scales are usually relative and independent.\\nWith this characteristic, we encounter a problem known as\\ndomain shift [ 53,63,68,69]. Since quality scales are rela-\\ntive, similar scores may not indicate the same level of per-\\nceptual quality across different datasets. This can lead to', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='confusion when attempting to learn from multiple sources.\\nAs a result, understanding image semantics is necessary.\\nCurrent BIQA models implicitly try to learn semantics and\\nquality simultaneously. However, it is extremely difficult to\\nmerge these two problems, as they seem to be contradictory\\n[9,26]. Some papers try to solve this problem using multi-\\ntask learning [ 9,22,53,63]. Explicitly separating semantic\\ninformation from quality is not well represented in previous\\nworks. Su et al. [52] propose HyperIQA, a self-adaptive\\nhyper network that implicitly extracts semantic information\\nand adapts the quality prediction accordingly. The hyper\\nnetwork, however, is not trained to predict categories ex-\\nplicitly. Since PIQ23 consists of multiple relative content-\\ndependent scales, we propose to combine multitasking with\\nHyperIQA in order to adapt the quality scale of each scene\\nbased on semantic understanding.\\n3. PIQ23\\n3.1. Dataset details', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Legal aspects. We believe that unrestricted access to\\nPIQ23 for public research is crucial. Accordingly, we have\\ntaken steps to address any potential legal obstacles that may\\nobstruct this access. All individuals in the photos have given\\nexplicit permission for image rights via signed transfer and\\nreceived a privacy notice detailing how their images will\\nbe processed. Also, to ensure the effectiveness of people’s\\nrights, we have tagged each photo with a unique identifier\\nassigned to each person by using a face clustering algo-\\nrithm. This pseudonymization technique prevents access to\\nindividuals’ names by dataset users. Finally, we contractu-\\nally require all dataset users to comply with relevant data\\nprotection laws, including the GDPR.\\nDataset properties. We have constructed PIQ23 with the\\nintent of reducing annotation biases and covering a variety\\n9970\\nof common real-life scenarios. To achieve this goal, we\\nhave broken down the factors affecting the quality of a por-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='trait image. We consider lighting to be a primary element\\ninfluencing the quality. Hence, we have separated lighting\\nconditions into four groups: outdoor, indoor, low light, and\\nnight. Also, we have paid attention to lighting homogene-\\nity, which describes the reflection of the light on the subject\\n(e.g. front light, side light, backlight). The characteristics of\\nthe subject, such as age, skin tone, gender, subject position,\\nframing, face orientation, movement, and subject-to-lens\\ndistance play an equally important role. Our skin tone ruler\\nis based on the Fitzpatrick skin type (FST) [ 12]. We have\\ntried to cover a sizeable chunk of smartphone devices and\\nbrands utilized over the past decade. Additionally, we have\\nincluded diverse smartphone camera lens focal lengths such\\nas zoom, wide, and selfie along with distinct camera modes\\nsuch as night and bokeh. Furthermore, we have contem-\\nplated the possibility of augmenting our dataset with high-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='quality images sourced from DSLR cameras. Nonetheless,\\nwe have found through our experimentation that artificially\\ndistorting DSLR images to ensure comparability with pho-\\ntos taken by smartphones is a challenging task. As a result,\\nwe have excluded DSLR cameras from our dataset.\\nTo comply with the previous description, we have de-\\nsigned a collection of 50 distinct portrait scenarios (re-\\nferred to as ªscenesº), captured in predetermined locations\\nthat encompass a diverse range of factors (see Fig. 1(a)).\\nThe dataset images were taken with about 100 smartphones\\n(2014- 2022) from 14 brands and different price segments.\\nWe have collected around 5116 images, averaging 100 im-\\nages per scene. We note that PIQ23 was subsampled from a\\nlarger dataset that was collected over a long period of time\\n(a couple of years) and is a result of cumulative efforts in\\nengineering and photography. We, therefore, believe in its\\ncapacity to cover a broad spectrum of smartphone photogra-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='phy. More information about the PIQ23 characteristics can\\nbe found in the supplementary material.\\n3.2. Portrait quality assessment\\nPortrait quality attributes. In a portrait, most attention\\nis given to the person depicted, which is known as the\\nhuman region priority (HRP) [ 30]. Portrait quality may\\nvary significantly depending on the application. For ex-\\nample, Redi et al. [46] try to define all the characteristics\\nto capture the ’beauty’ of a portrait. Quality in this case\\nis strongly correlated with beauty and aesthetics. In other\\napplications, such as FIQA [ 49], quality assessment is a\\nmeasure of utility, to filter out poor quality faces from face\\nrecognition systems. Neither application totally aligns with\\nPQA [ 40]. Thus, we intend to broaden the research on PQA\\nby studying a preliminary group of three attributes: face\\ndetail preservation, face target exposure, and overall image', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='quality. Additionally, we have conducted a study concern-ing a fourth attribute, namely global color quality. However,\\ndue to the difficulty in annotating this attribute through pair-\\nwise comparisons on different content, we have decided to\\nexclude it from our dataset (Fig. 3). The annotation guide-\\nlines can be found in the supplementary material.\\nAnnotation strategy. Perception-based IQA experiments\\npresent a high degree of difficulty and are usually subjec-\\ntive. Opinions can vary widely depending on multiple fac-\\ntors: viewing conditions, the observer’s cultural and pro-\\nfessional backgrounds, image content, etc. The objective\\nof PIQ23 is to deliver image quality annotations obtained\\n(as much as possible) from impartial and unbiased observa-\\ntions. To maximize objectivity and consistency, we propose\\ntwo elementary steps:\\n• First, we have chosen to annotate each scene separately\\nusing a forced-choice pairwise comparison approach\\n(PWC). Combined with the active sampling technique', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='proposed in [ 35], we have been able to reach good an-\\nnotation consistency with a minimal number of com-\\nparisons (see Sec. 4).\\n• Second, we have fixed the region of interest (ROI) for\\neach attribute, as shown in Fig. 1(b). For details\\npreservation and target exposure, we have extracted\\nthe face area using RetinaFace [ 7]. We have then up-\\nscaled it using standard bicubic upsampling to a ref-\\nerence resolution of about 4.5 megapixels with a fixed\\naspect ratio. For color and overall attributes, we have\\nresized the images to an approximate Full HD resolu-\\ntion (about 2.5 megapixels) while keeping the original\\naspect ratio (i.e. portrait or landscape).\\nExperiment details. We have reached out to professional\\nphotographers and experts with a solid background in pho-\\ntography and image quality to help us annotate the dataset.\\nThe opinions of more than 30 experts were gathered using\\nan internal PWC tool. Observers were asked to select the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='best out of two images, following the guidelines described\\nin the supplementary material. We have adapted our set-\\ntings so that the viewing conditions are aligned with that of\\na human eye, with a cutoff frequency νcut= 30cpd. Hence,\\nwe have used a BenQ 32º 4k monitor with a pixel pitch\\nof 0.185, and we have fixed the eye-to-screen distance at\\n65cm. We have calibrated the display to standard sRGB set-\\ntings (D65 white point with luminance ≥75cd/m2with no\\ndirect illumination of the screen and a background illumina-\\ntion with a lighting panel set to D65 / 15% for reducing eye\\nstress). We have also converted all images in DCI-P3 color\\nspace to sRGB. We have kept the sessions short, around five\\nminutes per attribute, in order to reduce fatigue and stress on\\nthe observers. The annotation procedure took around eight\\n9971\\nmonths. For each scene and each attribute, we have col-\\nlected around 4k pairwise comparisons, a total of 600k data', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='points. Though, for a limited number of comparisons, given\\nthe subjectivity of the task, noise and outliers are commonly\\nencountered. In Sec. 4, we present a new statistical analysis\\nmethod to rectify this noise.\\n4. Statistical analysis\\nWe present a new approach to quantifying uncertainty in\\nIQA experiments. We recall in Sec. 4.1how quality scores\\nare extracted from a PWC experiment and how uncertainty\\ncan be estimated using bootstrapped confidence intervals.\\nWe then introduce in Sec. 4.2a new statistical analysis strat-\\negy to go beyond the calculation of confidence intervals.\\nThe complete pipeline is illustrated in Fig. 2\\n4.1. Psychometric scaling and confidence intervals\\nPsychometric scaling. Designing a PWC experiment re-\\nquires modeling the statistical distribution of the image\\nquality. Commonly, the quality of an image is described by\\nthe Thurstone Case V observer model [ 5,54] as a Gaussian\\ndistribution N(µ,σ). The average µrepresents the actual', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='quality and σ2is its ªperceptualº variance across observa-\\ntions. The latter encompasses the intra-variance and inter-\\nvariance of the perceptual quality. The intra-variance repre-\\nsents the uncertainty of one observer when the observation\\nis repeated multiple times. The inter-variance represents the\\nuncertainty across multiple observers. Based on this formu-\\nlation, psychometric scaling methods [ 42,43] transform the\\ncomparison matrix Mconstructed from a PWC experiment,\\ninto a continuous scale of image scores representing the av-\\nerage opinions across multiple observers. The results are\\ntypically expressed in Just-Objectionable-Difference (JOD)\\nunits [ 43]. Two images are 1 JOD apart if 75% of ob-\\nservers choose one as better than the other. In our work, we\\nhave adopted the psychometric scaling method proposed by\\nMikhailiu et al. [35]. The authors propose an efficient ac-\\ntive pair selection technique via approximate message pass-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ing and information gain maximization, combined with the\\nTrueSkill scaling algorithm [ 19] to minimize the PWC ex-\\nperiment cost. Thus, Mis typically a very sparse matrix\\n(so-called incomplete design) with a limited number cof\\nnon-zero elements.\\nIQA limitations. The choice of image and observer sam-\\nples plays a critical role in the accuracy of the JOD scores.\\nFrom a statistical point of view, these samples are taken\\nfrom infinitely large populations of images and observers\\nrespectively. When sampling images of similar quality, for\\nexample, the comparison becomes harder, requiring a corre-\\nspondingly larger number of comparisons than a sample of\\nimages with distinguishable quality differences. Similarly,a sample of inexperienced observers generally leads to nois-\\nier annotations, requiring more observers than a sample of\\nexperts. In addition, psychometric scaling algorithms intro-\\nduce an estimation error that is inversely proportional to the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='size of the data. In conclusion, estimating the difficulty of\\nthe comparison task (linked to image sampling), the qual-\\nity of the experiment (linked to observer sampling), and the\\nprecision of the psychometric scaling algorithm, contribute\\nto what we call the experiment error, which is the image\\nJOD score estimation error.\\nEstimating the uncertainty in IQA. One way of quanti-\\nfying the experiment error is by calculating the confidence\\ninterval (CI) of the JOD scores. The original formulation of\\nthe CI does not directly apply to PWC experiments, since it\\ndoes not take into account the error introduced by the image\\nsampling process [ 38]. A practical alternative for comput-\\ning CIs is bootstrapping [ 8]. We follow the approach pro-\\nposed in [ 42] and resort to the percentile method of boot-\\nstrapping [ 6]. We repeatedly generate JOD scores by sam-\\npling, with replacement, the observer comparison matrices,\\neach of which is a unique opinion on all images. The CI', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='boundaries for each image are then defined as the 2.5th and\\n97.5th percentiles of the JOD scores (Fig. 2(a)).\\n4.2. JOD clustering via confidence intervals\\nCI limitations. Confidence intervals represent well the\\nsample mean error for independent variables and samples,\\nbut in PWC these conditions are not achieved [ 42]. The psy-\\nchometric scaling algorithm calculates all the JOD scores at\\nthe same time. This means that every change in the com-\\nparison matrix will simultaneously affect the scores of all\\nimages. This behavior makes the image JOD scores some-\\nwhat interdependent, which is not apparent in the CIs. To\\nidentify which images have a significant difference in qual-\\nity, we need to analyze the overlap of their confidence in-\\ntervals. But how to quantify this overlap? Do we consider\\nthe overlap of 20%, 30%, or 60% to be significant? What\\nto do in case of multiple overlaps? We address these issues\\nby combining two techniques. We first cluster the images', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='using their CIs, then, we use variance analysis to identify\\nwhich images have significant quality differences.\\nPreliminary clustering. Overlapping CIs may indicate\\npossible quality similarity, so we can use this information\\nto group the images. To define the distance between inter-\\nvals, we consider each CI as a point C(x,y)in the subspace\\n{(x,y)∈R2|y−x >= 0}wherexis the lower bound\\nof the CI, and yis the upper bound of the CI. In this way,\\nwe can calculate the Euclidian distance between C1andC2.\\nWe then resort to the K-means algorithm [ 33] to define our\\npreliminary quality groups (Fig. 2(a-b)). We estimate the\\n9972\\nFigure 2. Diagram of the statistical analysis strategy used to estimate the uncertainty in a PWC experiment. (a) Given a PWC matrix, we\\ngenerate confidence intervals (CIs) using percentile bootstrapping. (a-b) We then apply the K-means algorithm to the ª2d representationº of', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the CIs to cluster the images into preliminary quality groups (b). (b-c) In each group, we apply RMANOV A to detect significant differences\\nbetween the JOD scores. (c) For groups with such differences, we construct a weighted undirected graph, where the weights consist of the\\np-value of paired t-tests between the image pairs. (c-d) Finally, we apply Louvain community detection to extract sub-clusters of similar\\nquality inside each group (d). Figures (b) and (d) represent the boxplots of the bootstrapped image scores generated in (a).\\nnumber of preliminary quality groups by dividing the total\\nJOD range by the median size of the CIs (Fig. 2(b)).\\n4.3. JOD clustering via variance analysis\\nVariance analysis. To estimate the significance of the CI\\noverlaps, we turn to the analysis of variance (ANOV A)\\n[10,11] and particularly repeated measures ANOV A\\n(RMANOV A) [ 18]. RMANOV A is a statistical significance\\ntest used to investigate the differences in mean scores of a', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='given continuous variable (called dependent variable), that\\nhas been ªrepeatedly testedº, on the same group of subjects,\\nunder three or more different conditions taken from a cate-\\ngorical variable (called the within-subject factor or the in-\\ndependent variable). In a PWC experiment, we interpret the\\nset of images as the independent variable and consider each\\nbootstrapped matrix (Sec. 4.1) as a subject that was tested\\non different conditions (one matrix ≡one subject). Finally,\\nsince the JOD scores are estimated from the same matrix,\\nwe consider them as measurements of the dependent vari-\\nable, that is, the image quality.\\nStatistical hypothesis. LetMbe the sparse comparison\\nmatrix defined in Sec. 4.1. LetX={Xi\\n1×n,i= 1,...,b},\\nwhereXi\\n1×n=/bracketleftbig\\nxi\\n1xi\\n2...xi\\nn/bracketrightbig\\n, be the set of score\\nvectors inferred from bcomparison matrices bootstrapped\\nfromM, andnthe number of images. We define the twohypotheses:\\n/braceleftBigg\\nH0: ¯x1= ¯x2=···= ¯xn;', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='H1:At least two means are different.(1)\\nwherexkrepresents the inferred average score of the image\\nIkfrom the PWC experiment. Refusing the null hypothe-\\nsisH0only guarantees that at least two image scores are\\ndifferent ( H1). Accepting H0means that all images in the\\ntest have indistinguishable quality. We apply the previous\\nhypothesis testing on each of the preliminary groups and\\ndeduce whether there is a significant difference between the\\nimages or not (Fig. 2(b-c)). We can identify two cases:\\n1.No significant difference has been found: we con-\\nsider in this case that all the images of the group have\\nthe same average score and variance.\\n2.A significant difference exists: we don’t know how\\nmany images are significantly different. In this case,\\nwe conduct a post hoc analysis, using paired t-tests\\n[51], at a confidence level of 0.95, on all the possible\\npairs in the given cluster.\\nSignificance graph. For the groups where the significant', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='difference exists, we create a weighted undirected graph by\\nweighting the connections between pairs with the p-value\\nof their corresponding paired t-tests (Fig. 2(c)). Then, we\\napply the Louvain community detection algorithm [ 2] to\\ngroup dense regions of nodes into the same ªcommunityº\\n9973\\nor cluster (Fig. 2(c-d)). With this method, we separate the\\ngraph into sub-clusters, then assign their average score and\\nvariance to the corresponding images (Fig. 2(d)).\\n4.4. Results and discussion\\nWe show the results of our statistical analysis on 20\\nscenes for the four attributes in Fig. 3, from which we can\\nmake several interesting observations. First, we note that\\nthe number of clusters and groups is correlated with the\\nJOD range and the median CI size (rows 1, 2). A wider JOD\\ninterval indicates a wider quality coverage, which implies a\\nhigher number of quality levels. Similarly, a smaller confi-\\ndence interval indicates that the images can be more easily', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='separated, which in turn implies a higher number of qual-\\nity levels. Second, we observe that the median CI size de-\\ncreases with the JOD range (row 3, left). This confirms our\\nhypothesis in Sec. 4.1that sampling images of close qual-\\nity makes the task more difficult. Finally, we note that detail\\npreservation and exposure have smaller CIs, while color has\\nthe largest, indicating a greater difficulty in annotating this\\nattribute (row 3, right), thus justifying its omission.\\n5. Blind image quality assessment with a tweak\\nBased on the PIQ23 dataset, we introduce a deep BIQA\\nmethod (SEM-HyperIQA) that adapts to the specific struc-\\nture of the dataset, where each scene has a separate quality\\nscale. We retrain several existing BIQA methods from the\\nliterature and compare them to our proposed approach.\\n5.1. Semantics aware IQA\\nThe PIQ23 dataset contains individually annotated\\nscenes, each with its own quality scale and unique con-\\ntent. This characteristic introduces a problem known as do-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='main shift (Sec. 2.3), involving both content-dependent and\\nannotation-dependent factors. This emphasizes the need to\\nunderstand the scene’s semantics and align the predicted\\nquality with its corresponding scale.\\nIn order to address the challenges of domain shift in\\nPIQ23, we propose SEM-HyperIQA, a solution that in-\\nvolves combining the HyperIQA architecture, which inte-\\ngrates semantic information, with multitasking, which al-\\nlows scene-specific rescaling. Based on the HyperIQA ar-\\nchitecture, we concatenate the semantic features of multi-\\nple random crops and feed them to a multi-layer perceptron\\n(MLP) that predicts the scene category for the respective\\nimage. We then feed the predicted category to a smaller\\nMLP that predicts a multiplier aiand offset bito adapt the\\npredicted quality score of each patch to its respective scene\\nscale, such as ˆqi=aiqi+bi, whereqiis the predicted\\nquality score of patch i(Fig. 4). The loss is the sum of the\\nℓ1-norm loss and the cross entropy.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='We also propose two other variants, SEM-HyperIQA-SO\\nand SEM-HyperIQA-CO. In the first variant, we omit the\\nFigure 3. Statistical analysis on 20 scenes for the four attributes.\\nFrom top to bottom row (shared x-axis): distribution of the number\\nof clusters (row 1) and preliminary groups (row 2) in terms of the\\nJOD range (left) and the median CI size (right). Row 3 displays\\nthe distribution of the median CI in terms of the JOD range (left),\\nas well as the median CI distribution per attribute (right).\\nscene category prediction and instead feed the scene infor-\\nmation directly to the MLP that rescales the predicted score.\\nIn the second variant, we omit the rescaling part and only\\nkeep the scene prediction. The two variants will help us ex-\\nplore the individual importance of scene-specific rescaling\\nand semantic prediction, respectively.\\n5.2. Performance evaluation\\nTraining strategy. We test different training configura-\\ntions for all the proposed methods and report the best re-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='sults. Specifically, we randomly sample 70% of the images\\nin PIQ23 for training and leave the rest for testing. We ran-\\ndomly crop the images to patches of one of the three follow-\\ning sizes: 672, 448, and 224. We use Adam stochastic op-\\ntimization [ 28] with different learning rates between 10−6\\nand10−4. We fix the training for 300 epochs and adopt\\na learning rate decay factor of 0.05 for every 10 epochs.\\nThe final image quality score is computed by averaging the\\nindividual patch scores. To evaluate the performance, we\\ncompute Spearman’s rank correlation coefficient (SRCC)\\nbetween the model outputs and the JOD scores. Since each\\nscene is annotated separately, we compute the correlation\\nover the scores for each individual scene and evaluate the\\nperformance as C=1\\ns/summationtexts\\ni=1Ci, wheres= number of\\nscenes,Ci= correlation for scene i.\\n9974\\nFigure 4. The SEM-HyperIQA architecture. We combine the semantic representation acquired by HyperIQA [ 52] for multiple patches to', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='predict the scene category. We then use the predicted category to rescale the patch quality. The image score is averaged across patches.\\n# Method Details Exposure Overall\\n1 BRISQUE [ 36] 0.323 0.307 0.192\\n2 NIQE [ 37] 0.378 0.265 0.298\\n3 ILNIQE [ 64] 0.353 0.312 0.214\\n4 DB-CNN [ 67] 0.628 ±0.07 0.635±0.06 0.555±0.07\\n5 HyperIQA [ 52] 0.649 ±0.08 0.706±0.04 0.611±0.06\\n6 MUSIQ [ 25] 0.671 ±0.07 0.725±0.04 0.589±0.07\\n7 SEM-HyperIQA 0.671 ±0.07 0.71±0.04 0.621±0.06\\n8 SEM-HyperIQA-SO 0.722±0.06 0.721±0.06 0.642±0.08\\n9 SEM-HyperIQA-CO 0.664 ±0.07 0.71±0.06 0.621±0.07\\nTable 1. Comparison of the baselines according to their average\\nscene Spearman’s rank correlation coefficient with the measured\\nJOD scores and their error margin across the scenes. As shown by\\nthe table, the deep learning methods tested perform significantly\\nbetter than their classical counterparts on PIQ23.\\nBaseline methods. We compare SEM-HyperIQA with\\nexisting BIQA models, including BRISQUE [ 36], NIQE', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[37], ILNIQE [ 64], DB-CNN [ 67], HyperIQA [ 52] and\\nMUSIQ [ 25]. We train these models on PIQ23 using their\\nofficial implementations. NIQE and ILNIQE do not re-\\nquire any training. DB-CNN and MUSIQ are pre-trained on\\nLIVE Challenge and PaQ-2-PiQ, respectively. HyperIQA is\\npre-trained on ImageNet. Results are shown in Table 1.\\nDiscussion. From Table 1we can make the following ob-\\nservations. First, the deep learning methods tested (4-9) per-\\nform better than their classical counterparts (1-3), indicat-\\ning a difficulty to adapt to high-resolution images, scene-\\nspecific scales, and attribute-specific annotations. Zhu et\\nal. [70] have demonstrated the ineffectiveness of such meth-\\nods when the annotations do not represent an overall sub-\\njective evaluation of the quality, Second, the proposed\\nSEM-HyperIQA method improves upon the original Hyper-\\nIQA, which indicates the effectiveness of scene semantics\\nand multitasking in quality prediction, especially for sepa-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='rate scene scales. Third, SEM-HyperIQA-SO with scene-\\nspecific rescaling achieves the best performance. It notably\\nenhances the detail preservation attribute, possibly due tothe limited information available in face crops for scene\\nanalysis. Therefore, semantic information cannot be fully\\nutilized and we are better off using scene-specific rescaling\\nonly. Fourth, we note that deep BIQA models perform sig-\\nnificantly better for detail preservation and exposure than\\noverall, which directly reflects this task’s difficulty and the\\nuncertainty of the annotations, as discussed in Sec. 4.4.\\n6. Conclusion\\nWe have presented PIQ23, a new dataset for portrait\\nquality assessment with a wide variety of smartphone cam-\\neras and use cases, which has been annotated by image qual-\\nity experts using pairwise comparisons. We have shown the\\nimportance of identifying the uncertainty in the annotations\\nby providing a new statistical analysis method to cluster the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='quality scale into consistent levels of quality. Finally, we\\nadopt a training strategy and a deep neural network archi-\\ntecture that adapts to the high-resolution images of PIQ23\\nand profits from semantic information and multitasking, in\\norder to adjust to the per-scene quality scaling of the dataset.\\nOur results have shown the necessity and effectiveness of\\nquality scale quantification and clustering of similar quality\\nimages to contain annotation uncertainty, as well as the im-\\nportance of semantic information in training IQA models.\\nWe believe that this work will be the foundation for a new\\narea of application of IQA for portrait images, as well as for\\na higher caliber of annotations in IQA datasets.\\nAcknowledgments\\nThis work was funded in part by the French govern-\\nment under the management of Agence Nationale de la\\nRecherche as part of the ªInvestissements d’avenirº pro-\\ngram, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Insti-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tute), the Louis Vuitton/ENS chair in artificial intelligence\\nand the Inria/NYU collaboration. NC was supported in part\\nby a DXOMARK/PRAIRIE CIFRE Fellowship. We thank\\nDXOMARK engineers and photographs for their time in-\\nvestment and fruitful discussions about image quality.\\n9975\\nReferences\\n[1] Lacey Best-Rowden and Anil K Jain. Learning face image\\nquality from human assessments. IEEE Transactions on In-\\nformation Forensics and Security , 13(12):3064±3077, 2018.\\n3\\n[2] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lam-\\nbiotte, and Etienne Lefebvre. Fast unfolding of communities\\nin large networks. Journal of statistical mechanics: theory\\nand experiment , 2008(10):P10008, 2008. 6\\n[3] Martin Cadik, Michael Wimmer, Laszlo Neumann, and\\nAlessandro Artusi. Image attributes and quality for evalu-\\nation of tone mapping operators. In National Taiwan Uni-\\nversity . Citeseer, 2006. 2\\n[4] Fr ÂedÂeric Cao, Frederic Guichard, and Herv Âe Hornung. Mea-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='suring texture sharpness of a digital camera. In Digital Pho-\\ntography V , volume 7250, pages 146±153. SPIE, 2009. 1\\n[5] Roger R Davidson and Peter H Farquhar. A bibliography on\\nthe method of paired comparisons. Biometrics , pages 241±\\n252, 1976. 5\\n[6] Anthony Christopher Davison and David Victor Hinkley.\\nBootstrap methods and their application . Number 1. Cam-\\nbridge university press, 1997. 5\\n[7] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kot-\\nsia, and Stefanos Zafeiriou. Retinaface: Single-shot multi-\\nlevel face localisation in the wild. In Proceedings of\\nthe IEEE/CVF conference on computer vision and pattern\\nrecognition , pages 5203±5212, 2020. 4\\n[8] Bradley Efron. Bootstrap methods: another look at the\\njackknife. In Breakthroughs in statistics , pages 569±593.\\nSpringer, 1992. 5\\n[9] Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and Zhou\\nWang. Perceptual quality assessment of smartphone photog-\\nraphy. In Proceedings of the IEEE/CVF Conference on Com-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='puter Vision and Pattern Recognition , pages 3677±3686,\\n2020. 1,2,3\\n[10] Ronald A Fisher. Xv.Ðthe correlation between relatives on\\nthe supposition of mendelian inheritance. Earth and Envi-\\nronmental Science Transactions of the Royal Society of Ed-\\ninburgh , 52(2):399±433, 1919. 6\\n[11] Ronald Aylmer Fisher et al. 014: On theº probable errorº\\nof a coefficient of correlation deduced from a small sample.\\n1921. 6\\n[12] Thomas B Fitzpatrick. The validity and practicality of sun-\\nreactive skin types i through vi. Archives of dermatology ,\\n124(6):869±871, 1988. 4\\n[13] Deepti Ghadiyaram and Alan C Bovik. Massive online\\ncrowdsourced study of subjective and objective picture qual-\\nity.IEEE Transactions on Image Processing , 25(1):372±387,\\n2015. 1,2,3\\n[14] Deepti Ghadiyaram and Alan C Bovik. Perceptual quality\\nprediction on authentically distorted images using a bag of\\nfeatures approach. Journal of vision , 17(1):32±32, 2017. 1,\\n3\\n[15] S Alireza Golestaneh, Saba Dadsetan, and Kris M Kitani.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='No-reference image quality assessment via transformers, rel-\\native ranking, and self-consistency. In Proceedings of theIEEE/CVF Winter Conference on Applications of Computer\\nVision , pages 1220±1230, 2022. 1,3\\n[16] Yann Gousseau and Franc Ëois Roueff. Modeling occlusion\\nand scaling in natural images. Multiscale Modeling & Simu-\\nlation , 6(1):105±134, 2007. 1\\n[17] P Grother, Austin Hom, Mei Ngan, and Kayee Hanaoka. On-\\ngoing face recognition vendor test (frvt) part 5: Face image\\nquality asssessment. Draft NIST Interagency Report , 2020.\\n3\\n[18] Ralitza Gueorguieva and John H Krystal. Move over anova:\\nprogress in analyzing repeated-measures data andits reflec-\\ntion in papers published in the archives of general psychiatry.\\nArchives of general psychiatry , 61(3):310±317, 2004. 6\\n[19] Ralf Herbrich, Tom Minka, and Thore Graepel. Trueskill™:\\na bayesian skill rating system. Advances in neural informa-\\ntion processing systems , 19, 2006. 5', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[20] Javier Hernandez-Ortega, Javier Galbally, Julian Fierrez,\\nRudolf Haraksim, and Laurent Beslay. Faceqnet: Quality\\nassessment for face recognition based on deep learning. In\\n2019 International Conference on Biometrics (ICB) , pages\\n1±8. IEEE, 2019. 3\\n[21] Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar Saupe.\\nKoniq-10k: An ecologically valid database for deep learning\\nof blind image quality assessment. IEEE Transactions on\\nImage Processing , 29:4041±4056, 2020. 1,2\\n[22] Chen-Hsiu Huang and Ja-Ling Wu. Multi-task deep cnn\\nmodel for no-reference image quality assessment on smart-\\nphone camera photos. arXiv preprint arXiv:2008.11961 ,\\n2020. 3\\n[23] Kunihiko Kanafusa, Keiichi Miyazaki, Hiroshi Umemoto,\\nKazuhiko Takemura, and Hitoshi Urabe. A standard portrait\\nimage and image quality assessment. In IS AND TS PICS\\nCONFERENCE , pages 317±320. SOCIETY FOR IMAG-\\nING SCIENCE & TECHNOLOGY , 2000. 3\\n[24] Le Kang, Peng Ye, Yi Li, and David Doermann. Convolu-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tional neural networks for no-reference image quality assess-\\nment. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition , pages 1733±1740, 2014. 1,3\\n[25] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and\\nFeng Yang. Musiq: Multi-scale image quality transformer.\\nInProceedings of the IEEE/CVF International Conference\\non Computer Vision , pages 5148±5157, 2021. 1,3,8\\n[26] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task\\nlearning using uncertainty to weigh losses for scene geome-\\ntry and semantics. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition , pages 7482±7491,\\n2018. 3\\n[27] Jongyoo Kim and Sanghoon Lee. Fully deep blind image\\nquality predictor. IEEE Journal of selected topics in signal\\nprocessing , 11(1):206±220, 2016. 1,3\\n[28] Diederik P Kingma and Jimmy Ba. Adam: A method for\\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\\n2014. 7\\n[29] Eric Cooper Larson and Damon Michael Chandler. Most', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='apparent distortion: full-reference image quality assessment\\nand the role of strategy. Journal of electronic imaging ,\\n19(1):011006, 2010. 2\\n9976\\n[30] Jie Liang, Hui Zeng, Miaomiao Cui, Xuansong Xie, and\\nLei Zhang. Ppr10k: A large-scale portrait photo retouch-\\ning dataset with human-region mask and group-level consis-\\ntency. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition , pages 653±661, 2021.\\n3,4\\n[31] Zhang Lijun, Shao Xiaohu, Yang Fei, Deng Pingling, Zhou\\nXiangdong, and Shi Yu. Multi-branch face quality assess-\\nment for face recognition. In 2019 IEEE 19th International\\nConference on Communication Technology (ICCT) , pages\\n1659±1664. IEEE, 2019. 3\\n[32] Christian Loebich, Dietmar Wueller, Bruno Klingen, and\\nAnke Jaeger. Digital camera resolution measurements using\\nsinusoidal siemens stars. In Digital Photography III , volume\\n6502, pages 214±224. SPIE, 2007. 1\\n[33] J MacQueen. Classification and analysis of multivariate ob-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='servations. In 5th Berkeley Symp. Math. Statist. Probability ,\\npages 281±297, 1967. 5\\n[34] Rafaø K Mantiuk, Anna Tomaszewska, and Radosøaw Man-\\ntiuk. Comparison of four subjective methods for image qual-\\nity assessment. In Computer graphics forum , volume 31,\\npages 2478±2491. Wiley Online Library, 2012. 3\\n[35] Aliaksei Mikhailiuk, Clifford Wilmot, Maria Perez-Ortiz,\\nDingcheng Yue, and Rafaø K Mantiuk. Active sampling\\nfor pairwise comparisons via approximate message passing\\nand information gain maximization. In 2020 25th Inter-\\nnational Conference on Pattern Recognition (ICPR) , pages\\n2559±2566. IEEE, 2021. 4,5\\n[36] Anish Mittal, Anush Krishna Moorthy, and Alan Con-\\nrad Bovik. No-reference image quality assessment in the\\nspatial domain. IEEE Transactions on image processing ,\\n21(12):4695±4708, 2012. 1,3,8\\n[37] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Mak-\\ning a ªcompletely blindº image quality analyzer. IEEE Sig-\\nnal processing letters , 20(3):209±212, 2012. 1,3,8', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[38] Ethan D Montag. Louis leon thurstone in monte carlo: creat-\\ning error bars for the method of paired comparison. In Image\\nQuality and System Performance , volume 5294, pages 222±\\n230. SPIE, 2003. 5\\n[39] Anush Krishna Moorthy and Alan Conrad Bovik. Blind\\nimage quality assessment: From natural scene statistics to\\nperceptual quality. IEEE transactions on Image Processing ,\\n20(12):3350±3364, 2011. 1,3\\n[40] Chahine Nicolas and Belkarfa Salim. Portrait quality assess-\\nment using multi-scale cnn. In London Imaging Meeting ,\\nvolume 2021, pages 5±10. Society for Imaging Science and\\nTechnology, 2021. 1,2,3,4\\n[41] Fu-Zhao Ou, Xingyu Chen, Ruixin Zhang, Yuge Huang,\\nShaoxin Li, Jilin Li, Yong Li, Liujuan Cao, and Yuan-Gen\\nWang. Sdd-fiqa: unsupervised face image quality assess-\\nment with similarity distribution distance. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 7670±7679, 2021. 3\\n[42] Maria Perez-Ortiz and Rafal K Mantiuk. A practical guide', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and software for analysing pairwise comparison experi-\\nments. arXiv preprint arXiv:1712.03686 , 2017. 5\\n[43] Maria Perez-Ortiz, Aliaksei Mikhailiuk, Emin Zerman,\\nVedad Hulusic, Giuseppe Valenzise, and Rafaø K Mantiuk.From pairwise comparisons and rating to a unified quality\\nscale. IEEE Transactions on Image Processing , 29:1139±\\n1151, 2019. 3,5\\n[44] Nikolay Ponomarenko, Lina Jin, Oleg Ieremeiev, Vladimir\\nLukin, Karen Egiazarian, Jaakko Astola, Benoit V ozel,\\nKacem Chehdi, Marco Carli, Federica Battisti, et al. Im-\\nage database tid2013: Peculiarities, results and perspectives.\\nSignal processing: Image communication , 30:57±77, 2015.\\n2,3\\n[45] Nikolay Ponomarenko, Vladimir Lukin, Alexander Zelen-\\nsky, Karen Egiazarian, Marco Carli, and Federica Battisti.\\nTid2008-a database for evaluation of full-reference visual\\nquality assessment metrics. Advances of Modern Radioelec-\\ntronics , 10(4):30±45, 2009. 2\\n[46] Miriam Redi, Nikhil Rasiwasia, Gaurav Aggarwal, and Ale-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='jandro Jaimes. The beauty of capturing faces: Rating the\\nquality of digital portraits. In 2015 11th IEEE International\\nConference and Workshops on Automatic Face and Gesture\\nRecognition (FG) , volume 1, pages 1±8. IEEE, 2015. 1,3,4\\n[47] Jacob Rose and Thirimachos Bourlai. Deep learning based\\nestimation of facial attributes on challenging mobile phone\\nface datasets. In Proceedings of the 2019 IEEE/ACM Inter-\\nnational Conference on Advances in Social Networks Analy-\\nsis and Mining , pages 1120±1127, 2019. 3\\n[48] Michele A Saad, Alan C Bovik, and Christophe Charrier.\\nBlind image quality assessment: A natural scene statistics\\napproach in the dct domain. IEEE transactions on Image\\nProcessing , 21(8):3339±3352, 2012. 1,3\\n[49] Torsten Schlett, Christian Rathgeb, Olaf Henniger, Javier\\nGalbally, Julian Fierrez, and Christoph Busch. Face image\\nquality assessment: A literature survey. ACM Computing\\nSurveys (CSUR) , 54(10s):1±49, 2022. 2,3,4', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[50] Hamid R Sheikh, Muhammad F Sabir, and Alan C Bovik.\\nA statistical evaluation of recent full reference image quality\\nassessment algorithms. IEEE Transactions on image pro-\\ncessing , 15(11):3440±3451, 2006. 2\\n[51] Student. The probable error of a mean. Biometrika , pages\\n1±25, 1908. 6\\n[52] Shaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin Ge,\\nJinqiu Sun, and Yanning Zhang. Blindly assess image qual-\\nity in the wild guided by a self-adaptive hyper network. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition , pages 3667±3676, 2020. 1,3,\\n8\\n[53] Wei Sun, Xiongkuo Min, Guangtao Zhai, and Siwei Ma.\\nBlind quality assessment for in-the-wild images via hierar-\\nchical feature fusion and iterative mixed database training.\\narXiv preprint arXiv:2105.14550 , 2021. 3\\n[54] Louis L Thurstone. A law of comparative judgment. Psy-\\nchological review , 101(2):266, 1994. 5\\n[55] Oliver van Zwanenberg, Sophie Triantaphillidou, Robin', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Jenkin, and Alexandra Psarrou. Edge detection techniques\\nfor quantifying spatial imaging system performance and im-\\nage quality. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition Workshops , pages\\n0±0, 2019. 1\\n[56] Toni Virtanen, Mikko Nuutinen, Mikko Vaahteranoksa,\\nPirkko Oittinen, and Jukka H Èakkinen. Cid2013: A database\\n9977\\nfor evaluating no-reference image quality assessment algo-\\nrithms. IEEE Transactions on Image Processing , 24(1):390±\\n402, 2014. 1,2\\n[57] Wufeng Xue, Lei Zhang, and Xuanqin Mou. Learning with-\\nout human scores for blind image quality assessment. In\\nProceedings of the IEEE conference on computer vision and\\npattern recognition , pages 995±1002, 2013. 1,3\\n[58] Fei Yang, Xiaohu Shao, Lijun Zhang, Pingling Deng, Xi-\\nangdong Zhou, and Yu Shi. Dfqa: Deep face image qual-\\nity assessment. In International Conference on Image and\\nGraphics , pages 655±667. Springer, 2019. 3\\n[59] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang.\\nManiqa: Multi-dimension attention network for no-reference\\nimage quality assessment. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition ,\\npages 1191±1200, 2022. 1,3\\n[60] Peng Ye, Jayant Kumar, Le Kang, and David Doermann.\\nUnsupervised feature learning framework for no-reference\\nimage quality assessment. In 2012 IEEE conference on\\ncomputer vision and pattern recognition , pages 1098±1105.\\nIEEE, 2012. 1,3\\n[61] Zhenqiang Ying, Haoran Niu, Praful Gupta, Dhruv Maha-\\njan, Deepti Ghadiyaram, and Alan Bovik. From patches to\\npictures (paq-2-piq): Mapping the perceptual space of pic-\\nture quality. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pages 3575±\\n3585, 2020. 1,2\\n[62] Junyong You and Jari Korhonen. Transformer for image\\nquality assessment. In 2021 IEEE International Conference\\non Image Processing (ICIP) , pages 1389±1393. IEEE, 2021.\\n1,3', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='1,3\\n[63] Emin Zerman, Giuseppe Valenzise, and Frederic Dufaux. An\\nextensive performance evaluation of full-reference hdr image\\nquality metrics. Quality and User Experience , 2(1):1±16,\\n2017. 3\\n[64] Lin Zhang, Lei Zhang, and Alan C Bovik. A feature-enriched\\ncompletely blind image quality evaluator. IEEE Transactions\\non Image Processing , 24(8):2579±2591, 2015. 1,3,8\\n[65] Lijun Zhang, Lin Zhang, and Lida Li. Illumination quality\\nassessment for face images: A benchmark and a convolu-\\ntional neural networks based model. In International Con-\\nference on Neural Information Processing , pages 583±593.\\nSpringer, 2017. 3\\n[66] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\\nman, and Oliver Wang. The unreasonable effectiveness of\\ndeep features as a perceptual metric. In Proceedings of the\\nIEEE conference on computer vision and pattern recogni-\\ntion, pages 586±595, 2018. 2,3\\n[67] Weixia Zhang, Kede Ma, Jia Yan, Dexiang Deng, and Zhou', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Wang. Blind image quality assessment using a deep bilinear\\nconvolutional neural network. IEEE Transactions on Cir-\\ncuits and Systems for Video Technology , 30(1):36±47, 2018.\\n1,3,8\\n[68] Weixia Zhang, Kede Ma, Guangtao Zhai, and Xiaokang\\nYang. Learning to blindly assess image quality in the lab-\\noratory and wild. In 2020 IEEE International Conference on\\nImage Processing (ICIP) , pages 111±115. IEEE, 2020. 3[69] Weixia Zhang, Kede Ma, Guangtao Zhai, and Xiaokang\\nYang. Uncertainty-aware blind image quality assessment in\\nthe laboratory and wild. IEEE Transactions on Image Pro-\\ncessing , 30:3474±3486, 2021. 1,3\\n[70] Wenhan Zhu, Guangtao Zhai, Zongxi Han, Xiongkuo Min,\\nTao Wang, Zicheng Zhang, and Xiaokang Yangand. A mul-\\ntiple attributes image quality database for smartphone cam-\\nera photo quality assessment. In 2020 IEEE International\\nConference on Image Processing (ICIP) , pages 2990±2994.\\nIEEE, 2020. 1,2,8\\n9978', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Robust Outlier Rejection for 3D Registration with Variational Bayes\\nHaobo Jiang1, Zheng Dang2, Zhen Wei2, Jin Xie∗1, Jian Yang∗1, and Mathieu Salzmann∗2\\n1PCA Lab, Nanjing University of Science and Technology, China\\n2CVLab, EPFL, Switzerland\\n{jiang.hao.bo, csjxie, csjyang }@njust.edu.cn\\n{zheng.dang, zhen.wei, mathieu.salzmann }@epfl.ch\\nAbstract\\nLearning-based outlier (mismatched correspondence)\\nrejection for robust 3D registration generally formulates\\nthe outlier removal as an inlier/outlier classification prob-\\nlem. The core for this to be successful is to learn the dis-\\ncriminative inlier/outlier feature representations. In this\\npaper, we develop a novel variational non-local network-\\nbased outlier rejection framework for robust alignment. By\\nreformulating the non-local feature learning with varia-\\ntional Bayesian inference, the Bayesian-driven long-range\\ndependencies can be modeled to aggregate discriminative\\ngeometric context information for inlier/outlier distinction.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Specifically, to achieve such Bayesian-driven contextual de-\\npendencies, each query/key/value component in our non-\\nlocal network predicts a prior feature distribution and a\\nposterior one. Embedded with the inlier/outlier label, the\\nposterior feature distribution is label-dependent and dis-\\ncriminative. Thus, pushing the prior to be close to the dis-\\ncriminative posterior in the training step enables the fea-\\ntures sampled from this prior at test time to model high-\\nquality long-range dependencies. Notably, to achieve ef-\\nfective posterior feature guidance, a specific probabilis-\\ntic graphical model is designed over our non-local model,\\nwhich lets us derive a variational low bound as our op-\\ntimization objective for model training. Finally, we pro-\\npose a voting-based inlier searching strategy to cluster the\\nhigh-quality hypothetical inliers for transformation estima-\\ntion. Extensive experiments on 3DMatch, 3DLoMatch, and\\nKITTI datasets verify the effectiveness of our method. Code', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='is available at https://github.com/Jiang-HB/VBReg.\\n∗Corresponding authors\\nHaobo Jiang, Jin Xie, and Jian Yang are with PCA Lab, Key Lab of\\nIntelligent Perception and Systems for High-Dimensional Information of\\nMinistry of Education, and Jiangsu Key Lab of Image and Video Under-\\nstanding for Social Security, School of Computer Science and Engineering,\\nNanjing University of Science and Technology, China.1. Introduction\\nPoint cloud registration is a fundamental but challenging\\n3D computer vision task, with many potential applications\\nsuch as 3D scene reconstruction [1,39], object pose estima-\\ntion [13, 44], and Lidar SLAM [15, 51]. It aims to align\\ntwo partially overlapping point clouds by estimating their\\nrelative rigid transformation, i.e., 3D rotation and 3D trans-\\nlation. A popular approach to address the large-scale scene\\nregistration problem consists of extracting point descrip-\\ntors [11,14,17,37,38,50] and establishing correspondences', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='between the two point clouds, from which the transforma-\\ntion can be obtained geometrically. In this context, much\\neffort has been dedicated to designing traditional and deep\\nlearning-based descriptors [3, 11, 21, 41, 50]. However, the\\nresulting correspondences inevitably still suffer from out-\\nliers (wrong matchings), particularly in challenging cases,\\nsuch as low-overlap, repetitive structures, or noisy point\\nsets, leading to registration failure.\\nTo address this, many outlier filtering strategies have\\nbeen developed to robustify the registration process. These\\ninclude traditional rejection methods using random sample\\nconsensus [16], point-wise descriptor similarity [7, 32] or\\ngroup-wise spatial consistency [46]. Deep learning meth-\\nods have also been proposed, focusing on learning corre-\\nspondence features used to estimate inlier confidence val-\\nues [2, 10, 33]. In particular, the current state-of-the-art\\nmethod, PointDSC [2], relies on a spatial consistency-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='driven non-local network to capture long-range context\\nin its learned correspondence features. While effective,\\nPointDSC still yields limited registration robustness, par-\\nticularly for scenes with a high outlier ratio, where the spa-\\ntial consistency constraints may become ambiguous [36],\\nthereby degrading the correspondence features’ quality.\\nIn this paper, we propose to explicitly account for the\\nambiguities arising from high outlier ratios by developing\\na probabilistic feature learning framework. To this end, we\\nintroduce a variational non-local network based on an at-\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n1148\\ntention mechanism to learn discriminative inlier/outlier fea-\\nture representations for robust outlier rejection. Specifi-\\ncally, to capture the ambiguous nature of long-range con-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='textual dependencies, we inject a random feature in each\\nquery, key, and value component in our non-local network.\\nThe prior/posterior distributions of such random features\\nare predicted by prior/posterior encoders. To encourage the\\nresulting features to be discriminative, we make the pos-\\nterior feature distribution label-dependent. During train-\\ning, we then push the prior distribution close to the label-\\ndependent posterior, thus allowing the prior encoder to also\\nlearn discriminative query, key, and value features. This\\nenables the features sampled from this prior at test time to\\nmodel high-quality long-range dependencies.\\nTo achieve effective variational inference, we customize\\na probabilistic graphical model over our variational non-\\nlocal network to characterize the conditional dependencies\\nof the random features. This lets us derive a variational\\nlower bound as the optimization objective for network train-\\ning. Finally, we propose a voting-based deterministic inlier', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='searching mechanism for transformation estimation, where\\nthe correspondence features learned from all non-local iter-\\nations jointly vote for high-confidence hypothetical inliers\\nfor SVD-based transformation estimation. We theoretically\\nanalyze the robustness of our deterministic inlier searching\\nstrategy compared to RANSAC, which also motivates us to\\ndesign a conservative seed selection mechanism to improve\\nrobustness in sparse point clouds.\\nTo summarize, our contributions are as follows:\\n• We propose a novel variational non-local network for\\noutlier rejection, learning discriminative correspon-\\ndence features with Bayesian-driven long-range con-\\ntextual dependencies.\\n• We customize the probabilistic graphical model over\\nour variational non-local network and derive the varia-\\ntional low bound for effective model optimization.\\n• We introduce a Wilson score-based voting mechanism\\nto search high-quality hypothetical inliers , and theo-\\nretically demonstrate its superiority over RANSAC.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Our experimental results on extensive benchmark datasets\\ndemonstrate that our framework outperforms the state-of-\\nthe-art registration methods.\\n2. Related Work\\nEnd-to-end Registration Methods. With the advances of\\ndeep learning in the 3D vision field [34], the learning-based\\nend-to-end registration model has achieved increasing re-\\nsearch attention. DCP [42] uses the feature similarity to\\nestablish pseudo correspondences for SVD-based transfor-\\nmation estimation. RPM-Net [47] exploits the Sinkhornlayer and annealing for discriminative matching map gen-\\neration. [22, 23] integrate the cross-entropy method into\\nthe deep model for robust registration. RIENet [40] uses\\nthe structure difference between the source neighborhood\\nand the pseudo-target one for inlier confidence evaluation.\\nWith the powerful feature representation of Transformer,\\nRegTR [48] effectively aligns large-scale indoor scenes in\\nan end-to-end manner. [13] propose a match-normalization', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='layer for robust registration in the real-world 6D object\\npose estimation task. More end-to-end models such as\\n[10, 18, 28–30, 33, 53] also present impressive precisions.\\nLearning-based Feature Descriptors. To align the com-\\nplex scenes, a popular pipeline is to exploit feature descrip-\\ntors for 3D matching. Compared to hand-crafted descriptors\\nsuch as [17, 37, 38], the deep feature descriptor presents su-\\nperior registration precision and has achieved much more\\nattention in recent years. The pioneering 3DMatch [50] ex-\\nploits the Siamese 3D CNN to learn the local geometric fea-\\nture via contrastive loss. FCGF [11] exploits a fully convo-\\nlutional network for dense feature extraction in a one-shot\\nfashion. Furthermore, D3feat [3] jointly learns the dense\\nfeature descriptor and the detection score for each point. By\\nintegrating the overlap-attention module into D3feat, Preda-\\ntor [21] largely improves the registration reliability in low-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='overlapping point clouds. YOHO [41] utilizes the group\\nequivariant feature learning to achieve the rotation invari-\\nance and shows great robustness to the point density and the\\nnoise interference. [35] develops a geometric transformer to\\nlearn the geometric context for robust super-point matching.\\nLepard [31] embeds the relative 3D positional encoding into\\nthe transformer for discriminative descriptor learning.\\nOutlier Rejection Methods. Despite significant progress\\nin learning-based feature descriptor, generating mismatched\\ncorrespondences (outliers) in some challenging scenes re-\\nmains unavoidable. Traditional outlier filtering methods,\\nsuch as RANSAC [16] and its variants [4, 24, 27], use re-\\npeated sampling and verification for outlier rejection. How-\\never, these methods tend to have a high time cost, particu-\\nlarly in scenes with a high outlier ratio. Instead, FGR [52]\\nand TEASER [45] integrate the robust loss function into the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='optimization objective to weaken the interference from out-\\nliers. Recently, Chen et al. [9] developed second-order spa-\\ntial compatibility for robust consensus sampling. With the\\nrise of deep 3D vision, most learnable outlier rejection mod-\\nels [10,33] formulate outlier rejection as a binary classifica-\\ntion task and reject correspondences with low confidence.\\nYi et al. [49] proposed a context normalization-embedded\\ndeep network for inlier evaluation, while Brachmann et al.\\n[6] enhanced classical RANSAC with neural-guided prior\\nconfidence. As our baseline, PointDSC [2] proposes ex-\\nploiting a spatial consistency-guided non-local inlier classi-\\nfier for inlier evaluation, followed by neural spectral match-\\ning for robust registration. However, under high outlier\\n1149\\nratios, spatial consistency can be ambiguous (as shown in\\nFig. 1), misleading non-local feature aggregation. Instead,\\nwe propose exploiting Bayesian-driven long-range depen-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='dencies for discriminative non-local feature learning.\\n3. Approach\\n3.1. Background\\nProblem Setting. In the pairwise 3D registration task,\\ngiven a source point cloud X={xi∈R3|i= 1, ...,|X|}\\nand a target point cloud Y={yj∈R3|j= 1, ...,|Y|},\\nwe aim to find their optimal rigid transformation consisting\\nof a rotation matrix R∗∈SO(3)and a translation vector\\nt∗∈R3to align their overlapping region precisely. In this\\nwork, we focus on the descriptor-based pipeline for large-\\nscale scene registration. Based on the feature-level near-\\nest neighbor, we construct a set of putative correspondences\\nC=\\x08\\nci= (xi,yi)∈R6|i= 1, ...,|C|\\t\\n. The inlier (cor-\\nrectly matched correspondence) is defined as the correspon-\\ndence satisfying ∥R∗xi+t∗−yi∥< ε, where εindicates\\nthe inlier threshold.\\nVanilla Non-local Feature Embedding. Given the putative\\ncorrespondence set C, [2] leverages the spatial consistency-\\nguided non-local network ( SCNonlocal ) for their feature', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='embedding. The injected geometric compatibility matrix\\ncan effectively regularize the long-range dependencies for\\ndiscriminative inlier/outlier feature learning. In detail, it\\ncontains Literations and the feature aggregation in l-th it-\\neration can be formulated as:\\nFl+1\\ni=Fl\\ni+ MLP\\x10|C|X\\nj=1softmax j(αlβ)Vl\\nj\\x11\\n, (1)\\nwhere Fl\\ni∈Rdindicates the feature embedding of corre-\\nspondence ciinl-th iteration (the initial feature F0\\niis ob-\\ntained via linear projection on ci) andVl\\ni=fl\\nv(Fl\\ni)∈Rd\\nis the projected value feature. αl∈R|C|×|C|is the non-local\\nattention map whose entry αl\\ni,jreflects the feature similar-\\nity between the projected query feature Ql\\ni=fl\\nq(Fl\\ni)∈Rd\\nand the key feature Kl\\ni=fl\\nk(Fl\\nj)∈Rd.β∈R|C|×|C|\\nrepresents the geometric compatibility matrix of correspon-\\ndences, where the compatibility between ciandcjis:\\nβi,j= max\\x10\\n0,1−d2\\nij\\nε2\\x11\\n, dij=|∥xi−xj∥ − ∥yi−yj∥|.\\n(2)\\nBased on the fact that the geometric distance di,jof inliers', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ciandcjtend to be minor, Eq. 2 will assign a high compat-\\nibility value on the inlier pair, thereby promoting the non-\\nlocal network to effectively cluster the inlier features for\\ndiscriminative inlier/outlier feature learning.\\n3.2. Variational Non-local Feature Embedding\\nWhile effective, SCNonlocal still suffers from ambigu-\\nous long-range dependencies, especially in some challeng-\\n250 500 1000 2500 5000\\nNumber of points01020304050Compat. error (%)3DLoMatch (Predator)\\n3DLoMatch (FCGF)Figure 1. The ratio of inlier-outlier pairs with positive compatibil-\\nities in 3DLoMatch [21] using FCGF and Predator descriptors.\\ning scenes (e.g., the low-overlapping case). Two essential\\nreasons are: (i)Wrong geometric compatibility. As shown\\nin Fig. 1, for 3DLoMatch dataset with Predator and FCGF\\ndescriptors, almost 30% and 17% of inlier-outlier pairs own\\nthe positive compatibility values, respectively, which poten-\\ntially misleads the attention weight for wrong feature clus-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tering. (ii)Lack of uncertainty modeling. In symmetric or\\nrepetitive scenes, the inlier/outlier prediction contains sig-\\nnificant uncertainty. Therefore, it’s necessary to design a\\nrobust feature representation to capture such uncertainty.\\nTo overcome them, we develop a variational non-local\\nnetwork, a probabilistic feature learning framework, for\\ndiscriminative correspondence embedding. Our core idea\\nis to inject random features into our model to capture the\\nambiguous nature of long-range dependencies, and then\\nleverage the variational Bayesian inference to model the\\nBayesian-driven long-range dependencies for discrimina-\\ntive feature aggregation. Specifically, we first introduce\\nthe random feature variables zl\\nk,i,zl\\nq,iandzl\\nv,iinto the key\\nKl\\ni, query Ql\\niand value Vl\\nicomponents in our non-local\\nmodule to capture their potential uncertainty in the long-\\nrange dependency modeling. Then, the prior/posterior en-\\ncoders are constructed to predict their prior feature distribu-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tion and the posterior one, respectively. Embedded with the\\ninlier/outlier label, the posterior feature distribution is label-\\ndependent and discriminative. Thus, by pushing the prior\\nclose to the discriminative posterior in the training phase,\\nthis prior at test time also tends to sample discriminative\\nquery, key, and value features for high-quality long-range\\ndependency modeling.\\nProbabilistic Graphical Model over Variational Non-\\nlocal Network. To achieve effective variational Bayesian\\ninference, we need to first characterize the conditional de-\\npendencies of the injected random features so that the varia-\\ntional lower bound can be derived as the optimization objec-\\ntive for model training. As shown in Fig. 2, we customize\\nthe probabilistic graphical model over our non-local net-\\nwork to clarify the dependencies of random features (the\\ncircles). The solid line denotes the label prediction pro-\\ncess, while the dashed line represents our label-dependent', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='posterior encoder ( i.e., inference model). Notably, the de-\\nterministic hidden query/key/value features hl\\nk,i∈Rd′,\\n1150\\nhl\\nq,i∈Rd′, and hl\\nv,i∈Rd′are also introduced to sum-\\nmarize the historical information for better feature updating\\nin each iteration.\\nInlier/outlier Prediction Process. Based on the defined\\nconditional dependencies in Fig. 2, the prediction process of\\ncorrespondence labels b=\\x08\\nb1, b2, ..., b|C||bi∈ {0,1}\\t\\n(1 indicates inlier and 0 outlier) is formulated as follows.\\nBeginning with the initial linear projection ˜F0∈R|C|×d\\nof correspondences C={ci}, we iteratively perform the\\nprobabilistic non-local aggregation for feature updating. In\\nthel-th iteration, we first employ a Gated Recurrent Unit\\n(GRU) [12] to predict the hidden query/key/value features\\nwhich summarize the historical query/key/value features\\n(sampled from the prior distributions) and the correspon-\\ndence features in previous iterations:\\nhl\\nq,i= GRU q(hl−1\\nq,i,[zl−1\\nq,i,˜Fl−1\\ni]),\\nhl\\nk,i= GRU k(hl−1', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='hl\\nk,i= GRU k(hl−1\\nk,i,[zl−1\\nk,i,˜Fl−1\\ni]),\\nhl\\nv,i= GRU v(hl−1\\nv,i,[zl−1\\nv,i,˜Fl−1\\ni]),(3)\\nwhere [·,·]denotes the feature concatenation and ˜Fl−1\\niis\\nthe learned correspondence features of ciin iteration l−1.\\nThen, with as input the predicted hidden features, the prior\\nencoder pθ(·)is utilized to predict the prior feature distri-\\nbution for query/key/value, respectively. Furthermore, we\\nsample features zl\\nq,i∈R˜d,zl\\nk,i∈R˜dandzl\\nv,i∈R˜d\\nfrom the predicted prior query/key/value distribution and\\ncombine them with the hidden features to predict the corre-\\nsponding query ˜Ql\\ni∈Rd, key ˜Kl\\ni∈Rdand value ˜Vl\\ni∈Rd\\nthrough a neural network fq,k,v\\nθ:Rd′+˜d→Rd:\\nzl\\nq,i∼pθ(zl\\nq,i|hl\\nq,i),˜Ql\\ni=fq\\nθ(\\x02\\nzl\\nq,i,hl\\nq,i\\x03\\n),\\nzl\\nk,i∼pθ(zl\\nk,i|hl\\nk,i),˜Kl\\ni=fk\\nθ(\\x02\\nzl\\nk,i,hl\\nk,i\\x03\\n),\\nzl\\nv,i∼pθ(zl\\nv,i|hl\\nv,i),˜Vl\\ni=fv\\nθ(\\x02\\nzl\\nv,i,hl\\nv,i\\x03\\n),(4)\\nwhere the prior feature distribution is the Gaussian distribu-\\ntion with the mean and the standard deviation parameterized', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='by a neural network. Finally, with the learned ˜Ql\\ni,˜Kl\\niand\\n˜Vl\\ni, the correspondence feature ˜Fl\\niinl-th iteration can be\\naggregated with the same non-local operation in Eq. 1. Af-\\nterLfeature iterations, we feed the correspondence feature\\n˜FL\\niin the last iteration into a label prediction model yθto\\npredict the inlier/outlier labels bi∼yθ(bi|˜FL\\ni), where\\nthe label prediction model outputs a scalar Gaussian distri-\\nbution with the mean parameterized by the neural network\\nand the unit variance.\\nVariational Posterior Encoder. Due to the nonlinear-\\nity of our variational non-local model, we cannot di-\\nrectly derive the precise posterior distribution for random\\nquery/key/value features using the standard Bayes’ theo-\\nrem. Taking inspiration from the Variational Bayesian in-\\nference, we construct a label-dependent posterior encoder\\nC ˜F0 h0\\nkh0\\nq\\nh0\\nvz0\\nkz0\\nq\\nz0\\nv˜K0˜Q0\\n˜V0˜F1 h1\\nkh1\\nq\\nh1\\nvz1\\nkz1\\nq\\nz1\\nv˜K1˜Q1\\n˜V1˜F2\\nVariational\\nPosteriorb', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Posteriorb\\n1Figure 2. Probabilistic graphical model for our variational non-\\nlocal network. For simplicity, we just demonstrate two itera-\\ntions. The white circles indicate the random features and the white\\nsquares denote the deterministic hidden features. The solid line\\nrepresents the inlier/outlier prediction process and the dashed line\\ndenotes the label-dependent variational posterior encoder. We just\\nshow the variational posterior for z1\\nk.\\nqϕ(·)to to approximate the feature posterior:\\nzl\\nq,i∼qϕ(zl\\nq,i|[hl\\nq,i,[bi]×k])\\nzl\\nk,i∼qϕ(zl\\nk,i|[hl\\nk,i,[bi]×k])\\nzl\\nv,i∼qϕ(zl\\nv,i|[hl\\nv,i,[bi]×k]),(5)\\nwhere [bi]×kindicates a label vector generated by tiling the\\nscalar label ktimes. The output of each posterior encoder is\\na diagonal Gaussian distribution with parameterized mean\\nand standard deviation.\\nVariational Lower Bound. Finally, we derive the op-\\ntimization objective ELBO( θ, ϕ), the variational (evi-\\ndence) lower bound of log-likelihood correspondence la-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='belslnyθ(b| C), to train our variational non-local network\\n(Please refer to Appendix A for the detailed derivation):\\nELBO( θ, ϕ) =EQL−1\\nl=0qϕ(zl\\nq,k,v|hl\\nq,k,v,b)h\\nlnyθ(b|˜FL)i\\n−\\nL−1X\\nl=0Eqϕh\\nDKL\\x10\\nqϕ(zl\\nq,k,v|hl\\nq,k,v,b)||pθ(zl\\nq,k,v|hl\\nq,k,v)\\x11i\\n(6)\\nwhere for clarity, we utilize the subscript q, k, v to denote\\nthe same operator performed on query/key/value. DKL(·||·)\\ndenotes the Kullback–Leibler (KL) divergence between two\\ndistributions. By maximizing the variational lower bound\\nabove, we can optimize the network parameters to in-\\ndirectly maximize the log-likelihood value of correspon-\\ndence labels. Eq. 6 indicates that the discriminative, label-\\ndependent feature posterior explicitly constrains the prior\\nby reducing their KL divergence in the training phase,\\nwhich promotes the query, key, and value features sampled\\nfrom the prior to model the high-quality long-term depen-\\ndencies at test time.\\n3.3. Voting-based Inlier Searching\\nWith the learned correspondence features above, we then', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='propose a voting-based sampling strategy to search the de-\\nsired inlier subset from the entire putative correspondences\\n1151\\nfor optimal transformation estimation. Our sampling mech-\\nanism is deterministic and efficient. We first select a set\\nof high-confidence seeds CseedfromCbased on their inlier\\nconfidence (predicted in § 3.2) and the Non-Maximum Sup-\\npression (as performed in [2]), where the number of seeds\\n|Cseed|=⌊|C| ∗v⌋(vis a fixed seed ratio). Then, for each\\nseed, we cluster its most compatible correspondences into\\nit to form the hypothetical inliers . Ideally, if the seed is an\\ninlier and the compatible value is correct (without ambigu-\\nity problem as in Fig. 2), its most compatible correspon-\\ndences are also inliers, thus we can cluster the desired inlier\\nsubset successfully. To cluster high-confidence hypotheti-\\ncal inliers , we perform a coarse-to-fine clustering strategy,\\nincluding the voting-based coarse clustering and the Wilson\\nscore-based fine clustering.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Voting-based Coarse Clustering. We view the correspon-\\ndence features {˜F1, ...,˜FL}learned from all non-local it-\\nerations as Lvoters. For l-th voter, it deterministically\\nclusters κ−1most compatible correspondences(κ−1)Al\\nci\\nfor each seed ci∈ Cseed to from the hypothetical inliers\\n(κ)Ml\\nci={ci} ∪(κ−1)Al\\nci. To this end, we first compute\\nthe compatibility matrix Sl∈R|C|×|C|forl-th voter, where\\neach entry Sl\\ni,j= clip(1 −(1−cos(˜Fl\\ni,˜Fl\\nj)/σ2),0,1)·βi,j,\\nwhere the parameter σis to control the feature compatibility\\nand geometric compatibility βi,jis defined in Eq. 2. Thus,\\n(κ−1)Al\\ncican be determined as {cj|Sl\\ni,j≥γκ−1}, where\\nthreshold γκ−1is the (κ−1)-th largest compatibility. Ben-\\nefitting from our discriminative feature representations, Sl\\ncan effectively handle the ambiguity problem in Fig. 2 and\\npromote robust inlier clustering. After the voting process\\nabove, each seed cican achieve Lcandidate hypothetical\\ninliers\\x08(κ)M1\\nci, ...,(κ)ML\\nci\\t\\n.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ci, ...,(κ)ML\\nci\\t\\n.\\nWilson score-based Fine Clustering. Based on the voted\\nhypothetical inliers above, we then fuse these voted results\\nwith Wilson score [43] to form the high-quality hypothet-\\nical inliers(κ)˜Mci. In detail. we denote Il\\ni,j=I{cj∈\\n(κ)Ml\\nci}to indicate whether(κ)Ml\\ncicontains cj. The Wil-\\nson score Wn\\ni,jof accepting cjinto(κ)˜Mcican be com-\\nputed as:\\nWn\\ni,j=1\\n1 +z2\\nn\"\\nˆp(n)\\ni,j+z2\\n2n−zs\\nˆp(n)\\ni,j(1−ˆp(n)\\ni,j)\\nn+z2\\n4n2#\\n,\\n(7)\\nwhere ˆp(n)\\ni,j=1\\nnPn\\nτ=1Iτ\\ni,jis the average acceptation ratio\\nof top nvoters and z= 1.96(i.e., z-score at 95% con-\\nfidence level). Eq. 7 indicates that the Wilson score not\\nonly considers the sample mean but also the confidence\\n(positively related to sample number n). Finally, among\\nthe set of Wilson scores under different sample numbers\\n{W1\\ni,j, ...,WL\\ni,j}, we choose the largest one as the final\\nWilson score ˜Wi,j= max\\nn∈{1,...,L}Wn\\ni,jforcj. Thus, the\\nfinal hypothetical inliers set of cican be determined as(κ)˜Mci={ci} ∪ {cj|˜Wi,j≥γ′', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='κ−1}, where γ′\\nκ−1is\\nthe (κ−1)-th largest Wilson score.\\nTheoretical Analysis and Conservative Seed Selection.\\nFinally, we try to theoretically analyze our deterministic in-\\nlier searching mechanism compared to RANSAC [16] and\\nfurther propose a simple but effective conservative seed se-\\nlection strategy for more robust 3D registration in sparse\\npoint clouds. We let {(κ)Msac\\ni}J\\ni=1be the randomly sam-\\npled hypothetical inlier subset in RANSAC, and let Cin,\\nCoutandpinbe the inlier subset, outlier subset and the inlier\\nratio, respectively, C=Cin∪Cout,pin=|Cin|/|C|. We also\\ndenote the inliers in seeds as ˜Cin=Cin∩ Cseed. Then, we\\ncan derive the following theorem (Please refer to Appendix\\nB for the derivation process).\\nTheorem 1 Assume the number of outliers in(κ)˜Mci(ci∈\\n˜Cin) follows a Poisson distribution Pois (α·κ). Then, if\\nα <−1\\nκ·logh\\n1−(1−pκ\\nin)J/|˜Cin|i\\n≜U, the probability\\nof our method achieving the inlier subset is greater than or\\nequal to that of RANSAC.\\nP\\x10\\nmax', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='P\\x10\\nmax\\nci∈Cseed|(κ)˜Mci∩ Cin|=κ\\x11\\n≥\\nP\\x10\\nmax\\n1≤i≤J|(κ)Msac\\ni∩ Cin|=κ\\x11\\n.(8)\\nTheorem 1 shows that as the inlier ratio pindegrades, the\\nupper bound Uwill increase, indicating our method tends to\\nbe more likely to achieve the inlier subset than RANSAC in\\nhigh outlier cases. Therefore, our method tends to be more\\nrobust to outliers than RANSAC. In addition, Theorem 1\\nalso shows that the more inliers |˜Cin|inCseed, the better for\\nour method. It means selecting all correspondences in Cas\\nseeds theoretically seems the best choice since it can avoid\\nmissing any inlier. However, in real implementation, we\\nhave to perform inlier selection to accelerate the registration\\nspeed. Nevertheless, in sparse point clouds, the number of\\ncorrespondences is small. If we still perform the original\\ninlier selection, fewer seeds even may not contain any inlier,\\nleading to registration failure. To overcome it, we develop\\na conservative seed selection strategy, which changes the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='seed number to max{⌊v·|C|⌋, n}(n <|C|), where the lower\\nbarnof the seed number can effectively avoid selecting too\\nfewer inliers in sparse cases and in dense cases, it would\\ndegrade to the original selection strategy.\\n3.4. Rigid Transformation Estimation\\nWith the voted hypothetical inliers {(κ)˜Mci|ci∈\\nCseed}above, we estimate the optimal transformation pa-\\nrameter based on the Procrustes method [20] to minimize\\nthe least-squares errors for each hypothetical inlier group:\\nˆRi,ˆti= arg min\\nR,tX\\ncj∈(κ)˜Mciωj· ∥R⊤xj+t−yj∥2,(9)\\nwhere ωjis the error weight computed by the neural spec-\\ntral matching as in [2]. Then, we select the transformation\\n1152\\nparameter that maximizes the number of overlapped corre-\\nspondences as the final optimal transformation estimation:\\nˆR∗,ˆt∗= arg max\\n{ˆRi,ˆti}|C|\\ni=1X\\ncj∈Cvi·In\\n∥ˆR⊤\\nixj+ˆti−yj∥2< εo\\n,\\n(10)\\nwhere vi= 1−∥ˆR⊤\\nixj+ˆti−yj∥2\\n2/ε2is used to re-weight\\nthe inlier count as performed in [23]. Finally, we refine it', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='using all recovered inliers in a least-squares optimization as\\na common practice in [2, 4].\\n4. Experiments\\n4.1. Experimental Settings\\nImplementation Details. For our variational non-local\\nmodule, the number of iterations Lis 12, and the dimen-\\nsions of the correspondence feature, random feature, and\\nhidden feature are set to 128, 128, and 256, respectively.\\nFor our voting-based inlier sampling module, the size of hy-\\npothetical inliers κis 40. For seed selection, seed ratio vis\\n0.1and the lower bar of seed number nis 1000. Our model\\nis trained with 50epochs using Adam optimizer with learn-\\ning rate 10−4and weight decay 10−6. We utilize PyTorch\\nto implement our project and perform all experiments on\\nthe server equipped with an Intel i5 2.2 GHz CPU and one\\nTesla V100 GPU. For simplicity, we name our Variational\\nBayesian-based Registration framework as VBReg .\\nEvaluation Metric. We use three metrics to evaluate our\\nmethod, including (1) Registration Recall ( RR), the percent', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='of the successful registration satisfying the error thresholds\\nof rotation and translation at the same time, (2) average Ro-\\ntation Error ( RE) and (3) average Translation Error ( TE):\\nRE(ˆR) = arccosTr\\x10\\nˆR⊤R∗\\x11\\n−1\\n2,TE(ˆt) =', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ˆt−t∗', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='2\\n2,\\n(11)\\nwhere ˆRandˆtare the predicted rotation matrix and rotation\\nvector, respectively, while R∗andt∗are the corresponding\\nground truth. The average REandTEare computed only on\\nsuccessful aligned point cloud pairs.\\n4.2. Comparison with Existing Methods\\nEvaluation on 3DMatch. We first evaluate our method\\non 3DMatch benchmark [50], which contains 46 training\\nscenes, 8 validation scenes, and 8 test scenes. We first vox-\\nelize and down-sample the point cloud with 5cm voxel size\\nand then leverage FCGF [11] and FPFH [37] descriptors to\\nconstruct the putative correspondences based on the feature\\nnearest neighbor. We compare our method with eight state-\\nof-the-art (SOTA) correspondence-based methods, where\\nFGR [52], SM [26], RANSAC (50k) [16], TEASER++ [45],\\nand SC2 PCR [9] are representative traditional methods,\\nwhile DGR [10], DHVR [25], and PointDSC [2] are ad-\\nvanced deep learning-based methods. As shown in Table 1,\\nFigure 3. Registration visualization on 3DLoMatch [19].', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='in the FCGF setting, our method achieves the best per-\\nformance in RRandREcriteria while the same TEwith\\nPointDSC. We need to highlight that RRis a more impor-\\ntant criterion than REandTEsince the rotation and transla-\\ntion errors are just calculated in a successful registration. It\\nmeans that the higher RRmay include more challenging but\\nsuccessful registration cases, potentially increasing their er-\\nrors. In the FPFH setting, it can be observed that our method\\ncan still achieve the best RRscore among all deep methods,\\nbut perform slightly worse than SC2 PCR. Notably, com-\\npared to PointDSC (our baseline), the precision gain on RR\\nis impressive (5.24% ↑), which benefits from the effective-\\nness of our variational non-local feature learning and the\\nvoting-based inlier searching.\\nEvaluation on 3DLoMatch. We further test our method\\non 3DLoMatch benchmark [21]. Compared to 3DMatch\\nsharing more than 30% overlap, the overlaps of point cloud', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='pairs in 3DLoMatch just lie in 10%∼30%, thus present-\\ning much more challenges. We leverage FCGF [11] and\\nrecently popular Predator [21] as the feature descriptors for\\nputative correspondence generation. We choose six tradi-\\ntional methods: FGR [52], SM [26], RANSAC (50k) [16],\\nTEASER++ [45], SC2 PCR [9], and TR DE [8], and two\\ndeep methods: DHVR [25], and PointDSC [2] for compar-\\nison. The registration recalls ( RR) under different numbers\\nof correspondences are listed in Table 2. It can be observed\\nthat regardless of FCGF or Predator descriptor, our method\\nalmost achieves the best performance on all settings, except\\nfor FCGF setting with 250 points. Notably, in more chal-\\nlenging cases, the performance advantage over PointDSC is\\nfurther expanded (+9.5% and +7.8% in the cases of FCGF\\nwith 500 and 250 points). These experimental results fur-\\nther demonstrate the outstanding robustness of our method\\nwhen encountering those extremely low-overlapping cases', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='(high outlier ratios). Some visualization results are listed\\nin Fig. 3 and the RRchanges under different inlier ratios are\\npresented in Fig. 4 (c), which suggests that our performance\\ngains are mainly brought by model robustness in extremely\\nhigh outlier situations.\\nEvaluation on KITTI. Finally, we evaluate our method on\\nthe outdoor LIDAR-scanned driving scenarios from KITTI\\ndataset [19]. In line with [11], we utilize sequences 0-5, 6-\\n7, and 8-10 as the training set, validation set, and test set,\\nrespectively. Also, as the setting in [3,11], we further refine\\nthe ground-truth rigid transformations using ICP [5] and\\n1153\\n3DMatch (FCGF) 3DMatch (FPFH) KITTI (FCGF) KITTI (FPFH)\\nModels RR( ↑) RE( ↓) TE( ↓)RR(↑) RE( ↓) TE( ↓)RR(↑) RE( ↓) TE( ↓)RR(↑) RE( ↓) TE( ↓)Sec.\\nFGR [52] 79.17 2.93 8.56 41.10 4.05 10.09 96.58 0.38 22.30 1.26 1.69 47.18 1.39\\nSM [26] 86.57 2.29 7.07 55.82 2.94 8.13 96.58 0.50 19.88 75.50 0.66 15.01 0.02', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='RANSAC [16] 91.50 2.49 7.54 73.57 3.55 10.04 97.66 0.28 22.61 89.37 1.22 25.88 6.43\\nTEASER++ [45] 85.77 2.73 8.66 75.48 2.48 7.31 83.24 0.84 12.48 64.14 1.04 14.85 0.07\\nDGR [10] 91.30 2.40 7.48 69.13 3.78 10.80 95.14 0.43 23.28 73.69 1.67 34.74 1.36\\nDHVR [25] 89.40 2.19 6.95 67.10 2.56 7.67 – – – – – – 0.40\\nSC2 PCR [9] 93.10 2.04 6.53 83.92 2.09 6.66 97.48 0.33 20.66 97.84 0.39 9.09 0.09\\nPointDSC [2] 92.42 2.05 6.49 77.51 2.08 6.51 97.66 0.47 20.88 98.20 0.58 7.27 0.11\\nVBReg 93.53 2.04 6.49 82.75 2.14 6.77 98.02 0.32 20.91 98.92 0.32 7.17 0.22\\nTable 1. Quantitative comparison on 3DMatch [50] and KITTI [19] benchmark datasets with descriptors FCGF and FPFH. The registration\\nspeed is achieved by computing the averaged time cost on 3DMatch with FCGF descriptor.\\nFeature Model 5000 2500 1000 500 250\\nFCGFFGR [52] 18.6 19.4 16.9 16.0 12.4\\nSM [26] 32.4 31.3 31.4 28.0 23.5\\nRANSAC [16] 37.6 37.2 35.9 32.1 25.9\\nTEASER++ [45] 42.8 42.4 39.5 34.5 25.7\\nDHVR [25] 50.4 49.6 46.4 41.0 34.6', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='SC2 PCR [9] 57.4 56.5 51.8 46.4 36.2\\nTRDE [8] 49.5 50.4 48.4 43.4 34.3\\nPointDSC [2] 55.8 52.6 46.8 37.7 26.7\\nVBReg 58.3 56.8 52.9 47.2 34.5\\nPredatorFGR [52] 36.4 38.2 39.7 39.6 38.0\\nSM [26] 53.8 55.1 55.4 54.5 50.2\\nRANSAC [16] 62.3 62.8 62.4 61.5 58.2\\nTEASER++ [45] 62.9 62.6 61.9 59.0 56.7\\nDHVR [25] 67.2 67.3 66.1 64.6 60.5\\nSC2 PCR [9] 69.5 69.5 68.6 65.2 62.0\\nTRDE [8] 64.0 64.8 61.7 58.8 56.5\\nPointDSC [2] 68.1 67.3 66.5 63.4 60.5\\nVBReg 69.9 69.8 68.7 66.4 63.0\\nTable 2. Registration recall ( RR) with different numbers of points\\non 3DLoMatch benchmark dataset [21].\\nonly collect the point cloud pairs far away from each other\\nat most 10m as the test dataset. We downsample the point\\ncloud with a voxel size of 30cm and exploit FCGF [11]\\nand FPFH [37] descriptors for correspondence construction,\\nrespectively. The compared methods are consistent with\\nthose in 3DMatch . The comparison results are listed in Ta-\\nble 1. For the FCGF setting, our method can achieve the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='best scores on the most important RRcriterion while for the\\nFPFH setting, our method can consistently achieve the best\\nscores on all criteria.\\n4.3. Ablation Studies and Analysis\\nVariational Non-local Network. We first take PointDSC\\nas our baseline and compare our proposed variational\\nBayesian non-local network ( VBNonlocal ) to the spatial\\nconsistency-based non-local network ( SCNonlocal ) to high-\\nlight the effectiveness of our proposed method. (1): We\\n0 0.2 0.4 0.6 0.8 1.0\\nFeature similarity of inlier0.000.050.100.15RatioSCNonlocal\\nVBNonlocal\\n0 0.2 0.4 0.6 0.8 1.0\\nFeature similarity of inlier0.000.050.100.15RatioSCNonlocal\\nVBNonlocal\\n<1 1~2 2~4 4~6 6~10 >10\\nInlier rate (%)020406080100RR (%)RANSAC\\nPointDSC\\nVBReg\\n1 5 10 15 20 25\\nNumber of epochs5060708090RR (%)\\nSCNonlocal(FCGF)\\nVBNonlocal(FCGF)\\nSCNonlocal(FPFH)\\nVBNonlocal(FPFH)(a) The distribution of inlier feature\\nsimilarity on 3DMatch [50].\\n0 0.2 0.4 0.6 0.8 1.0\\nFeature similarity of inlier0.000.050.100.15RatioSCNonlocal\\nVBNonlocal', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='VBNonlocal\\n0 0.2 0.4 0.6 0.8 1.0\\nFeature similarity of inlier0.000.050.100.15RatioSCNonlocal\\nVBNonlocal\\n<1 1~2 2~4 4~6 6~10 >10\\nInlier rate (%)020406080100RR (%)RANSAC\\nPointDSC\\nVBReg\\n1 5 10 15 20 25\\nNumber of epochs5060708090RR (%)\\nSCNonlocal(FCGF)\\nVBNonlocal(FCGF)\\nSCNonlocal(FPFH)\\nVBNonlocal(FPFH)(b) The distribution of inlier feature\\nsimilarity on 3DLoMatch [21].\\n0 0.2 0.4 0.6 0.8 1.0\\nFeature similarity of inlier0.000.050.100.15RatioSCNonlocal\\nVBNonlocal\\n0 0.2 0.4 0.6 0.8 1.0\\nFeature similarity of inlier0.000.050.100.15RatioSCNonlocal\\nVBNonlocal\\n<1 1~2 2~4 4~6 6~10 >10\\nInlier rate (%)020406080100RR (%)RANSAC\\nPointDSC\\nVBReg\\n1 5 10 15 20 25\\nNumber of epochs5060708090RR (%)\\nSCNonlocal(FCGF)\\nVBNonlocal(FCGF)\\nSCNonlocal(FPFH)\\nVBNonlocal(FPFH)\\n(c)RRunder different inlier ratios.\\n0 0.2 0.4 0.6 0.8 1.0\\nFeature similarity of inlier0.000.050.100.15RatioSCNonlocal\\nVBNonlocal\\n0 0.2 0.4 0.6 0.8 1.0\\nFeature similarity of inlier0.000.050.100.15RatioSCNonlocal\\nVBNonlocal\\n<1 1~2 2~4 4~6 6~10 >10', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Inlier rate (%)020406080100RR (%)RANSAC\\nPointDSC\\nVBReg\\n1 5 10 15 20 25\\nNumber of epochs5060708090RR (%)\\nSCNonlocal(FCGF)\\nVBNonlocal(FCGF)\\nSCNonlocal(FPFH)\\nVBNonlocal(FPFH) (d) Training curves.\\nFigure 4. (a) and (b): The distribution of the learned feature sim-\\nilarity of inliers; (c): RRunder different inlier ratios; (d): RRon\\nthe validation sets of 3DMatch [50] (FCGF) and 3DMatch (FPFH)\\nunder different training epochs.\\nfirst compare their performance difference under two types\\nof network input: VBNonlocalxyzandSCNonlocalxyzindi-\\ncate using concatenated correspondence coordinate as input\\nwhile VBNonlocalfeatandSCNonlocalfeatrepresent using\\nconcatenated coordinate and descriptor of correspondence\\nas input. As shown in the top block in Table 3, with each\\ndata type as input, our VBNonlocal can consistently achieve\\nsignificant performance gains. Especially, on 3DMatch\\nwith FPFH descriptor, VBNonlocalfeatbrings 2.77% RR\\nimprovement and on 3DLoMatch with 500, 1000 and 2500', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='points, the RRimprovements even can reach 5.4%,6%and\\n5.9%, respectively. These impressive results support our\\nview that our Bayesian-driven long-range dependency mod-\\n1154\\n3DMatch 3DLoMatch (FCGF) 3DLoMatch (Predator)\\nModel FCGF FPFH 5000 2500 1000 500 250 5000 2500 1000 500 250 Sec.\\nPointDSC w/SCNonlocalxyz92.42 77.51 55.8 52.6 46.8 37.7 26.7 68.1 67.3 66.5 63.4 60.5 0.11\\nPointDSC w/VBNonlocalxyz93.04 80.16 57.7 55.6 50.2 39.9 26.1 69.7 69.6 67.9 64.9 61.9 0.17\\nPointDSC w/SCNonlocalfeat92.36 77.76 54.6 50.6 44.9 36.8 25.4 69.2 68.6 67.9 63.5 59.9 0.13\\nPointDSC w/VBNonlocalfeat93.04 80.53 56.9 56.5 50.9 42.2 28.9 69.2 68.7 68.0 64.6 60.6 0.18\\nPointDSC w/SCNonlocalcls92.98 78.99 54.1 52.2 46.0 38.7 27.7 67.6 66.9 67.2 63.7 60.2 0.11\\nPointDSC w/VBNonlocalfeat+V ote 93.41 81.21 58.3 56.5 51.9 44.7 31.1 69.3 69.5 68.2 65.3 61.2 0.20\\nPointDSC w/VBNonlocalfeat+V ote+CS 93.53 82.75 58.3 56.8 52.9 47.2 34.5 69.9 69.8 68.7 66.4 63.0 0.22', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Iteration times L= 6 93.41 82.32 58.1 57.1 52.9 48.3 34.8 69.7 69.7 68.7 66.3 63.8 0.19\\nIteration times L= 9 93.41 81.58 58.2 57.0 53.2 47.6 32.5 70.0 69.4 68.5 66.9 63.2 0.20\\nIteration times L= 12 * 93.53 82.75 58.3 56.8 52.9 47.2 34.5 69.9 69.8 68.7 66.4 63.0 0.22\\nRandom feat. dim. ˜d= 32 93.41 82.38 58.0 57.5 53.5 48.1 34.8 69.7 70.0 68.6 66.3 63.3 0.20\\nRandom feat. dim. ˜d= 64 93.41 81.45 57.9 56.9 52.8 48.6 35.0 69.6 69.7 68.4 66.5 62.3 0.21\\nRandom feat. dim. ˜d= 128 * 93.53 82.75 58.3 56.8 52.9 47.2 34.5 69.9 69.8 69.3 66.4 63.3 0.22\\nTable 3. Ablation studies on 3DMatch [50] and 3DLoMatch [21] datasets. SCNonlocal : Spatial consistency-guided non-local network;\\nVBNonlocal : Variational Bayesian-based non-local network; Vote: V oting-based inlier searching; CS: Conservative seed selection.\\neling can effectively learn the discriminative inlier/outlier\\nfeatures for reliable inlier search. (2): Then, to further high-\\nlight the superiority of our variational inference-guided fea-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ture learning, we also try to add classification loss on the\\nfeatures produced by each iteration in SCNonlocal to guide\\ntheir learning (denoted as SCNonlocalcls). As shown in the\\nfifth row in Table 3, such loss-based label-propagation way\\njust can achieve very limited performance gain and even\\nsometimes degrades score. It demonstrates the superiority\\nof our label-dependent posterior guidance for (prior) feature\\nlearning. Also, owing to such posterior guidance, the train-\\ning curves in Fig. 4 (d) show that our method can achieve\\nsignificantly faster convergence speed than SCNonlocal .\\nDiscriminative Feature Learning? In order to verify\\nwhether VBNonlocal can learn more discriminative features\\nthan SCNonlocal , we visualize the distribution of the fea-\\nture similarity of inliers on 3DMatch (Fig. 4 (a)) and 3DLo-\\nMatch (Fig. 4 (b)). As we can see, our learned inlier features\\nown much higher similarities (approximate to 1) than SC-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Nonlocal on both datasets, which demonstrates that our pro-\\nposed Bayesian-inspired non-local network truly can pro-\\nmote more discriminative correspondence-feature learning.\\nVariational Non-local Setting. We further test the perfor-\\nmance changes under different VBNonlocal settings. (1)We\\nfirst test the model robustness under different numbers of\\nnon-local iterations. The second block in Table 3 verifies\\nthat our method is robust to the iteration time and tends to\\nconsistently achieve outstanding RRscore. (2)Then, the\\nbottom block in Table 3 further shows our model stability\\nto different dimensions of random features.\\nVoting-based Inlier Searching. Furthermore, we evaluate\\nthe performance contribution of the proposed voting-based\\ninlier searching strategy ( Vote). As we can see in the sixth\\nrow of Table 3, voting strategy can consistently achieve per-\\nformance improvement regardless in 3DMatch or in morechallenging 3DLoMatch. It mainly benefits from the high-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='quality hypothetical inliers sampled by our voting policy.\\nConservative Seed Selection. Finally, we test the effec-\\ntiveness of the proposed conservative seed selection strat-\\negy ( CS) motivated by Theorem 1. As we can see in the\\nseventh row of Table 3, CScan achieve consistent perfor-\\nmance gain in each setting. Especially, in the cases of fewer\\npoints ( e.g., FCGF setting with 250 and 500 points), the im-\\nprovement is much more significant (+3.4%, +2.5%). As\\nthe analysis before, in sparse point clouds, the original in-\\nlier selection strategy like in [2] is aggressive and prone to\\nmiss too many inlier seeds. Instead, our conservative se-\\nlection strategy can effectively mitigate it as well as keep\\nregistration efficiency.\\n5. Conclusion\\nIn this paper, we adapted the variational Bayesian infer-\\nence into the non-local network and developed the effective\\nBayesian-guided long-term dependencies for discriminative\\ncorrespondence-feature learning. To achieve effective vari-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ational inference, a probabilistic graphical model was cus-\\ntomized over our non-local network, and the variational low\\nbound was derived as the optimization objective for model\\ntraining. In addition, we proposed a Wilson score-based\\nvoting mechanism for high-quality inlier sampling and the-\\noretically verified its superiority over RANSAC. Extensive\\nexperiments on indoor/outdoor datasets demonstrated its\\npromising performance.\\n6. Acknowledgments\\nThis work was supported by the National Science Fund\\nof China (Grant Nos. U1713208, U62276144).\\n1155\\nReferences\\n[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Si-\\nmon, Brian Curless, Steven M Seitz, and Richard Szeliski.\\nBuilding rome in a day. Communications of the ACM (2011) ,\\n54(10):105–112. 1\\n[2] Xuyang Bai, Zixin Luo, Lei Zhou, Hongkai Chen, Lei Li,\\nZeyu Hu, Hongbo Fu, and Chiew-Lan Tai. Pointdsc: Ro-\\nbust point cloud registration using deep spatial consistency.\\nInProceedings of the IEEE/CVF Conference on Computer', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Vision and Pattern Recognition , pages 15859–15869, 2021.\\n1, 2, 3, 5, 6, 7, 8\\n[3] Xuyang Bai, Zixin Luo, Lei Zhou, Hongbo Fu, Long Quan,\\nand Chiew-Lan Tai. D3feat: Joint learning of dense detec-\\ntion and description of 3d local features. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 6359–6367, 2020. 1, 2, 6\\n[4] Daniel Barath and Ji ˇr´ı Matas. Graph-cut ransac. In Proceed-\\nings of the IEEE conference on computer vision and pattern\\nrecognition , pages 6733–6741, 2018. 2, 6\\n[5] Paul J Besl and Neil D McKay. Method for registration of\\n3-D shapes. In Sensor fusion IV: control paradigms and data\\nstructures (1992) , volume 1611, pages 586–606. 6\\n[6] Eric Brachmann and Carsten Rother. Neural-guided ransac:\\nLearning where to sample model hypotheses. In ICCV , 2019.\\n2\\n[7] Gary Bradski. The opencv library. Dr. Dobb’s Journal: Soft-\\nware Tools for the Professional Programmer , 25(11):120–\\n123, 2000. 1', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='123, 2000. 1\\n[8] Wen Chen, Haoang Li, Qiang Nie, and Yun-Hui Liu. De-\\nterministic point cloud registration via novel transformation\\ndecomposition. In CVPR , 2022. 6, 7\\n[9] Zhi Chen, Kun Sun, Fan Yang, and Wenbing Tao. Sc2-pcr:\\nA second order spatial compatibility for efficient and robust\\npoint cloud registration. In CVPR , 2022. 2, 6, 7\\n[10] Christopher Choy, Wei Dong, and Vladlen Koltun. Deep\\nglobal registration. In CVPR (2020) , pages 2514–2523. 1, 2,\\n6, 7\\n[11] Christopher Choy, Jaesik Park, and Vladlen Koltun. Fully\\nconvolutional geometric features. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision ,\\npages 8958–8966, 2019. 1, 2, 6, 7\\n[12] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and\\nYoshua Bengio. Empirical evaluation of gated recurrent\\nneural networks on sequence modeling. arXiv preprint\\narXiv:1412.3555 , 2014. 4\\n[13] Zheng Dang, Lizhou Wang, Yu Guo, and Mathieu Salzmann.\\nLearning-based point cloud registration for 6d object pose', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='estimation in the real world. In ECCV , 2022. 1, 2\\n[14] Haowen Deng, Tolga Birdal, and Slobodan Ilic. Ppf-foldnet:\\nUnsupervised learning of rotation invariant 3d local descrip-\\ntors. In Proceedings of the European Conference on Com-\\nputer Vision (ECCV) , pages 602–618, 2018. 1\\n[15] Jean-Emmanuel Deschaud. IMLS-SLAM: Scan-to-model\\nmatching based on 3D data. In ICRA (2018) . 1\\n[16] Martin A Fischler and Robert C Bolles. Random sample\\nconsensus: a paradigm for model fitting with applications toimage analysis and automated cartography. Communications\\nof the ACM (1981) , 24(6):381–395. 1, 2, 5, 6, 7\\n[17] Andrea Frome, Daniel Huber, Ravi Kolluri, Thomas B ¨ulow,\\nand Jitendra Malik. Recognizing objects in range data us-\\ning regional point descriptors. In European conference on\\ncomputer vision , pages 224–237. Springer, 2004. 1, 2\\n[18] Kexue Fu, Shaolei Liu, Xiaoyuan Luo, and Manning Wang.\\nRobust point cloud registration framework based on deep', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='graph matching. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition , pages\\n8893–8902, 2021. 2\\n[19] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we\\nready for autonomous driving? the kitti vision benchmark\\nsuite. In 2012 IEEE conference on computer vision and pat-\\ntern recognition , pages 3354–3361. IEEE, 2012. 6, 7\\n[20] John C Gower. Generalized procrustes analysis. Psychome-\\ntrika , 40(1):33–51, 1975. 5\\n[21] Shengyu Huang, Zan Gojcic, Mikhail Usvyatsov, Andreas\\nWieser, and Konrad Schindler. Predator: Registration of\\n3d point clouds with low overlap. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 4267–4276, 2021. 1, 2, 3, 6, 7, 8\\n[22] Haobo Jiang, Jianjun Qian, Jin Xie, and Jian Yang. Planning\\nwith learned dynamic model for unsupervised point cloud\\nregistration. IJCAI (2021) . 2\\n[23] Haobo Jiang, Yaqi Shen, Jin Xie, Jun Li, Jianjun Qian, and\\nJian Yang. Sampling network guided cross-entropy method', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='for unsupervised point cloud registration, 2021. 2, 6\\n[24] Huu M Le, Thanh-Toan Do, Tuan Hoang, and Ngai-Man\\nCheung. SDRSAC: Semidefinite-based randomized ap-\\nproach for robust point cloud registration without correspon-\\ndences. In CVPR (2019) . 2\\n[25] Junha Lee, Seungwook Kim, Minsu Cho, and Jaesik Park.\\nDeep hough voting for robust global registration. In Proceed-\\nings of the IEEE/CVF International Conference on Com-\\nputer Vision , pages 15994–16003, 2021. 6, 7\\n[26] Marius Leordeanu and Martial Hebert. A spectral technique\\nfor correspondence problems using pairwise constraints.\\n2005. 6, 7\\n[27] Jiayuan Li, Qingwu Hu, and Mingyao Ai. Gesac: Robust\\ngraph enhanced sample consensus for point cloud registra-\\ntion. ISPRS Journal of Photogrammetry and Remote Sens-\\ning, 167:363–374, 2020. 2\\n[28] Jiahao Li, Changhao Zhang, Ziyao Xu, Hangning Zhou, and\\nChi Zhang. Iterative distance-aware similarity matrix convo-\\nlution with mutual-supervised point elimination for efficient', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='point cloud registration. In ECCV (2020) . 2\\n[29] Xiang Li, Lingjing Wang, and Yi Fang. PC-Net: Unsuper-\\nvised point correspondence learning with neural networks.\\nIn3DV (2019) . 2\\n[30] Xiang Li, Lingjing Wang, and Yi Fang. Unsupervised partial\\npoint set registration via joint shape completion and registra-\\ntion. arXiv preprint arXiv:2009.05290 (2020) . 2\\n[31] Yang Li and Tatsuya Harada. Lepard: Learning partial point\\ncloud matching in rigid and deformable scenes. In CVPR ,\\n2022. 2\\n1156\\n[32] David G Lowe. Distinctive image features from scale-\\ninvariant keypoints. International journal of computer vi-\\nsion, 60(2):91–110, 2004. 1\\n[33] G Dias Pais, Srikumar Ramalingam, Venu Madhav Govindu,\\nJacinto C Nascimento, Rama Chellappa, and Pedro Miraldo.\\n3DRegNet: A deep neural network for 3D point registration.\\nInCVPR (2020) . 1, 2\\n[34] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\\nPointNet: Deep learning on point sets for 3D classification', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and segmentation. In CVPR (2017) , pages 652–660. 2\\n[35] Zheng Qin, Hao Yu, Changjian Wang, Yulan Guo, Yuxing\\nPeng, and Kai Xu. Geometric transformer for fast and robust\\npoint cloud registration. In CVPR , 2022. 2\\n[36] Siwen Quan and Jiaqi Yang. Compatibility-guided sampling\\nconsensus for 3-d point cloud registration. IEEE Transac-\\ntions on Geoscience and Remote Sensing , 2020. 1\\n[37] Radu Bogdan Rusu, Nico Blodow, and Michael Beetz. Fast\\npoint feature histograms (fpfh) for 3d registration. In 2009\\nIEEE international conference on robotics and automation ,\\n2009. 1, 2, 6, 7\\n[38] Samuele Salti, Federico Tombari, and Luigi Di Stefano.\\nShot: Unique signatures of histograms for surface and tex-\\nture description. Computer Vision and Image Understand-\\ning, 125:251–264, 2014. 1, 2\\n[39] Johannes L Schonberger and Jan-Michael Frahm. Structure-\\nfrom-motion revisited. In CVPR (2016) . 1\\n[40] Yaqi Shen, Le Hui, Haobo Jiang, Jin Xie, and Jian Yang.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Reliable inlier evaluation for unsupervised point cloud regis-\\ntration. arXiv preprint arXiv:2202.11292 , 2022. 2\\n[41] Haiping Wang, Yuan Liu, Zhen Dong, Wenping Wang, and\\nBisheng Yang. You only hypothesize once: Point cloud reg-\\nistration with rotation-equivariant descriptors. arXiv preprint\\narXiv:2109.00182 , 2021. 1, 2\\n[42] Yue Wang and Justin M Solomon. Deep closest point: Learn-\\ning representations for point cloud registration. In ICCV\\n(2019) . 2\\n[43] Edwin B Wilson. Probable inference, the law of succession,\\nand statistical inference. Journal of the American Statistical\\nAssociation , 1927. 5\\n[44] Jay M Wong, Vincent Kee, Tiffany Le, Syler Wagner, Gian-\\nLuca Mariottini, Abraham Schneider, Lei Hamilton, Rahul\\nChipalkatty, Mitchell Hebert, David MS Johnson, et al.\\nSegicp: Integrated deep semantic segmentation and pose es-\\ntimation. In 2017 IEEE/RSJ International Conference on In-\\ntelligent Robots and Systems (IROS) , 2017. 1', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[45] Heng Yang, Jingnan Shi, and Luca Carlone. Teaser: Fast\\nand certifiable point cloud registration. IEEE Transactions\\non Robotics , 2020. 2, 6, 7\\n[46] Jiaqi Yang, Ke Xian, Peng Wang, and Yanning Zhang. A\\nperformance evaluation of correspondence grouping meth-\\nods for 3d rigid data matching. IEEE transactions on pattern\\nanalysis and machine intelligence , 2019. 1\\n[47] Zi Jian Yew and Gim Hee Lee. RPM-Net: Robust point\\nmatching using learned features. In CVPR (2020) . 2\\n[48] Zi Jian Yew and Gim Hee Lee. Regtr: End-to-end point\\ncloud correspondences with transformers. In CVPR , 2022. 2[49] Kwang Moo Yi, Eduard Trulls, Yuki Ono, Vincent Lepetit,\\nMathieu Salzmann, and Pascal Fua. Learning to find good\\ncorrespondences. In CVPR , 2018. 2\\n[50] Andy Zeng, Shuran Song, Matthias Nießner, Matthew\\nFisher, Jianxiong Xiao, and Thomas Funkhouser. 3dmatch:\\nLearning local geometric descriptors from rgb-d reconstruc-\\ntions. In Proceedings of the IEEE conference on computer', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='vision and pattern recognition , pages 1802–1811, 2017. 1,\\n2, 6, 7, 8\\n[51] Ji Zhang and Sanjiv Singh. Loam: Lidar odometry and map-\\nping in real-time. In Robotics: Science and Systems , 2014.\\n1\\n[52] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Fast global\\nregistration. In ECCV , 2016. 2, 6, 7\\n[53] Jing Zhu and Yi Fang. Reference grid-assisted network for\\n3D point signature learning from point clouds. In WACV\\n(2020) . 2\\n1157', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Contrastive Semi-supervised Learning for Underwater Image Restoration via\\nReliable Bank\\nShirui Huang1*, Keyan Wang1*†, Huan Liu2, Jun Chen2, Yunsong Li1\\n1Xidian University,2McMaster University,*Equal Contribution,†Corresponding Author\\nsrhuang@stu.xidian.edu.cn, {kywang, ysli }@mail.xidian.edu.cn, {liuh127, chenjun }@mcmaster.ca\\nAbstract\\nDespite the remarkable achievement of recent underwa-\\nter image restoration techniques, the lack of labeled data\\nhas become a major hurdle for further progress. In this\\nwork, we propose a mean-teacher based Semi -supervised\\nUnderwater Image Restoration ( Semi-UIR ) framework to\\nincorporate the unlabeled data into network training. How-\\never, the naive mean-teacher method suffers from two main\\nproblems: (1) The consistency loss used in training might\\nbecome ineffective when the teacher’s prediction is wrong.\\n(2) Using L1 distance may cause the network to over-\\nfit wrong labels, resulting in confirmation bias. To ad-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='dress the above problems, we first introduce a reliable\\nbank to store the “best-ever” outputs as pseudo ground\\ntruth. To assess the quality of outputs, we conduct an\\nempirical analysis based on the monotonicity property to\\nselect the most trustworthy NR-IQA method. Besides, in\\nview of the confirmation bias problem, we incorporate con-\\ntrastive regularization to prevent the overfitting on wrong\\nlabels. Experimental results on both full-reference and non-\\nreference underwater benchmarks demonstrate that our\\nalgorithm has obvious improvement over SOTA methods\\nquantitatively and qualitatively. Code has been released at\\nhttps://github.com/Huang-ShiRui/Semi-UIR.\\n1. Introduction\\nDue to light refraction, absorption and scattering in un-\\nderwater scenes, images taken in the water usually suffer\\nseverely from color distortion, low contrast and blur. Im-\\nages with these defects tend to be less visually appealing\\nand can potentially hinder the well-functioning of underwa-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ter robotic systems. Recently, many deep learning based\\nmethods [5–7, 24, 51] have been proposed to address im-\\nage restoration problems. Numerous efforts have also been\\ndevoted to the specific domain of underwater image restora-\\n1This work is supported by the Nature Science Foundation of Shaanxi\\nProvince of China (2021JM-125).\\n(a) UIEBD\\n(c)EUVP(b) UWCNN\\nFigure 1. Examples from different benchmarks. (a) shows real-\\nworld underwater images from UIEB [22] with degraded images\\n(first and second row). (b) shows the UWCNN training set [21]\\n(synthesized based on the image formation model) and (c) shows\\nthe EUVP dataset [17] (synthesized by GAN). The ambient light\\nand color cast of (b) and (c) are quite different from that of (a).\\ntion [11, 17, 20, 22, 49]. Compared with traditional methods\\nthat mostly rely on hand-crafted priors, deep learning based\\nsolutions are able to deliver superior restoration results due\\nto their data-driven nature.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Despite their success, most of deep learning based meth-\\nods are designed to learn the restoration mapping on paired\\ndatasets in a supervised manner. As is known, it is ex-\\ntremely hard, if not impossible, to acquire paired underwa-\\nter images in real scenes. The existing datasets for under-\\nwater image restoration have several non-negligible issues:\\n(1)Lack of real data. A popular way to construct paired\\ndatasets is to synthesize underwater images using some\\nphysical model [21] or GAN [17, 48]. However, there is a\\nsignificant discrepancy between synthesized and real data.\\nAs is shown in Fig. 1, the ambient light and color cast of\\nsynthetic data are quite different from the real counterparts.\\nDue to domain shift, models trained on synthetic datasets\\noften exhibit poor generalization in real scenes. Another\\nway [22] is to manually construct pseudo labels by select-\\ning the best results among those produced by traditional al-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n18145\\ngorithms. Inevitably, the quality of the pseudo ground truth\\nis restricted by the restoration capability of traditional algo-\\nrithms. (2) Limited data size. The current benchmarks only\\nprovide a very limited amount of paired data. For example,\\nin UIEB [22], there are only 890 paired underwater images.\\nModels learned on such a small dataset may run the risk of\\noverfitting. In comparison, the standard datasets for image\\nclassification, such as ImageNet [35], are several orders of\\nmagnitude larger in size.\\nOn the other hand, unlabeled underwater images are rel-\\natively easy to collect. The challenge is how to make ef-\\nfective use of these unlabeled data. Semi-supervised learn-\\ning, which capitalizes on both labeled and unlabeled data', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='for model training, is best suited in this kind of scenarios.\\nThis motivates us to propose a semi-supervised scheme with\\nthe goal of improving the generalization of the resulting\\nmodel on real-world underwater images. To be specific, we\\nadopt the mean teacher method [40] as the basis. The mean\\nteacher method finds a way to obtain pseudo labels for un-\\nlabeled data and utilizes a consistency loss to improve the\\naccuracy and robustness of the network. Specifically, it con-\\nstructs a teacher model with improved performance from a\\nstudent model via the exponential moving average (EMA)\\nstrategy. The teacher’s prediction serves as the pseudo la-\\nbel to guide the training of the student. However, it is a\\nnon-trivial task to tailor the mean teacher method to the un-\\nderwater image restoration problem. The reasons are as fol-\\nlows: (1) There is no guarantee that the teacher can con-\\nsistently outperform the student. Wrong pseudo labels may', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='jeopardize the training of the student network. (2) The com-\\nmonly used consistency loss is based on L1 distance. The\\n“strict” L1 loss can easily make the model overfit wrong\\npredictions, resulting in confirmation bias.\\nTo address the first issue, we construct a reliable bank to\\narchive the best-ever outputs from the teacher as pseudo la-\\nbels. The main challenge here is how to determine what are\\nthe “best-ever” outputs? Intuitively, non-reference image\\nquality assessment (NR-IQA) can be leveraged to evaluate\\nthe quality of each output. However, as noted in [3, 10, 22],\\nthe current NR-IQA metrics for underwater images are, to\\nsome extent, inconsistent with human visual perception. To\\nidentify the right one for our purpose, we compare several\\nNR-IQA metrics using the monotonicity property as the re-\\nliability criterion. Our empirical analysis suggests MUSIQ\\n[19] best meets the criterion. For the second issue, we in-\\ntroduce contrastive learning as a supplementary regulariza-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tion to alleviate overfitting. Unlike those conventional loss\\nfunctions that are only concerned with how close the out-\\nputs and ground truths are, contrastive loss provides addi-\\ntional supervision to prevent the degradation of the outputs.\\nIn this sense, contrastive regularization is ideally suited to\\nour semi-supervised learning framework since we only have\\naccess to the degraded images in the unlabeled dataset. Itenables the model to take advantage of unlabeled data.\\nIn summary, our main contributions are as follows: (1)\\nWe propose a mean teacher based semi-supervised un-\\nderwater image restoration framework named Semi-UIR,\\nwhich effectively leverages the knowledge from unlabeled\\ndata to improve the generalization of the trained model on\\nreal-world data. (2) We evaluate teacher outputs by a judi-\\nciously chosen NR-IQA metric and build a reliable bank to\\nstore best-ever teacher outputs, which ensures the reliabil-\\nity of pseudo-labels. (3) We adopt contrastive loss as a form', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='of regularization to alleviate confirmation bias. (4) Exten-\\nsive experimental results demonstrate the effectiveness of\\nour proposed methods.\\n2. Related Work\\n2.1. Underwater Image Restoration Methods\\nTraditional underwater image restoration methods can\\nbe categorized into model-based and model-agnostic meth-\\nods. Model-based methods [3, 8, 33] use hand-crafted pri-\\nors to estimate unknown parameters of underwater imaging\\nmodel [36], such as transmission and ambient light. In con-\\ntrast, model-agnostic methods rely on the design of appro-\\npriate image enhancement techniques such as CLAHE [14],\\nRetinex [52], fusion [2] and MMLE [53]. Despite their suc-\\ncess, the traditional methods usually fail to cope with com-\\nplex real scenes.\\nMost of early deep learning based underwater image\\nrestoration methods [18,41] accomplish the goal by exploit-\\ning physical imaging models. Specifically, they make use\\nof neural networks to estimate the transmission and am-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='bient light. However, inaccurate estimation of these pa-\\nrameters hinders such methods from achieving good perfor-\\nmance. Recent years have seen many deep learning based\\nmethods that directly learn the restoration mapping from\\nthe labeled dataset in a supervised manner without resort-\\ning to imaging models. [22] employs an effective network\\nto fuse three feature maps enhanced by traditional model-\\nagnostic methods. [20] designs a multi-color space encoder\\nand a transmission-guided decoder by leveraging the ideas\\nfrom traditional model-based approaches. [16] proposes a\\nwavelet boost learning strategy, through which features in\\nthe frequency domain are utilized for fine detail restora-\\ntion. [17] introduces an end-to-end network based on GAN\\nfor the purpose of real-time inference.\\n2.2. Semi-supervised Learning\\nIn recent years, semi-supervised learning [55] has played\\nan increasingly important role in tackling computer vision\\nproblems. It focuses on making effective use of both labeled', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and unlabeled data. Many semi-supervised methods have\\nbeen developed, such as mean teacher [40], virtual adver-\\nsarial learning [30] and FixMatch [38]. Among them, the\\n18146\\nmean teacher method [40], which is based on consistency\\nregularization, has achieved remarkable success in semi-\\nsupervised image recognition. This success also triggers its\\napplications to other vision tasks such as semantic segmen-\\ntation [15, 26, 44] and image restoration [27, 42]. Unfortu-\\nnately, to the best of our knowledge, semi-supervised learn-\\ning is rarely explored in underwater image restoration. [54]\\nmakes an initial attempt in this direction by training a sin-\\ngle network with both supervised and unsupervised losses.\\nIn comparison, we adopt a more systematic approach and\\nintroduce several techniques in handling unlabeled data, in-\\ncluding mean teacher, reliable bank and contrastive loss.\\n2.3. Contrastive Learning\\nContrastive learning has emerged as an effective', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='paradigm in self-supervised learning [4, 9, 12]. It enables\\nvisual representation to be learned empirically by instance\\ndiscrimination, through which similar samples are kept\\nclose to each other whereas dissimilar samples are sepa-\\nrated far apart. To take advantage of contrastive learning\\nin image restoration, previous works largely focus on the\\nconstruction of contrastive samples and feature space. For\\nexample, [23, 46] take clean images as positive instances\\nand degraded images as negative instances and then project\\nthem into a new feature space by VGG [37]. Note that\\nin the above works, contrastive loss is applied in a super-\\nvised manner, which is infeasible for unlabeled data. How\\nto make contrastive loss applicable to unlabeled data re-\\nmains an open problem. [11] is the first to employ con-\\ntrastive learning in the context of underwater image restora-\\ntion. However, it is still a supervised learning method in\\nnature, and contrastive loss is used as a regularization term', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='to boost the performance of supervised learning. Against\\nthis backdrop, the present work puts forward a systematic\\napproach for making use of contrastive learning to exploit\\nunlabeled data.\\n3. Method\\n3.1. Problem Formulation\\nSemi-supervised learning aims to enable a learning sys-\\ntem to learn from both labeled and unlabeled data. The\\nproblem for underwater image restoration is defined as fol-\\nlows. Let DL={(xl\\ni, yl\\ni)|xl\\ni∈ ILQ\\ns, yl\\ni∈ IHQ\\ns}N\\ni=1\\ndenote the labeled dataset, where xl\\niandyl\\niare respec-\\ntively the underwater image and clean ground truth from\\ndegraded set ILQ\\nsand ground truth set IHQ\\ns. Similarly, let\\nDU={xu\\ni|xu\\ni∈ ILQ\\nu}M\\ni=1denote the unlabeled dataset,\\nwhere xu\\niis the underwater image sampled from the de-\\ngraded set ILQ\\nu. It is worth mentioning that the data in DL\\nandDUare disjoint, i.e.DL∩DU=∅. Our goal is to learn\\na mapping on D=DL∪DUthat converts an underwater\\nimage xto its clean counterpart y.3.2. Semi-supervised Underwater Restoration', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Our semi-supervised learning framework follows the\\ntypical setup in semi-supervised learning [38, 40], as illus-\\ntrated in Fig. 2. Specifically, our Semi-UIR consists of two\\nnetworks of the same structure, called teacher and student\\nrespectively. The two networks differ mainly in how their\\nweights are updated.\\nThe teacher’s weights θtare updated by exponential\\nmoving average (EMA) of the student’s weights θs:\\nθt=ηθt+ (1−η)θs, (1)\\nwhere η∈(0,1)is the momentum. Using this update strat-\\negy, the teacher model can aggregate previously learned\\nweights immediately after each training step. As is noted\\nin [34], temporal weight averaging can stabilize the training\\nprocess and help improve the performance compared with\\nstandard gradient descent.\\nThe weights of student network θsare updated using gra-\\ndient descent. Usually, the optimization of the student net-\\nwork can be formulated as minimizing the following loss:\\nLtotal=Lsup+ηLun, (2)\\nwhere Lsup=PN\\ni=0|fθs(xl\\ni)−yl\\ni|denotes the supervised', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='loss and Lun=PM\\ni=0|fθs(ϕs(xu\\ni))−fθt(ϕt(xu\\ni))|repre-\\nsents the unsupervised teacher-student consistency loss. |·|\\nrefers to L1 distance. ϕsandϕtare respectively the data\\naugmentations of student’s inputs and teacher’s inputs.\\nIdeally, since the teacher network is in general better than\\nthe student network, Luncould provide effective supervi-\\nsion to train the student network on the unlabeled dataset.\\nWe thus refer to the teacher’s output ˆyu\\ni=fθt(ϕt(xu\\ni))as\\npseudo label. However, it is not guaranteed that the outputs\\nof the teacher are consistently better than those of the stu-\\ndent. Wrong pseudo labels can potentially jeopardize the\\ntraining of the student network.\\n3.3. Reliable Teacher-Student Consistency\\nTo address the above issue, we shall select the reliable\\noutputs of teacher as pseudo labels. In image classification\\nand semantic segmentation [15,38,44], the reliability of the\\nnetwork’s outputs is usually measured by entropy and con-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='fidence. However, the extension to image restoration prob-\\nlems does not appear to be straightforward due to the pres-\\nence of new challenges. In particular, as a regression task,\\nunderwater image restoration requires recovering fine tex-\\ntures and removing color cast.\\nTo this end, we propose a reliable bank to store the best-\\never outputs of the teacher network during the training pro-\\ncess. To be specific, We first initialize our reliable bank to\\nbe an empty set, i.e.BU=∅. In each training iteration,\\nwe compare the current output of teacher with both the stu-\\ndent’s output and the pseudo label in the reliable bank. If\\n18147\\nBatch 2AIM-Net(Student)\\nAIM-Net(Teacher)\\nGround Truth\\n𝑳𝒔𝒖𝒑$\\nNR-IQA\\nReliable Bank\\nUpdateBatch 1𝑳𝒖𝒏$Strongly Augmented\\nWeakly AugmentedBatch 0Normally Augmented\\n𝑥!\"Labeled Data\\nUnlabeled Data……\\n𝜙#(𝑥!\")\\n𝜙$(𝑥!\")Samplepullpush𝑳𝒄𝒓', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='NegativeAnchorPositiveReliable Pseudo Label 𝑥!%𝑦!%𝑦!&Figure 2. Illustration of our framework Semi-UIR. Semi-UIR is based on the mean teacher scheme with a student model and a teacher\\nmodel. To guarantee the reliability of pseudo labels for unlabeled data, we build a reliable bank to archive best-ever teacher outputs\\nmeasured by NR-IQA. Reliable pseudo labels guide the student’s training via the unsupervised teacher-student consistency loss L′\\nunand\\ncontrastive loss Lcr. The weights of the student are updated by minimizing the supervised loss ( L′\\nsup) and unsupervised losses ( L′\\nunand\\nLcr). The teacher is updated with EMA from the student.\\nthe teacher’s output is the best in quality, then we replace\\nthe pseudo label in the reliable bank with the teacher’s cur-\\nrent output. In this way, we could maintain a reliable bank\\nBU={yb\\ni}M\\ni=1. Note that D′=DU∪ BU={(xu\\ni, yb\\ni)}M\\ni=1\\nis a pseudo labeled dataset. This reliable bank can keep', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='track of the best pseudo labels and therefore avoid the\\nwrong labels involved in the calculation of the unsupervised\\nconsistency loss Lun. Then, we can re-write the Lunin Eq.\\n(2) as:\\nL′\\nun=MX\\ni=0|fθs(ϕs(xu\\ni))−yb\\ni|. (3)\\nNow arises the obvious question: How to determine the\\nquality of a prediction without the true label?\\n3.4. Reliable Metric Selection\\nIntuitively, we could resort to non-reference image qual-\\nity assessment (NR-IQA). Unfortunately, as is noted in\\n[3, 22], the commonly used UCIQE [47] and UIQM [31]\\ncannot accurately reflect the quality of restored underwater\\nimages. Therefore, building our reliable bank based on such\\nmetrics is questionable. To find the best possible NR-IQA\\nfor underwater images, we conduct an empirical analysis of\\nseveral NR-IQA metrics.\\nGiven a degraded underwater image xland a paired\\nclean image yl, we perform various linear combination of\\nthem to get a set of images with different quality. Specif-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ically, let αi= 0.1×i, i= 1,2, ...,10, we can obtain a\\nset of ten images {αixl+(1−αi)yl}10\\ni=1. With the increase\\nofαi, the visual quality of the corresponding image deterio-\\nrates, as shown in Fig. 3. It thus makes sense to evaluate the\\nNR-IQA metrics based on how well they capture this mono-\\ntonicity law. In particular, an NR-IQA metric is identified\\nas reliable if its score on the αixl+ (1−αi)yldecreasesAlgorithm 1 Update of Reliable Bank\\nRequire: NR-IQA method Ψ(·);\\nInitialize BU=∅;\\nSample a batch of unlabeled images {xu\\ni}b\\ni=1from DU;\\nforeachxu\\nido\\nGet teacher’s prediction: ˆyu\\ni=fθt(ϕt(xu\\ni));\\nGet student prediction: ˜yu\\ni=fθs(ϕs(xu\\ni));\\nCompute NR-IQA scores of ˆyu\\ni,˜yu\\niandyb\\ni∈ BU:\\nzt= Ψ(ˆ yu\\ni),zs= Ψ(˜ yu\\ni),zb= Ψ( yb\\ni);\\nifzt> zsandzt> zpthen\\nReplace the yb\\niinBUbyˆyu\\ni;\\nend if\\nend for\\nFigure 3. Examples of image fusion based on different α.\\nwith the increase of αi. Following this rule, we conduct ex-\\nperiments with seven NR-IQA approaches on EUVP bench-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='mark [17], as it covers a wide range of underwater scenes.\\nThe experimental results are shown in Fig. 4. We can ob-\\nserve that the deep learning based MUSIQ [19] is most in\\nline with the monotonicity law. Therefore, it is selected to\\nmeasure the reliability of the networks’ outputs.\\nThe overall procedure of our reliable bank construction\\nis summarized in Algorithm 1.\\n3.5. Contrastive Regularization\\nTypically, many mean teacher based methods uses L1\\ndistance as consistency loss as is shown by Lunin Eq. (2).\\n18148\\n13.45%48.16%48.69%76.87%\\n41.05%82.11%91.21%\\nNIQEUCIQEBRISQUEUIQMNIMAPAQ2PIQMUSIQNR-IQA Methods0102030405060708090100Reliability (%)Figure 4. The results of different non-reference IQA indicators on\\nEUVP benchmark, including UIQM [31], UCIQE [47], BRISQUE\\n[28], NIQE [29], NIMA [39], PAQ2PIQ [50] and MUSIQ [19].\\nThe simple consistency loss can easily make the student\\nmodel overfit on wrong predictions, resulting in confirma-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tion bias. To address this problem, we introduce contrastive\\nloss in the training. Contrastive learning has emerged as an\\neffective paradigm in the self-supervised domain [4, 9, 12].\\nThe goal of contrastive learning is to enable a model to pro-\\nduce a similar representation of positive pairs and a dissim-\\nilar representation of negative pairs. Recently, it has been\\nextended to address image restoration problems [23, 46].\\nDespite the tremendous success, they usually construct con-\\ntrastive loss on paired datasets, where the positive andneg-\\native samples are respectively the labels and degraded im-\\nages. In this section, we propose to incorporate contrastive\\nloss in handling unlabeled data. To achieve this, we first\\nneed to construct positive and negative pairs. [43] provide\\nan idea of directly using the teacher’s output as positive\\nsamples. However, due to the wrong label problem we have\\ndiscussed in the previous sections, using the teacher’s out-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='puts as positive samples might be harmful.\\nThanks to our proposed reliable bank, where the samples\\nare potentially of higher quality than the student’s outputs,\\nwe can take yb\\nias our positive sample. For the negative sam-\\nple, we follow [23, 43, 46] to take the strongly augmented\\ndegraded image ϕs(xu\\ni)as our negative sample. After con-\\nstructing the positive and negative samples, we can calcu-\\nlate the contrastive loss as follows:\\nLcr=KX\\nj=1MX\\ni=1ωj|φj(˜yu\\ni), φj(yb\\ni)|\\n|φj(˜yu\\ni), φj(ϕs(xu\\ni))|, (4)\\nwhere ˜yu\\ni=fθs(ϕs(xu\\ni))is the student’s prediction on the\\nunlabeled dataset DU.φj(·)represents the jthhidden layer\\nof the pre-trained VGG-19 [37] and ωjis the weight coef-\\nficient. We use L1 loss to measure the distance in feature\\nspace between the students’ outputs with the positive and\\nnegative samples.\\nAIM-Net\\nGradient BranchIllumination-aware Restoration BranchfuseConvConv𝒙𝑳Conv', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='InputIllumination MapOutputGradient OutputFigure 5. An overview of the proposed Asymmetric Illumination-\\naware Multi-scale Network (AIM-Net).\\n3.6. Overall Optimization Objective\\nSimilar to Eq. (2), our final optimization objective con-\\nsists of supervised loss and unsupervised loss.\\nFor the supervised loss, unlike the one defined in Eq.\\n(2) that only calculate the L1 distance, we follow [16] to\\nextend the original Lsupby adding perceptual loss Lperand\\ngradient penalty Lgrad:\\nL′\\nsup=Lsup+β1Lper+β2Lgrad. (5)\\nFor the unsupervised loss, we replace the original Lun\\nby a combination of the proposed reliable teacher-student\\nconsistency loss and contrastive loss:\\nL′′\\nun=L′\\nun+γLcr. (6)\\nFinally, we rewrite our overall optimization objective\\nfollowing Eq. (2):\\nLoverall =L′\\nsup+λL′′\\nun. (7)\\nDue to the page limit, please refer to supplementary ma-\\nterial for the detailed perceptual loss and gradient penalty.\\n4. Experimental Results\\n4.1. Implementation Details', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Network Structure Our student and teacher model are\\nbased on the same structure, AIM-Net. To tackle the promi-\\nnent issues with underwater images (e.g., low contrast,\\ncolor distortion and blur), certain prior information of such\\nimages (e.g., illumination prior and gradient prior) are ef-\\nfectively exploited. As is shown in Fig. 5, the network con-\\nsists of two branches: illumination-aware restoration branch\\nand gradient branch. The restoration branch incorporates il-\\nlumination prior to enhance the color and light source per-\\nception capabilities. The gradient branch is introduced to\\nenhance the edge structure. Please refer to supplementary\\nmaterial for detailed network structure.\\nTraining Details Our method is implemented using Pytorch\\nlibrary [32] and conducted on NVIDIA RTX 3090 GPUs.\\nWe use AdamP [13] as our optimizer. In consideration of\\nits fast convergence to optimum, we select AdamP mainly\\n18149', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='18149\\nInputGDCPMMLEWaterNetUcolorPRWNetFUnIEGANCWROursGorundTruthFigure 6. Visual comparisons of full-reference data from UIEB benchmark.\\nTable 1. Evaluations of different methods on full-reference bench-\\nmarks in terms of PSNR and SSIM. Best results are in bold and\\nthe second best results are with underline .\\nMethodtestS testR\\nPSNR↑ SSIM↑ PSNR↑ SSIM↑\\nInput 14.64 0.641 18.23 0.746\\nGDCP [33] 12.89 0.576 15.78 0.757\\nMMLE [53] 12.76 0.651 20.01 0.781\\nWaterNet [22] 15.44 0.706 21.58 0.858\\nUcolor [20] 23.32 0.853 22.92 0.881\\nPRWNet [16] 17.27 0.723 20.98 0.848\\nFGAN [17] 18.54 0.743 19.41 0.824\\nCWR [11] 14.79 0.697 21.87 0.815\\nSemi-UIR 23.40 0.821 24.59 0.901\\nto reduce the training time. During training, we use a mini-\\nbatch size of 16, where 8 samples are labeled and 8 sam-\\nples are unlabeled. The initial learning rate is set to 2e−4.\\nWe train for 200 epochs with the learning rate multiplied\\nby 0.1 at 100 epochs. The training images are all cropped', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='to a size of 256×256. For the data augmentation on un-\\nlabeled data, we only apply resize on the teacher’s inputs\\nand impose strong data augmentation on the student’s in-\\nputs. The strong augmentation includes resize, color jitter,\\ngaussian blur and gray scale. Labeled data is normally aug-\\nmented, including resize, random crop and rotation. The\\nweights of different loss components are set as follows:\\nβ1= 0.3,β2= 0.1,γ= 1 andλis updated with training\\nepoch tfollowing an exponential warming up function [27]:\\nλ(t) = 0 .2×e−5(1−t/200)2.\\n4.2. Datasets\\nOur training set contains 1600 labeled image pairs and\\n1600 unlabeled images. The labeled image pairs are ran-\\ndomly sampled from [21] and UIEB [22] with a ratio of\\n1:1. [21] provides a synthesized underwater image dataset\\nin indoor scene. The UIEB [22] dataset contains 890 real\\nunderwater images with corresponding ground truths. The\\nunlabeled images are sampled from the unpaired data in the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='EUVP benchmark [17], which cover a variety of underwater\\nscenes, water types and lighting conditions.\\nTest set is built with full-reference and non-referencebenchmarks. Full-reference test set includes 110 pairs from\\n[21] and 90 pairs from UIEB [22], namely testS and testR.\\nNon-reference test set includes nearly 700 real world under-\\nwater images without ground truths from benchmarks such\\nas UIEB, EUVP, RUIE [25] and Seathru [1].\\n4.3. Comparison with the State-of-the-Arts\\nWe compare our proposed Semi-UIR with seven state-\\nof-the-art underwater restoration methods, including two\\ntraditional methods (GDCP [33], MMLE [53]) and five\\ndeep learning based methods (WaterNet [22], Ucolor [20],\\nFUnIE-GAN [17], PRWNet [16] and CWR [11]). All the\\ncompared methods are re-trained on our training dataset.\\nFor testS and testR, we conduct full-reference evaluations\\nusing PSNR, SSIM [45]. For non-reference test set, we pro-\\nvide evaluation results using UIQM [31], UCIQE [47] and\\nMUSIQ [19].', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='MUSIQ [19].\\nResults on full-reference datasets. The quantitative re-\\nsults on testS and testR are shown in Table 1. On testS,\\nour method performs the best in PSNR, but slightly worse\\nthan Ucolor in terms of SSIM. One potential reason is that\\nincorporating unlabeled real underwater images in training\\nmight emphasize the network to pay more attention to the\\nreal underwater scene. This can be confirmed by checking\\nthe quantitative results on testR. On testR, our method out-\\nperforms the other methods by a significant margin (outper-\\nforms the second best by 1.67dB in PSNR). In addition to\\nthe quantitative results, qualitative results are shown in Fig.\\n6. Our results are visually pleasant, while the compared\\nmethods suffer from color cast and over-enhancement.\\nResults on non-reference datasets. The quantitative re-\\nsults on UIEB, EUVP, RUIE and Seathru are shown in Ta-\\nble 2. By quickly checking throughout the table, we can\\nobserved that our method significantly outperform the com-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='pared method in MUSIQ. Besides, we also achieve compet-\\nitive performance in terms of UIQM and UCIQE. However,\\nas is noted in [3, 10, 22], UIQM and UCIQE might be bi-\\nased to some characteristics and thus cannot accurately re-\\nflect the true visual quality of restored images. Similarly,\\nthe performance in MUSIQ is also for a reference. There-\\nfore, the quantitative results might be insufficient to indicate\\nthe quality of restored underwater images due the underde-\\n18150\\nTable 2. Evaluations of different methods on non-reference benchmarks in terms of UIQM, UCIQE and MUSIQ. Best results are in bold\\nand the second best results are with underline .\\nMethodUIQM (higher, better) UCIQE (higher, better) MUSIQ (higher, better)\\nUIEB EUVP RUIE Seathru UIEB EUVP RUIE Seathru UIEB EUVP RUIE Seathru\\nInput 3.066 4.729 3.948 5.925 0.509 0.517 0.490 0.537 41.70 42.73 33.53 60.25\\nGDCP [33] 3.401 4.738 4.509 5.343 0.564 0.599 0.565 0.590 40.07 42.49 34.63 60.54', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='MMLE [53] 4.283 4.723 4.967 5.555 0.578 0.596 0.571 0.620 40.33 47.55 36.80 66.16\\nWaterNet [22] 4.118 5.317 4.568 6.829 0.572 0.595 0.572 0.610 40.32 43.07 32.23 64.38\\nUcolor [20] 3.894 5.286 4.426 6.752 0.542 0.566 0.534 0.594 40.08 41.81 33.66 64.44\\nPRWNet [16] 4.371 5.330 4.395 6.778 0.518 0.543 0.518 0.572 40.30 43.52 33.12 62.82\\nFGAN [17] 4.315 4.469 4.519 4.853 0.541 0.561 0.527 0.564 40.95 43.36 34.48 64.25\\nCWR [11] 4.133 5.152 4.469 6.067 0.587 0.596 0.565 0.624 38.46 41.46 31.25 64.21\\nSemi-UIR 4.598 5.291 4.671 6.846 0.587 0.593 0.557 0.632 43.77 51.66 37.87 66.61\\n(a)UIEB(b)EUVP(c)RUIE(d)SeathruInputGDCPMMLEWaterNetUcolorPRWNetFUnIEGANCWROurs\\nFigure 7. Visual comparisons on non-reference benchmarks UIEB [22], EUVP [17], RUIE [25] and Seathru [1]\\nveloped NR-IQA metrics. We further show the qualitative\\nresults on the four benchmarks in Fig. 7. Compared with\\nother methods, our approach can robustly restore various\\ntypes of underwater images with natural color and rich de-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tails. Under the guidance of reliable pseudo labels and pow-\\nerful contrastive regularization, our framework generalizes\\nwell on various underwater scenes.\\n4.4. Ablation Study\\nTo analyze the effectiveness of Semi-UIR, we conduct\\nablation studies to reveal the influence of the key compo-nents in our method. They are presented as follows: (a)\\nSup-base : where we train the network AIM-Net without\\nsemi-supervised learning and unlabeled data. (b) Semi-\\nbase : Base semi-supervised training with consistency loss\\nLun. (c) Semi-base+RB* : using reliable bank based on\\nSemi-base without contrastive loss. (d) Semi-base+CL* :\\nadding contrastive loss to Semi-base , without using reliable\\nbank. (e) Semi-UIR : our proposed Semi-UIR.\\nThe quantitative results of above methods are shown in\\nTable 3. We can observe that our full solution performs best.\\nIn addition, by comparing Semi-base+RB* with Semi-base\\n18151\\nFigure 8. Results of ablation study about Semi-UIR.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Table 3. Ablation studies on testR, EUVP and UIEB benchmarks\\nin terms of PSNR or MUSIQ. MT denotes mean teacher frame-\\nwork, CL represents contrastive loss, and RB is reliable bank.\\nMethod MT RB CL testR EUVP UIEB\\nSup-base 24.38 42.66 40.70\\nSemi-base√23.11 42.48 40.33\\nSemi-base+RB*√ √24.53 43.27 42.16\\nSemi-base+CL*√ √23.97 46.59 40.64\\nSemi-UIR√ √√24.59 51.66 43.77\\nand Semi-UIR with Semi-base+CL* , it is easy to verify the\\neffectiveness of incorporating the reliable bank.\\nBesides, the qualitative results are shown in Fig. 8,\\nwhere special attention should be paid on Semi-base+CL*\\nSemi-base+RB* . (1) In Semi-base+CL* , without the re-\\nliable positive samples, the contrastive loss pushes the net-\\nwork to produce extremely different results than the neg-\\native samples (inputs). However, this unfortunately re-\\nsults in over-enhancement. (2) On the contrary, in Semi-\\nbase+RB* , without the help of contrastive loss, the restored\\nimages still suffer from color distortion and are close to the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='degraded inputs. The two ablation studies verify the utility\\nof the reliable bank and contrastive regularization.\\n4.5. Breakdown of the Training\\nTo further illustrate that the teacher’s outputs can be used\\nto train student network, we here provide some intermedi-\\nate results during training. The results are shown in Fig. 9.\\nAt the beginning of the training (10 epochs), the teacher’s\\nprediction is much better than that of student. As the train-\\ning processes, the student’s outputs and teacher’s outputs\\nare improved simultaneously.\\n4.6. Influence of Non-reference Metric\\nWe here conduct experiments to show the influence of\\nusing different NR-IQA approaches in building our reliable\\nbank. We conclude in Sec. 3.4 that MUSIQ is the most reli-\\nable one. To further demonstrate the correctness of this se-\\nlection, we here show the final performance of using NIMA,\\nPAQ2PIQ and MUSIQ on the labeled dataset, i.e. testS and\\ntestR. Table 4 shows the results. It can be observed that', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Figure 9. Examples of intermediate predictions of the teacher\\nmodel and student model.\\nwe can achieve the best performance by using MUSIQ. It\\nalso shows that using PAQ2PIQ is better than using NIMA,\\nwhich is consistent with their reliability shown in Fig. 4.\\nTable 4. Evaluation the influence of adopting different NR-IQA\\nmetrics on testS and testR.\\nMethod ReliabilityPSNR SSIM\\ntestS testR testS testR\\nNIMA 41.05% 23.01 23.88 0.815 0.888\\nPAQ2PIQ 82.11 % 23.08 24.28 0.818 0.893\\nMUSIQ 91.21 % 23.40 24.59 0.821 0.901\\n4.7. Influence of Data Augmentation\\nWe finally show the influence of using different data aug-\\nmentations in Table 5. By comparing the method using\\ndata augmentation with baseline, it is easy to conclude that\\nadopting any of the data augmentations is beneficial. Be-\\nsides, using a mixture of the three strategies achieves the\\nbest performance.\\nTable 5. Evaluation of using different data augmentation. Baseline\\nis our full solution without using the three strong data augmenta-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tions. Numbers are either in SSIM or MUSIQ.\\nStrategy testR UIEB EUVP RUIE Seathru\\nBaseline 0.880 40.12 46.06 31.14 64.71\\nColor Jitter 0.889 40.31 49.16 33.66 64.87\\nGaussian Blur 0.896 41.23 49.27 36.88 64.88\\nGray Scale 0.895 40.61 47.57 32.51 65.19\\nAll 0.901 43.77 51.66 37.87 66.61\\n5. Conclusion\\nWe propose an efficient semi-supervised underwater im-\\nage restoration method named Semi-UIR. As demonstrated\\nthe ablation experiments, the superior performance of the\\nproposed method over other SOTA algorithms can be at-\\ntributed to reliable teacher-student consistency and con-\\ntrastive regularization. The follow-up research can be car-\\nried out in two directions: 1) extend the semi-supervised\\nframework to cover other restoration tasks, 2) optimize\\nmemory usage during training and improve performance via\\nmemory management.\\n18152\\nReferences\\n[1] Derya Akkaynak and Tali Treibitz. Sea-thru: A method for\\nremoving water from underwater images. In Proceedings of', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the IEEE/CVF conference on computer vision and pattern\\nrecognition , pages 1682–1691, 2019. 6, 7\\n[2] Codruta O Ancuti, Cosmin Ancuti, Christophe\\nDe Vleeschouwer, and Philippe Bekaert. Color bal-\\nance and fusion for underwater image enhancement. IEEE\\nTransactions on image processing , 27(1):379–393, 2017. 2\\n[3] Dana Berman, Deborah Levy, Shai Avidan, and Tali Treibitz.\\nUnderwater single image color restoration using haze-lines\\nand a new quantitative dataset. IEEE transactions on pattern\\nanalysis and machine intelligence , 43(8):2822–2837, 2020.\\n2, 4, 6\\n[4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\\noffrey Hinton. A simple framework for contrastive learning\\nof visual representations. In International conference on ma-\\nchine learning , pages 1597–1607. PMLR, 2020. 3, 5\\n[5] Zhixiang Chi, Xiao Shu, and Xiaolin Wu. Joint demosaick-\\ning and blind deblurring using deep convolutional neural net-\\nwork. In 2019 IEEE International Conference on Image Pro-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='cessing (ICIP) , pages 2169–2173. IEEE, 2019. 1\\n[6] Zhixiang Chi, Yang Wang, Yuanhao Yu, and Jin Tang. Test-\\ntime fast adaptation for dynamic scene deblurring via meta-\\nauxiliary learning. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition , pages\\n9137–9146, 2021. 1\\n[7] Minghan Fu, Huan Liu, Yankun Yu, Jun Chen, and Keyan\\nWang. Dw-gan: A discrete wavelet transform gan for nonho-\\nmogeneous dehazing. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition , pages\\n203–212, 2021. 1\\n[8] Adrian Galdran, David Pardo, Artzai Pic ´on, and Aitor\\nAlvarez-Gila. Automatic red-channel underwater image\\nrestoration. Journal of Visual Communication and Image\\nRepresentation , 26:132–145, 2015. 2\\n[9] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin\\nTallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch,\\nBernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-\\nlaghi Azar, et al. Bootstrap your own latent-a new approach', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='to self-supervised learning. Advances in neural information\\nprocessing systems , 33:21271–21284, 2020. 3, 5\\n[10] Chunle Guo, Ruiqi Wu, Xin Jin, Linghao Han, Zhi Chai,\\nWeidong Zhang, and Chongyi Li. Underwater ranker: Learn\\nwhich is better and how to be better. In Proceedings of the\\nAAAI Conference on Artificial Intelligence , 2023. 2, 6\\n[11] Junlin Han, Mehrdad Shoeiby, Tim Malthus, Elizabeth\\nBotha, Janet Anstee, Saeed Anwar, Ran Wei, Moham-\\nmad Ali Armin, Hongdong Li, and Lars Petersson. Under-\\nwater image restoration via contrastive learning and a real-\\nworld dataset. Remote Sensing , 14(17):4297, 2022. 1, 3, 6,\\n7\\n[12] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\\nGirshick. Momentum contrast for unsupervised visual rep-\\nresentation learning. In Proceedings of the IEEE/CVF con-\\nference on computer vision and pattern recognition , pages\\n9729–9738, 2020. 3, 5[13] Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon\\nHan, Sangdoo Yun, Gyuwan Kim, Youngjung Uh, and Jung-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Woo Ha. Adamp: Slowing down the slowdown for momen-\\ntum optimizers on scale-invariant weights. In International\\nConference on Learning Representations (ICLR) , 2021. 5\\n[14] Muhammad Suzuri Hitam, Ezmahamrul Afreen Awalludin,\\nWan Nural Jawahir Hj Wan Yussof, and Zainuddin Bachok.\\nMixture contrast limited adaptive histogram equalization for\\nunderwater image enhancement. In 2013 International con-\\nference on computer applications technology (ICCAT) , pages\\n1–5. IEEE, 2013. 2\\n[15] Hanzhe Hu, Fangyun Wei, Han Hu, Qiwei Ye, Jinshi Cui,\\nand Liwei Wang. Semi-supervised semantic segmentation\\nvia adaptive equalization learning. Advances in Neural In-\\nformation Processing Systems , 34:22106–22118, 2021. 3\\n[16] Fushuo Huo, Bingheng Li, and Xuegui Zhu. Efficient\\nwavelet boost learning-based multi-stage progressive re-\\nfinement network for underwater image enhancement. In\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision , pages 1944–1952, 2021. 2, 5, 6, 7', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[17] Md Jahidul Islam, Youya Xia, and Junaed Sattar. Fast un-\\nderwater image enhancement for improved visual percep-\\ntion. IEEE Robotics and Automation Letters , 5(2):3227–\\n3234, 2020. 1, 2, 4, 6, 7\\n[18] Aupendu Kar, Sobhan Kanti Dhara, Debashis Sen, and\\nPrabir Kumar Biswas. Zero-shot single image restoration\\nthrough controlled perturbation of koschmieder’s model. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition , pages 16205–16215, 2021. 2\\n[19] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and\\nFeng Yang. Musiq: Multi-scale image quality transformer.\\nInProceedings of the IEEE/CVF International Conference\\non Computer Vision , pages 5148–5157, 2021. 2, 4, 5, 6\\n[20] Chongyi Li, Saeed Anwar, Junhui Hou, Runmin Cong,\\nChunle Guo, and Wenqi Ren. Underwater image enhance-\\nment via medium transmission-guided multi-color space em-\\nbedding. IEEE Transactions on Image Processing , 30:4985–\\n5000, 2021. 1, 2, 6, 7', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[21] Chongyi Li, Saeed Anwar, and Fatih Porikli. Underwater\\nscene prior inspired deep underwater image and video en-\\nhancement. Pattern Recognition , 98:107038, 2020. 1, 6\\n[22] Chongyi Li, Chunle Guo, Wenqi Ren, Runmin Cong, Junhui\\nHou, Sam Kwong, and Dacheng Tao. An underwater image\\nenhancement benchmark dataset and beyond. IEEE Trans-\\nactions on Image Processing , 29:4376–4389, 2020. 1, 2, 4,\\n6, 7\\n[23] Dong Liang, Ling Li, Mingqiang Wei, Shuo Yang, Liyan\\nZhang, Wenhan Yang, Yun Du, and Huiyu Zhou. Semanti-\\ncally contrastive learning for low-light image enhancement.\\nInProceedings of the AAAI Conference on Artificial Intelli-\\ngence , volume 36, pages 1555–1563, 2022. 3, 5\\n[24] Huan Liu, Zijun Wu, Liangyan Li, Sadaf Salehkalaibar,\\nJun Chen, and Keyan Wang. Towards multi-domain single\\nimage dehazing via test-time training. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 5831–5840, 2022. 1', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[25] Risheng Liu, Xin Fan, Ming Zhu, Minjun Hou, and Zhongx-\\nuan Luo. Real-world underwater enhancement: Chal-\\n18153\\nlenges, benchmarks, and solutions under natural light. IEEE\\nTransactions on Circuits and Systems for Video Technology ,\\n30(12):4861–4875, 2020. 6, 7\\n[26] Yuyuan Liu, Yu Tian, Yuanhong Chen, Fengbei Liu,\\nVasileios Belagiannis, and Gustavo Carneiro. Perturbed and\\nstrict mean teachers for semi-supervised semantic segmenta-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition , pages 4258–4267,\\n2022. 3\\n[27] Ye Liu, Lei Zhu, Shunda Pei, Huazhu Fu, Jing Qin, Qing\\nZhang, Liang Wan, and Wei Feng. From synthetic to real:\\nImage dehazing collaborating with unlabeled real data. In\\nProceedings of the 29th ACM International Conference on\\nMultimedia , pages 50–58, 2021. 3, 6\\n[28] Anish Mittal, Anush Krishna Moorthy, and Alan Con-\\nrad Bovik. No-reference image quality assessment in the\\nspatial domain. IEEE Transactions on image processing ,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='21(12):4695–4708, 2012. 5\\n[29] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Mak-\\ning a “completely blind” image quality analyzer. IEEE Sig-\\nnal processing letters , 20(3):209–212, 2012. 5\\n[30] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and\\nShin Ishii. Virtual adversarial training: a regularization\\nmethod for supervised and semi-supervised learning. IEEE\\ntransactions on pattern analysis and machine intelligence ,\\n41(8):1979–1993, 2018. 2\\n[31] Karen Panetta, Chen Gao, and Sos Agaian. Human-visual-\\nsystem-inspired underwater image quality measures. IEEE\\nJournal of Oceanic Engineering , 41(3):541–551, 2015. 4, 5,\\n6\\n[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-\\nperative style, high-performance deep learning library. Ad-\\nvances in neural information processing systems , 32, 2019.\\n5\\n[33] Yan-Tsung Peng, Keming Cao, and Pamela C Cosman. Gen-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='eralization of the dark channel prior for single image restora-\\ntion. IEEE Transactions on Image Processing , 27(6):2856–\\n2868, 2018. 2, 6, 7\\n[34] Boris T Polyak and Anatoli B Juditsky. Acceleration of\\nstochastic approximation by averaging. SIAM journal on\\ncontrol and optimization , 30(4):838–855, 1992. 3\\n[35] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\\nLi Fei-Fei. ImageNet Large Scale Visual Recognition Chal-\\nlenge. International Journal of Computer Vision (IJCV) ,\\n115(3):211–252, 2015. 2\\n[36] Yoav Y Schechner and Nir Karpel. Clear underwater vision.\\nInProceedings of the 2004 IEEE Computer Society Con-\\nference on Computer Vision and Pattern Recognition, 2004.\\nCVPR 2004. , volume 1, pages I–I. IEEE, 2004. 2\\n[37] Karen Simonyan and Andrew Zisserman. Very deep convo-\\nlutional networks for large-scale image recognition. arXiv\\npreprint arXiv:1409.1556 , 2014. 3, 5', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[38] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao\\nZhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk,Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying\\nsemi-supervised learning with consistency and confidence.\\nAdvances in neural information processing systems , 33:596–\\n608, 2020. 2, 3\\n[39] Hossein Talebi and Peyman Milanfar. Nima: Neural im-\\nage assessment. IEEE transactions on image processing ,\\n27(8):3998–4011, 2018. 5\\n[40] Antti Tarvainen and Harri Valpola. Mean teachers are better\\nrole models: Weight-averaged consistency targets improve\\nsemi-supervised deep learning results. Advances in neural\\ninformation processing systems , 30, 2017. 2, 3\\n[41] Keyan Wang, Yan Hu, Jun Chen, Xianyun Wu, Xi Zhao,\\nand Yunsong Li. Underwater image restoration based on\\na parallel convolutional neural network. Remote sensing ,\\n11(13):1591, 2019. 2\\n[42] Lin Wang and Kuk-Jin Yoon. Semi-supervised student-\\nteacher learning for single image super-resolution. Pattern', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Recognition , 121:108206, 2022. 3\\n[43] Yanbo Wang, Shaohui Lin, Yanyun Qu, Haiyan Wu,\\nZhizhong Zhang, Yuan Xie, and Angela Yao. Towards\\ncompact single image super-resolution via contrastive self-\\ndistillation. In Zhi-Hua Zhou, editor, Proceedings of the\\nThirtieth International Joint Conference on Artificial In-\\ntelligence, IJCAI-21 , pages 1122–1128. International Joint\\nConferences on Artificial Intelligence Organization, 8 2021.\\nMain Track. 5\\n[44] Yuchao Wang, Haochen Wang, Yujun Shen, Jingjing Fei,\\nWei Li, Guoqiang Jin, Liwei Wu, Rui Zhao, and Xinyi\\nLe. Semi-supervised semantic segmentation using unreliable\\npseudo-labels. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pages 4248–\\n4257, 2022. 3\\n[45] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-\\nmoncelli. Image quality assessment: from error visibility to\\nstructural similarity. IEEE transactions on image processing ,\\n13(4):600–612, 2004. 6', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[46] Haiyan Wu, Yanyun Qu, Shaohui Lin, Jian Zhou, Ruizhi\\nQiao, Zhizhong Zhang, Yuan Xie, and Lizhuang Ma. Con-\\ntrastive learning for compact single image dehazing. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pages 10551–10560, 2021. 3, 5\\n[47] Miao Yang and Arcot Sowmya. An underwater color im-\\nage quality evaluation metric. IEEE Transactions on Image\\nProcessing , 24(12):6062–6071, 2015. 4, 5, 6\\n[48] Tian Ye, Sixiang Chen, Yun Liu, Yi Ye, Erkang Chen, and\\nYuche Li. Underwater light field retention: Neural rendering\\nfor underwater imaging. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) Workshops , pages 488–497, June 2022. 1\\n[49] Xiangyu Yin, Xiaohong Liu, and Huan Liu. Fmsnet:\\nUnderwater image restoration by learning from a synthe-\\nsized dataset. In Artificial Neural Networks and Machine\\nLearning–ICANN 2021: 30th International Conference on', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Artificial Neural Networks, Bratislava, Slovakia, Septem-\\nber 14–17, 2021, Proceedings, Part III 30 , pages 421–432.\\nSpringer, 2021. 1\\n[50] Zhenqiang Ying, Haoran Niu, Praful Gupta, Dhruv Maha-\\njan, Deepti Ghadiyaram, and Alan Bovik. From patches to\\n18154\\npictures (paq-2-piq): Mapping the perceptual space of pic-\\nture quality. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pages 3575–\\n3585, 2020. 5\\n[51] Yankun Yu, Huan Liu, Minghan Fu, Jun Chen, Xiyao Wang,\\nand Keyan Wang. A two-branch neural network for non-\\nhomogeneous dehazing via ensemble learning. In Proceed-\\nings of the IEEE/CVF conference on computer vision and\\npattern recognition , pages 193–202, 2021. 1\\n[52] Shu Zhang, Ting Wang, Junyu Dong, and Hui Yu. Under-\\nwater image enhancement via extended multi-scale retinex.\\nNeurocomputing , 245:1–9, 2017. 2\\n[53] Weidong Zhang, Peixian Zhuang, Hai-Han Sun, Guohou\\nLi, Sam Kwong, and Chongyi Li. Underwater image en-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='hancement via minimal color loss and locally adaptive con-\\ntrast enhancement. IEEE Transactions on Image Processing ,\\n31:3997–4010, 2022. 2, 6, 7\\n[54] Huabo Zhu, Xu Han, and Yourui Tao. Semi-supervised ad-\\nvancement of underwater visual quality. Measurement Sci-\\nence and Technology , 32(1):015404, 2020. 3\\n[55] Xiaojin Jerry Zhu. Semi-supervised learning literature sur-\\nvey. 2005. 2\\n18155', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction\\nYuanhui Huang*Wenzhao Zheng*Yunpeng Zhang Jie Zhou Jiwen Lu†\\nBeijing National Research Center for Information Science and Technology, China\\nDepartment of Automation, Tsinghua University, China\\n{huangyh22,zhengwz18 }@mails.tsinghua.edu.cn; yunpengzhang97@gmail.com;\\n{jzhou,lujiwen }@tsinghua.edu.cn\\nOnly RGB Images As Inputs\\nVision-based Occupancy PredictionGround Truth\\nTPVFormer\\nFRONT_LEFTFRONTFRONT_RIGHTSemantic Occupancy Prediction (Ours) LiDAR Seg. (Ours) LiDAR Seg. (Cylinder3D) LiDAR Seg. (Ground Truth) \\nBACK_LEFTBACKBACK_RIGHTbarrierconstruction vehiclemotorcycletraffic conetrailerbicyclebuscarpedestriantruckdriveablesurfaceother flatsidewalkterrainmanmadevegetationCamera As Input:Camera As Input:LiDAR As Input:\\nFigure 1. Given only surround-camera RGB images as inputs, our model (trained using only sparse LiDAR point supervision) can predict', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the semantic occupancy for all volumes in the 3D space. This task is challenging as it requires both geometric and semantic understandings\\nof the 3D scene. We observe that our model can produce even more comprehensive and consistent volume occupancy than the groundtruth\\non the validation set (not seen during training) of nuScenes [4]. Despite the lack of geometric inputs like LiDAR, our model can accurately\\nidentify the 3D positions and sizes of close and distant objects. Particularly, our model even successfully identifies the partially occluded\\nbicycle captured only by two LiDAR points, demonstrating the potential advantage of vision-based 3D semantic occupancy prediction.\\nAbstract\\nModern methods for vision-centric autonomous driving\\nperception widely adopt the bird’s-eye-view (BEV) repre-\\nsentation to describe a 3D scene. Despite its better effi-\\nciency than voxel representation, it has difficulty describing\\nthe fine-grained 3D structure of a scene with a single plane.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='To address this, we propose a tri-perspective view (TPV)\\nrepresentation which accompanies BEV with two additional\\nperpendicular planes. We model each point in the 3D space\\nby summing its projected features on the three planes. To\\nlift image features to the 3D TPV space, we further pro-\\npose a transformer-based TPV encoder (TPVFormer) to ob-\\ntain the TPV features effectively. We employ the attention\\nmechanism to aggregate the image features corresponding\\nto each query in each TPV plane. Experiments show that\\nour model trained with sparse supervision effectively pre-\\ndicts the semantic occupancy for all voxels. We demon-\\nstrate for the first time that using only camera inputs can\\nachieve comparable performance with LiDAR-based meth-\\nods on the LiDAR segmentation task on nuScenes. Code:\\nhttps://github.com/wzzheng/TPVFormer .1. Introduction\\nPerceiving the 3D surroundings accurately and compre-\\nhensively plays an important role in the autonomous driving', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='system. Vision-based 3D perception recently emerges as\\na promising alternative to LiDAR-based one to effectively\\nextract 3D information from 2D images. Though lacking\\ndirect sensing of depth information, vision-based models\\nempowered by surrounding cameras demonstrate promising\\nperformance on various 3D perception tasks such as depth\\nestimation [17,42], semantic map reconstruction [1,19,48],\\nand 3D object detection [27, 30, 46].\\nThe core of 3D surrounding perceiving lies in how to ef-\\nfectively represent a 3D scene. Conventional methods split\\nthe 3D space into voxels and assign each voxel a vector to\\nrepresent its status. Despite its accuracy, the vast number\\nof voxels poses a great challenge to computation and re-\\nquires specialized techniques like sparse convolution [13].\\nAs the information in outdoor scenes is not isotropically\\ndistributed, modern methods collapse the height dimension\\nand mainly focus on the ground plane (bird’s-eye-view)', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='where information varies the most [20,26,28,31,35,46,48].\\n*Equal contribution. †Corresponding author.\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n9223\\nImage  \\nBackbone TPVFormer \\nTraining Phase  \\nTest Phase  LiDAR Ground T ruth \\nSemantic Occupancy Prediction  TPV Representation  Sparse Supervision  \\nDense Prediction  Camera Input  Figure 2. An overview of our method for 3D semantic occupancy\\nprediction. Taking camera images as inputs, the proposed TPV-\\nFormer only uses sparse LiDAR semantic labels for training but\\ncan effectively predict the semantic occupancy for all voxels.\\nThey implicitly encode the 3D information of each object in\\nthe vector representation in each BEV grid. Though more\\nefficient, BEV-based methods perform surprisingly well on', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the 3D object detection task [28, 31]. This is because 3D\\nobject detection only demands predictions of coarse-level\\nbounding boxes for commonly seen objects such as cars\\nand pedestrians. However, objects with various 3D struc-\\ntures can be encountered in real scenes and it is difficult\\n(if not impossible) to encode all of them using a flattened\\nvector. Therefore, it requires a more comprehensive and\\nfine-grained understanding of the 3D surroundings toward\\na safer and more robust vision-centric autonomous driving\\nsystem. Still, it remains unknown how to generalize BEV\\nto model fine-grained 3D structures while preserving its ef-\\nficiency and detection performance.\\nIn this paper, we advance in this direction and propose a\\ntri-perspective view (TPV) representation to describe a 3D\\nscene. Motivated by recent advances in explicit-implicit hy-\\nbrid scene representations [7, 8], we generalize BEV by ac-\\ncompanying it with two perpendicular planes to construct', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='three cross-planes perpendicular to each other. Each plane\\nmodels the 3D surroundings from one view and combining\\nthem provides a comprehensive description of the 3D struc-\\nture. Specifically, to obtain the feature of a point in the 3D\\nspace, we first project it into each of the three planes and use\\nbilinear interpolation to obtain the feature for each projected\\npoint. We then sum the three projected features as the com-\\nprehensive feature of the 3D point. The TPV representation\\nis thus able to describe the 3D scene at an arbitrary resolu-\\ntion and produces different features for different points in\\nthe 3D space. We further propose a transformer-based en-\\ncoder (TPVFormer) to effectively obtain the TPV features\\nfrom 2D images. We first perform image cross-attention\\nbetween TPV grid queries and the corresponding 2D image\\nfeatures to lift 2D information to the 3D space. We then per-\\nform cross-view hybrid-attention among the TPV features\\nto enable interactions among the three planes.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='To demonstrate the superiority of TPV , we formulate a\\npractical yet challenging task for vision-based 3D semanticoccupancy prediction, where only sparse lidar semantic la-\\nbels are provided for training and predictions for all voxels\\nare required for testing, as shown in Figure 2. However,\\nas no benchmark is provided on this challenging setting,\\nwe only perform qualitative analysis but provide a quanti-\\ntative evaluation on two proxy tasks: LiDAR segmentation\\n(sparse training, sparse testing) on nuScenes [4] and 3D se-\\nmantic scene completion (dense training, dense testing) on\\nSemanticKITTI [2]. For both tasks, we only use RGB im-\\nages as inputs. For LiDAR segmentation, our model use\\nthe LiDAR data only for point query to compute evalua-\\ntion metrics. Visualization results show that TPVFormer\\nproduces consistent semantic voxel occupancy prediction\\nwith only sparse point supervision during training, as shown\\nin Figure 1. We also demonstrate for the first time that', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='our vision-based method achieves comparable performance\\nwith LiDAR-based methods on LiDAR segmentation.\\n2. Related Work\\nVoxel-based Scene Representation: Obtaining an ef-\\nfective representation for a 3D scene is the basic procedure\\nfor 3D surrounding perception. One direct way is to dis-\\ncretize the 3D space into voxels and assign a vector to repre-\\nsent each voxel [49,51]. The ability to describe fine-grained\\n3D structures makes voxel-based representation favorable\\nfor 3D semantic occupancy prediction tasks including lidar\\nsegmentation [12, 29, 40, 44, 45, 51] and 3D scene comple-\\ntion [5, 10, 23, 38, 43]. Though they have dominated the\\n3D segmentation task [44], they still lag behind BEV-based\\nmethods on the 3D detection performance [26]. Despite the\\nsuccess of voxel-based representations in LiDAR-centric\\nsurrounding perception, only a few works have explored\\nvoxel-based representations for vision-centric autonomous\\ndriving [5,25]. MonoScene [5] first backprojects image fea-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tures to all possible positions in the 3D space along the opti-\\ncal ray to obtain the initial voxel representation and further\\nprocesses it using a 3D UNet. However, it is still challeng-\\ning to generalize it to 3D perception with multi-view images\\ndue to the inefficiency of voxel representations. This moti-\\nvates us to explore more efficient and expressive ways to\\ndescribe the fine-grained 3D structure of a scene.\\nBEV-based Scene Representation: The vast number\\nof voxels poses a great challenge to the computation effi-\\nciency of voxel-based methods. Considering that the height\\ndimension contains less information than the other two di-\\nmensions, BEV-based methods implicitly encode the height\\ninformation in each BEV grid for a more compact repre-\\nsentation of scenes [22]. Recent studies in BEV-based per-\\nception focus on how to effectively transform features from\\nthe image space to the BEV space [20, 26, 27, 35, 36, 48].\\nOne line of works explicitly predict a depth map for each', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='image and utilizes it to project image features into the 3D\\nspace followed by BEV pooling [20, 26, 28, 31, 35, 36, 48].\\n9224\\nAnother line of works employ BEV queries to implic-\\nitly assimilate information from image features using the\\ncross-attention mechanism [21, 27]. BEV-based perception\\nachieves great success on vision-centric 3D detection from\\nmulti-view images [26], demonstrating comparable perfor-\\nmance to LiDAR-centric methods. Yet, it is difficult to ap-\\nply BEV to 3D semantic occupancy prediction which re-\\nquires a more fine-grained description of the 3D space.\\nImplicit Scene Representation: Recent methods have\\nalso explored implicit representations to describe a scene.\\nThey learn a continuous function that takes as input the\\n3D coordinate of a point and outputs the representation\\nof this point [32–34]. Compared with explicit represen-\\ntations like voxel and BEV , implicit representations usu-\\nally share the advantage of arbitrary-resolution modeling', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and computation-efficient architectures [6, 11, 37]. These\\nadvantages enable them to scale to larger and more com-\\nplex scenes with more fine-grained descriptions. Especially,\\nour work is inspired by recent advances in hybrid explicit-\\nimplicit representations [7, 8]. They explicitly inject spatial\\ninformation into the continuous mapping of implicit repre-\\nsentations. Therefore, they share the computation-efficient\\narchitecture of implicit representations and better spatial\\nawareness of explicit representations. Still, they mainly fo-\\ncus on small-scale complex scenes for 3D-aware image ren-\\ndering. To the best of our knowledge, we are the first to use\\nimplicit representation to model outdoor scenes for 3D sur-\\nrounding perception in autonomous driving.\\n3. Proposed Approach\\n3.1. Generalizing BEV to TPV\\nAutonomous driving perception requires both expressive\\nand efficient representation of the complex 3D scene. V oxel\\nrepresentation [25, 40, 45] describes a 3D scene with dense', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='cubic features V∈RH×W×D×Cwhere H,W,Dare the\\nspatial resolution of the voxel space and Cdenotes the fea-\\nture dimension. A random point located at (x, y, z )in the\\nreal world maps to its voxel coordinates (h, w, d )through\\none-to-one correspondence. Therefore, voxel representa-\\ntion preserves the dimensionality of the real world and\\noffers sufficient expressiveness with appropriate H, W, D .\\nHowever, the storage and computation complexity of voxel\\nfeatures comes proportion to O(HWD ), making it chal-\\nlenging to deploy them in real-time onboard applications.\\nAs a popular alternative, BEV [21,26,27,31] representa-\\ntion uses a 2D feature map B∈RH×W×Cto encode the top\\nview of a scene. Different from the voxel counterpart, the\\npoint at (x, y, z )is projected to its BEV coordinates (h, w)\\nusing only the positional information from the ground plane\\nregardless of the z-axis. Each feature sampled from Bcor-\\nresponds to a pillar region covering the full range of z-axis', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='in the real world. Although BEV greatly reduces the storage\\nVoxelBEVTPV (ours)\\nFigure 3. Comparisons of the proposed TPV representation with\\nvoxel and BEV representation. While BEV is more efficient than\\nthe voxel representation, it discards the height information and\\ncannot comprehensively describe a 3D scene.\\nand computation burden to O(HW), completely omitting\\nthez-axis has an adverse effect on its expressiveness.\\nTo address this, we propose a Tri-Perspective View\\n(TPV) representation which is capable of modeling the 3D\\nspace at full scale without suppressing any axes and avoid-\\ning cubic complexity, as illustrated in Figure 3. Formally,\\nwe learn three axis-aligned orthogonal TPV planes:\\nT= [THW,TDH,TWD],THW∈RH×W×C,\\nTDH∈RD×H×C,TWD∈RW×D×C,(1)\\nwhich represent the top, side and front views of a 3D scene\\nrespectively. H, W, D denote the resolution of the three\\nplanes and Cis the feature dimension. Intuitively, a com-\\nplex scene, when examined from different perspectives, can', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='be better understood because these perspectives may pro-\\nvide complementary clues about the scene.\\nPoint Querying Formulation. Given a query point at\\n(x, y, z )in the real world, TPV representation aggregates\\nits projections on the top, side and front views in order to\\nget a comprehensive description of the point. To elabo-\\nrate, we first project the point onto the TPV planes to ob-\\ntain the coordinates [(h, w),(d, h),(w, d)], sample the TPV\\nplanes at these locations to retrieve the corresponding fea-\\ntures [th,w,td,h,tw,d], and aggregate them to generate the\\nfinalfx,y,z:\\nti,j=S(T,(i, j)) =S(T,P(x, y)), (2)\\nfx,y,z=A(th,w,td,h,tw,d), (3)\\nwhere the sampling function Sand the aggregation func-\\ntionAare implemented with bilinear interpolation and sum-\\nmation respectively, and each projection function Psimply\\nperforms scaling on the two relevant coordinates since TPV\\nplanes are aligned with the real-world axes.\\nVoxel Feature Formulation. The TPV planes, when re-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='peated along respective orthogonal directions and summed\\nup, construct a full-scale 3D feature space similar to the\\nvoxel feature space, but with storage and computation com-\\nplexity of only O(HW +DH+WD), which is an order\\nof magnitude lower than the voxel counterpart.\\n9225\\nCompared with BEV , as the three planes in TPV are per-\\npendicular to each other, point features along the orthogonal\\ndirection of one plane are diversified by features sampled\\nfrom the other two planes. Moreover, a grid feature in each\\nTPV plane is only responsible for view-specific information\\nof the corresponding pillar region rather than encoding the\\ncomplete information as in BEV . To sum up, TPV represen-\\ntation generalizes BEV from single top view to complemen-\\ntary and orthogonal top, side and front views and is able to\\noffer a more comprehensive and fine-grained understanding\\nof the 3D surroundings while remaining efficient.\\n3.2. TPVFormer\\nFor vision-centric autonomous driving perception, a 2D', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='backbone is often employed to obtain image features before\\nfeeding them into a 3D encoder. We present a transformer-\\nbased TPV encoder (TPVFormer) to lift image features to\\nTPV planes through the attention mechanism.\\nOverall Structure: In TPVFormer, we introduce TPV\\nqueries, image cross-attention (ICA) and cross-view hybrid-\\nattention (CVHA) to enable effective generation of TPV\\nplanes, as shown in Fig. 4. In fact, TPV queries and\\nTPV planes refer to the same set of feature vectors de-\\nfined in (1). Each TPV query t∈Tis a grid cell fea-\\nture belonging to one of the three planes and used to en-\\ncode view-specific information from the corresponding pil-\\nlar region. Cross-view hybrid-attention enables direct in-\\nteractions among TPV queries from the same or different\\ntpv planes in order to gather contextual information. Inside\\nimage cross-attention, TPV queries aggregate visual infor-\\nmation from image features through deformable attention.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='We further construct two kinds of transformer blocks:\\nhybrid-cross-attention block (HCAB) and hybrid-attention\\nblock (HAB). Composed of both CVHA and ICA atten-\\ntion, the HCAB block is employed in the first half of TPV-\\nFormer to effectively query visual information from image\\nfeatures. Following HCAB blocks, the HAB block contains\\nonly CVHA attention and specializes in contextual infor-\\nmation encoding. Finally, we build TPVFormer by stacking\\nN1HCAB blocks and N2HAB blocks.\\nTPV Queries: Although referring to the same list of 2D\\nfeatures defined in (1), TPV queries and TPV planes are\\nused in attention and 3D representation contexts, respec-\\ntively. Each TPV query maps to a 2D grid cell region of\\nsizes×s m2in the corresponding view, and further to a 3D\\npillar region extending from the view in the perpendicular\\ndirection. In our pipeline, TPV queries are first enhanced\\nwith raw visual information from image features in HCAB\\nblocks, and then refined with contextual clues from other', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='queries in HAB blocks. As for implementation, we initial-\\nize TPV queries as learnable parameters.\\nImage Cross-Attention: In TPVFormer, we use image\\ncross-attention to lift multi-scale and possibly multi-cameraimage features to the TPV planes. Considering the high\\nresolution nature of TPV queries ( ∼104queries) and mul-\\ntiple image feature maps ( ∼105pixels each), it is unfea-\\nsible to compute full-scale vanilla cross-attention between\\nthem. And thus we employ the efficient deformable atten-\\ntion [14, 27, 50] to implement image cross-attention.\\nWe take the local receptive field as an inductive bias\\nwhen sampling the reference points. Specifically, for a TPV\\nquery th,wlocated at (h, w)in the top plane, we first calcu-\\nlate its coordinates (x, y)in the top view in the real world\\nthrough the inverse projection function P−1\\nHW. Then we\\nsample uniformly Nref\\nHWreference points for the query th,w\\nalong the orthogonal direction of the plane:\\n(x, y) =P−1\\nHW(h, w) = (( h−H\\n2)×s,(w−W\\n2)×s).(4)', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='2)×s,(w−W\\n2)×s).(4)\\nRefw\\nh,w={(x, y, z i)}Nref\\nHW\\ni=1, (5)\\nwhere Refw\\nh,wdenotes the set of reference points in the\\nworld coordinate for query th,w. The similar procedure is\\nrepeated for all TPV queries, and note that the number of\\nreference points Nrefmay change across planes because of\\ndifferent ranges of axes. After deriving the reference points\\nforth,w, we need to project them into the pixel coordinate\\nin order to sample the image feature maps:\\nRefp\\nh,w=Ppix(Refw\\nh,w) =Ppix({(x, y, z i)}), (6)\\nwhere Refp\\nh,wis the set of reference points in the pixel\\ncoordinate for query th,wandPpixis the perspective pro-\\njection function determined by the camera extrinsic and in-\\ntrinsic. Note that we may have multiple cameras in differ-\\nent directions which will generate a set of {Refp,j\\nh,w}Nc\\nj=1\\nwhere Ncdenotes the number of cameras. Since not all\\ncameras can capture the reference points of query th,w, we\\ncan further reduce computation by removing invalid sets\\nfrom{Refp,j\\nh,w}Nc', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='from{Refp,j\\nh,w}Nc\\nj=1if none of the reference points falls onto\\nthe image captured by the corresponding camera. The fi-\\nnal step is to generate offsets and attention weights through\\ntwo linear layers applied on th,wand produce the updated\\nTPV queries by summing up the sampled image features\\nweighted by their attention weights:\\nICA(th,w,I)=1\\n|Nval\\nh,w |X\\nj∈Nval\\nh,wDA(th,w,Refp,j\\nh,w,Ij), (7)\\nwhere Nval\\nh,w,Ij,DA(·)denote the index set of valid cam-\\neras, the image features from the jth camera and the de-\\nformable attention function, respectively.\\nCross-View Hybrid-Attention: In image cross-\\nattention, TPV queries sample reference image features sep-\\narately and no direct interactions between them are allowed.\\nHere, we further propose cross-view hybrid-attention to en-\\nable queries to exchange their information across different\\n9226\\nPredictionHead', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='9226\\nPredictionHead\\n<latexit sha1_base64=\"14SSSImuYP0ue83rF2rNZny6qjk=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyhNnJbDJmdmaZ6RVCyD948aCIV//Hm3/jJNmDJhY0FFXddHdFqRQWff/bW1ldW9/YLGwVt3d29/ZLB4cNqzPDeJ1pqU0ropZLoXgdBUreSg2nSSR5MxreTv3mEzdWaPWAo5SHCe0rEQtG0UmNjk5lZrulsl/xZyDLJMhJGXLUuqWvTk+zLOEKmaTWtgM/xXBMDQom+aTYySxPKRvSPm87qmjCbTieXTshp07pkVgbVwrJTP09MaaJtaMkcp0JxYFd9Kbif147w/g6HAuVZsgVmy+KM0lQk+nrpCcMZyhHjlBmhLuVsAE1lKELqOhCCBZfXiaN80pwWQnuL8rVmzyOAhzDCZxBAFdQhTuoQR0YPMIzvMKbp70X7937mLeuePnMEfyB9/kD0faPSw==</latexit>\\x00SemanticOccupancy          of    Cross-AttentionCross-AttentionCross-AttentionHybrid-AttentionHybrid-AttentionImage InputsNetworkNetworkFeature MapsTPVFormerTPV RepresentationPoint PredictionFigure 4. Framework of the proposed TPVFormer for 3D semantic occupancy prediction. We employ an image backbone network to\\nextract multi-scale features for multi-camera images. We then perform cross-attention to adaptively lift 2D features to the TPV space and', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='use cross-view hybrid attention to enable the interactions between TPV planes. To predict the semantic occupancy of a point in the 3D\\nspace, we apply a lightweight prediction head on the sum of projected features on the three TPV planes.\\nviews, which benefits context extraction. We also adopt de-\\nformable attention to reduce computation, in which three\\nTPV planes serve as key and value. Taking the TPV query\\nth,wlocated at (h, w)in the top plane as an example, we\\ngroup its reference points into three disjoint subsets, which\\ncontains reference points belonging to the top, side and\\nfront planes respectively:\\nRh,w=Rtop\\nh,w∪Rside\\nh,w∪Rfront\\nh,w. (8)\\nTo collect reference points on the top plane, we simply sam-\\nple a few random points in the neighborhood of the query\\nth,w. As for the side and front planes, we first sample 3D\\npoints uniformly along the direction perpendicular to the\\ntop plane and project them onto the side and front planes:\\nRside\\nh,w={(di, h)}i,Rfront\\nh,w={(w, di)}i. (9)', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='h,w={(w, di)}i. (9)\\nFollowing the derivation of reference points is the typical\\npractice of deformable attention:\\nCVHA( th,w) = DA( th,w,Rh,w,T). (10)\\n3.3. Applications of TPV\\nThe TPV planes Tfrom TPVFormer encode fine-\\ngrained view-specific information of a 3D scene. Still, they\\nare in the form of orthogonal cross-planes and not readily\\ninterpretable to common task heads. Here we explain how\\nto convert TPV planes to point and voxel features and intro-\\nduce a lightweight segmentation head.\\nPoint Feature. Given locations in the real world, we\\nconsider the feature generation process as the points query-\\ning their features from the TPV representation. As defined\\nin (2) and (3), we first project the points onto the TPV planes\\nto retrieve the corresponding features [th,w,td,h,tw,d], and\\nsum them up to obtain the per-point features.\\nVoxel Feature. For dense voxel features, we actively\\nbroadcast each TPV plane along the corresponding orthog-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='onal direction to produce three feature tensors of the same\\nsizeH×W×D×C, and aggregate them by summation\\nto obtain the full-scale voxel features. Note that we do not\\nknow the position of any physical point in advance.To conduct fine-grained segmentation tasks, we apply a\\nlightweight MLP on the point or voxel features to predict\\ntheir semantic labels, instantiated by only two linear layers\\nand an intermediate activation layer.\\n4. Experiments\\n4.1. Task Descriptions\\nWe conduct three types of experiments, including 3D\\nsemantic occupancy prediction, LiDAR segmentation, and\\nsemantic scene completion (SSC). The first two tasks are\\nperformed on Panoptic nuScenes [16], and the last one is\\non Semantic KITTI [2]. For all tasks, our model only uses\\nRGB images as inputs.\\n3D semantic occupancy prediction. As dense seman-\\ntic labels are difficult to obtain, we formulate a practical yet\\nchallenging task for vision-based 3D semantic occupancy\\nprediction. Under this task, the model is only trained us-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ing sparse semantic labels (LiDAR points) but is required\\nto produce a semantic occupancy for all the voxels in the\\nconcerned 3D space during testing. As no benchmark is\\nprovided for this, we only perform a qualitative analysis of\\nour method. Still, our method is the first to demonstrate\\neffective results on this challenging task.\\nLiDAR segmentation. The LiDAR segmentation task\\ncorresponds to the point querying formulation discussed in\\nSection 3.3, where we predict the semantic label of a given\\npoint. The LiDAR segmentation task does not necessarily\\nuse point clouds as input. In our case, we use only RGB\\nimages as input, while the points are merely used to query\\ntheir features and for supervision in the training phase.\\nSemantic Scene Completion. In conventional SSC,\\ngiven a single initial LiDAR scan, one needs to predict\\nwhether each voxel is occupied and its semantic label for the\\ncomplete scene inside a certain volume. As a vision-centric', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='adaptation, we use as input only RGB images and predict\\nthe occupancy and semantic label of each voxel. Accord-\\ningly, we supervise the training process with voxel labels.\\nIn the case of TPV representation, we adopt the voxel fea-\\nture formulation in Section 3.3 to generate full-scale voxel\\nfeatures. Following common practices, we report the inter-\\n9227\\nFRONT_LEFTFRONTFRONT_RIGHTSemantic Occupancy Prediction (Ours) LiDAR Seg. (Ours) LiDAR Seg. (Cylinder3D) LiDAR Seg. (Ground Truth) \\nBACK_LEFTBACKBACK_RIGHTbarrierconstruction vehiclemotorcycletraffic conetrailerbicyclebuscarpedestriantruckdriveablesurfaceother flatsidewalkterrainmanmadevegetationCamera As Input:Camera As Input:LiDAR As Input:Figure 5. Visualization results on 3D semantic occupancy prediction and nuScenes LiDAR segmentation. Our method can generate\\nmore comprehensive prediction results than the LiDAR segmentation ground truth.\\n50×50×4\\n90×90×7\\n100×100×8\\n400×400×32\\n200×200×16\\n75×75×6\\nFRONT_LEFTFRONTFRONT_RIGHT', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='BACK_RIGHTBACK_LEFTBACKCameraInputsSemanticOccupancyPredictions\\nFigure 6. Arbitrary resolution at test time. We can adjust the prediction resolution through interpolation at test time.\\nsection over union (IoU) of occupied voxels, ignoring their\\nsemantic class, for the scene completion (SC) task and the\\nmIoU of all semantic classes for the SSC task.\\n4.2. Implementation Details\\n3D semantic occupancy prediction and LiDAR seg-\\nmentation. We construct two versions of TPVFormer,\\nnamely TPVFormer-Base and TPVFormer-Small, for dif-\\nferent trade-offs between performance and efficiency.\\nTPVFormer-Base uses the ResNet101-DCN [14,18] initial-\\nized from FCOS3D [41] checkpoint, while TPVFormer-\\nSmall adopts the ResNet-50 [18] pretrained on Ima-\\ngeNet [15]. Following Cylinder3D [51], we employ both\\ncross entropy loss and lovasz-softmax [3] loss to optimize\\nour network. For lovasz-softmax loss, we use features of\\nreal points from LiDAR scans as input to maximize the IoU', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='score for classes, while voxel features are used in cross en-tropy loss to improve point classification accuracy and avoid\\nsemantic ambiguity. For 3D semantic occupancy predic-\\ntion, we generate pseudo-per-voxel labels from sparse point\\ncloud by assigning a new label of empty to any voxel that\\ndoes not contain any point, and we use voxel predictions as\\ninput to both lovasz-softmax and cross-entropy losses.\\nSemantic Scene Completion. We follow the setting of\\nMonoScene [5] in the SSC task for a fair comparison. For\\nmodel architecture, we adopt the 2D UNet based on a pre-\\ntrained EfficientNetB7 [39] as image backbone to generate\\nmulti-scale image features. For optimization, we employ\\nthe losses in MonoScene except for the relation loss.\\n4.3. 3D Semantic Occupancy Prediction Results\\nMain results. In Figure 5, we provide the main visual-\\nization results. Our result is much denser and more realistic\\nthan the LiDAR segmentation ground truth, which validates\\n9228', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='9228\\nTable 1. LiDAR segmentation results on nuScenes test set. Despite critical modal difference, our TPVFormer-Base achieves comparable\\nperformance with LiDAR-based methods.\\nMethodInput\\nModalitymIoU\\n■barrier\\n■bicycle\\n■bus\\n■car\\n■const. veh.\\n■motorcycle\\n■pedestrian\\n■traffic cone\\n■trailer\\n■truck\\n■drive. suf.\\n■other flat\\n■sidewalk\\n■terrain\\n■manmade\\n■vegetation\\nMINet [24] LiDAR 56.3 54.6 8.2 62.1 76.6 23.0 58.7 37.6 34.9 61.5 46.9 93.3 56.4 63.8 64.8 79.3 78.3\\nPolarNet [47] LiDAR 69.4 72.2 16.8 77.0 86.5 51.1 69.7 64.8 54.1 69.7 63.5 96.6 67.1 77.7 72.1 87.1 84.5\\nPolarSteam [9] LiDAR 73.4 71.4 27.8 78.1 82.0 61.3 77.8 75.1 72.4 79.6 63.7 96.0 66.5 76.9 73.0 88.5 84.8\\nJS3C-Net [43] LiDAR 73.6 80.1 26.2 87.8 84.5 55.2 72.6 71.3 66.3 76.8 71.2 96.8 64.5 76.9 74.1 87.5 86.1\\nAMVNet [29] LiDAR 77.3 80.6 32.0 81.7 88.9 67.1 84.3 76.1 73.5 84.9 67.3 97.5 67.4 79.4 75.5 91.5 88.7\\nSPVNAS [40] LiDAR 77.4 80.0 30.0 91.9 90.8 64.7 79.0 75.6 70.9 81.0 74.6 97.4 69.2 80.0 76.1 89.3 87.1', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Cylinder3D++ [51] LiDAR 77.9 82.8 33.9 84.3 89.4 69.6 79.4 77.3 73.4 84.6 69.4 97.7 70.2 80.3 75.5 90.4 87.6\\nAF2S3Net [12] LiDAR 78.3 78.9 52.2 89.9 84.2 77.4 74.3 77.3 72.0 83.9 73.8 97.1 66.5 77.5 74.0 87.7 86.8\\nDRINet++ [45] LiDAR 80.4 85.5 43.2 90.5 92.1 64.7 86.0 83.0 73.3 83.9 75.8 97.0 71.0 81.0 77.7 91.6 90.2\\nLidarMultiNet [44] LiDAR 81.4 80.4 48.4 94.3 90.0 71.5 87.2 85.2 80.4 86.9 74.8 97.8 67.3 80.7 76.5 92.1 89.6\\nTPVFormer-Small (ours) Camera 59.2 65.6 15.7 75.1 80.0 45.8 43.1 44.3 26.8 72.8 55.9 92.3 53.7 61.0 59.2 79.7 75.6\\nTPVFormer-Base (ours) Camera 69.4 74.0 27.5 86.3 85.5 60.7 68.0 62.1 49.1 81.9 68.4 94.1 59.5 66.5 63.5 83.8 79.9\\nTable 2. Semantic scene completion results on SemanticKITTI test set. For fair comparison, we use the performances of RGB-inferred\\nversions of the first four methods reported in MonoScene [5]. We significantly outperform other methods in both IoU and mIoU, including\\nMonoScene which is based on 3D convolution.\\nMethodInput\\nModalitySC\\nIoUSSC', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ModalitySC\\nIoUSSC\\nmIoU\\nroad\\n(15.30%)\\nsidewalk\\n(11.13%)\\nparking\\n(1.12%)\\nother-grnd\\n(0.56%)\\nbuilding\\n(14.1%)\\ncar\\n(3.92%)\\ntruck\\n(0.16%)\\nbicycle\\n(0.03%)\\nmotorcycle\\n(0.03%)\\nother-veh.\\n(0.20%)\\nvegetation\\n(39.3%)\\ntrunk\\n(0.51%)\\nterrain\\n(9.17%)\\nperson\\n(0.07%)\\nbicyclist\\n(0.07%)\\nmotorcyclist.\\n(0.05%)\\nfence\\n(3.90%)\\npole\\n(0.29%)\\ntraf.-sign\\n(0.08%)\\nLMSCNet [38] Camera 31.38 7.07 46.70 19.50 13.50 3.10 10.30 14.30 0.30 0.00 0.00 0.00 10.80 0.00 10.40 0.00 0.00 0.00 5.40 0.00 0.00\\n3DSketch [10] Camera 26.85 6.23 37.70 19.80 0.00 0.00 12.10 17.10 0.00 0.00 0.00 0.00 12.10 0.00 16.10 0.00 0.00 0.00 3.40 0.00 0.00\\nAICNet [23] Camera 23.93 7.09 39.30 18.30 19.80 1.60 9.60 15.30 0.70 0.00 0.00 0.00 9.60 1.90 13.50 0.00 0.00 0.00 5.00 0.10 0.00\\nJS3C-Net [43] Camera 34.00 8.97 47.30 21.70 19.90 2.80 12.70 20.10 0.80 0.00 0.00 4.10 14.20 3.10 12.40 0.00 0.20 0.20 8.70 1.90 0.30', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='MonoScene [5] Camera 34.16 11.08 54.70 27.10 24.80 5.70 14.40 18.80 3.30 0.50 0.70 4.40 14.90 2.40 19.50 1.00 1.40 0.40 11.10 3.30 2.10\\nTPVFormer (ours) Camera 34.25 11.26 55.10 27.20 27.40 6.50 14.80 19.20 3.70 1.00 0.50 2.30 13.90 2.60 20.40 1.10 2.40 0.30 11.00 2.90 1.50\\nthe effectiveness of TPV representation in modeling the 3D\\nscene and semantic occupancy prediction.\\nArbitrary resolution at test time. Given the simplic-\\nity of our segmentation head, we can adjust the resolution\\nof TPV planes at test time arbitrarily without retraining the\\nnetwork. Figure 6 shows the results for resolution adjust-\\nment, in which we gradually increase the resolution of TPV\\nplanes from an initial 50x50x4 to 8 times larger. It is evi-\\ndent that as resolution increases, TPV representation is able\\nto capture more details about the 3D objects, such as shape.\\n4.4. LiDAR Segmentation Results\\nAs the first vision-based method for LiDAR segmenta-\\ntion task, we benchmark TPVFormer against LiDAR-based', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='methods. As shown in Table 1, TPVFormer achieves com-\\nparable mIoU ( ∼70%) with most LiDAR-based methods.\\nThis is nontrivial since our method needs to reconstruct the\\ncomplete 3D scene at a high resolution from only 2D image\\ninput, while the 3D structural information is readily avail-\\nable in the point clouds for LiDAR-based methods. Still, vi-\\nsual input can capture more semantic information than point\\nclouds. For example, in Figure 5, Cylinder3D fails to pre-\\ndict one of the two trucks on the rightmost side of the firstscene, while TPVFormer predicts correctly.\\n4.5. Semantic Scene Completion Results\\nIn Table 2, we report the results of the semantic scene\\ncompletion task on SemanticKITTI test set. We com-\\npare our TPVFormer against MonoScene [5], which is a\\nvision-based method based on 3D convolution in the voxel\\nspace. We also include the 4 baseline methods provided in\\nMonoScene [5]. TPVFormer outperforms all other meth-\\nods in both IoU and mIoU, demonstrating its effectiveness', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='in occupancy and semantics prediction. TPVFormer also\\nenjoys significant advantages over MonoScene in both pa-\\nrameter number and computation. Specifically, TPVFormer\\nhas only 6.0M parameters versus 15.7M for MonoScene,\\nand 128G FLOPS per image versus 500G for MonoScene.\\n4.6. Analysis\\nWe analyze TPVFormer on the validation sets of\\nnuScenes and SemanticKITTI for LiDAR segmentation and\\nsemantic scene completion, respectively.\\nLoss functions for LiDAR segmentation. We employ\\nboth cross entropy (CE.) loss and lovasz-softmax loss [3]\\nfor LiDAR segmentation. As TPVFormer can produce\\n9229\\nTable 3. Different prediction types as input to loss functions\\nfor LiDAR segmentation. V oxel and point in the loss column\\nrepresent voxel and point predictions. We report mIoUs calculated\\nwith both voxel and point predictions.\\nLoss mIoU\\nCE. Lovasz V oxel Point\\nV oxel V oxel 63.17 50.66\\nV oxel Point 63.37 64.80\\nPoint V oxel 64.07 64.46\\nPoint Point 49.94 64.02', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Table 4. Comparisons between BEV, voxel, TPV representa-\\ntions for LiDAR segmentation. Latency at inference is measured\\nwith a batch size of 1 using a RTX 3090 GPU.\\nRepr. Feature Resolution#Param. FLOPs Latency mIoU\\n(M) (G) (s) (%)\\nBEV 256 200x200 61.2 1351 0.323 54.5\\nV oxel 64 100x100x8 49.8 915 0.314 24.2\\nTPV 64 200x200x16 48.8 934 0.312 67.4\\nTable 5. Ablations on TPV resolutions and feature dimensions.\\nTPV Resolution Feature Point mIoU (%)\\n100x100x8 256 64.15\\n200x200x16 64 67.40\\npoint and voxel predictions in a single forward propaga-\\ntion, we investigate different prediction types as input to\\nthese loss functions. As shown in Table 3, when both voxel\\nand point predictions are used as input to the loss func-\\ntions, the mIoUs from both predictions are high and close to\\neach other. However, when only voxel or point prediction\\nis employed in optimization, the corresponding mIoU will\\nbe much higher than the other one. This indicates the im-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='portance of discrete and continuous supervision signals for\\nlearning a robust representation.\\nTPV resolution and feature dimension. We ablate res-\\nolution of TPV and feature dimension for LiDAR segmen-\\ntation in Table 5. TPVFormer favors resolution over fea-\\nture dimension because increasing the resolution directly\\nenhances its ability to model more fine-grained structures.\\nComparisons between BEV, voxel and TPV . We adopt\\nvoxel, BEV and TPV as the representation for 3D space and\\nuse a similar architecture to the proposed TPVFormer to lift\\nimage features to 3D. We show the comparisons in Table 4.\\nWe increase the feature dimension for BEV considering the\\ncapacity of the single top view, which accounts for the extra\\nparameters and FLOPs. Since voxel representation takes up\\nmuch more GPU memory, we have to reduce its grid reso-\\nlution to 100x100x8. We observe that TPVFormer achieves\\nthe highest mIoU with fewer parameters and FLOPs com-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='pared with BEV and voxel counterparts. For fair compar-\\nisons, we keep the model architectures the same, which may\\nresult in the inferior performance of voxel representation\\nand has room for improvement. For latency, sequential pro-\\ncessing of three TPV planes as in our code may account forTable 6. Different number of HCAB blocks and HAB blocks\\nfor semantic scene completion. We keep the total number of at-\\ntention modules the same in these experiments.\\n# HCAB # HAB SC IoU SSC mIoU\\n2 4 35.55 10.49\\n3 2 35.61 11.36\\n4 0 35.79 10.82\\nFigure 7. Failure cases of pedestrians for semantic occupancy pre-\\ndiction. TPVFormer has trouble distinguishing pedestrians close\\nto each other and could predict a strip area for a distant pedestrian.\\nthe comparable latency of TPV and voxel.\\nNumber of HCAB and HAB blocks. We study the\\nnumber of HCAB and HAB blocks in Table 6. The IoU im-\\nproves as HCAB blocks increase, validating the importance\\nof direct visual clues for geometry understanding. However,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='semantic prediction relies on both visual and contextual in-\\nformation as the highest mIoU is achieved with a moderate\\nnumber of HCAB and HAB blocks.\\nLimitations. As shown in Figure 5, the main strength\\nof the camera-based method over LiDAR-based methods\\nis the ability to output dense predictions for 3D space.\\nOn the other hand, current TPVFormer still underperforms\\nLiDAR-based methods for lidar segmentation. In Figure 7,\\nwe identify some failure cases on semantic occupancy pre-\\ndiction. TPVFormer fails to seperate pedestrians close to-\\ngether and could predict a distant pedestrian as a strip area.\\n5. Conclusion\\nIn this paper, we have presented a tri-perspective view\\n(TPV) representation, which is able to describe the fine-\\ngrained structures of a 3D scene efficiently. To lift im-\\nage features to the 3D TPV space, we have proposed\\na TPVFormer model based on the attention mechanism.\\nThe visualization results have shown that our TPVFormer', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='produces consistent semantic voxel occupancy prediction\\nwith only sparse point supervision during training. We\\nhave demonstrated for the first time that our vision-based\\nmethod achieves comparable performance with LiDAR-\\nbased methods on nuScenes LiDAR segmentation task.\\nAcknowledgement\\nThis work was supported in part by the National Key\\nResearch and Development Program of China under Grant\\n2017YFA0700802, in part by the National Natural Science\\nFoundation of China under Grant 62125603, and in part by\\na grant from the Beijing Academy of Artificial Intelligence\\n(BAAI).\\n9230\\nReferences\\n[1] Adil Kaan Akan and Fatma G ¨uney. Stretchbev: Stretching\\nfuture instance prediction spatially and temporally. arXiv\\npreprint arXiv:2203.13641 , 2022. 1\\n[2] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-\\nzel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Se-\\nmantickitti: A dataset for semantic scene understanding of\\nlidar sequences. In ICCV , pages 9297–9307, 2019. 2, 5', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[3] Maxim Berman, Amal Rannen Triki, and Matthew B\\nBlaschko. The lov ´asz-softmax loss: A tractable surrogate\\nfor the optimization of the intersection-over-union measure\\nin neural networks. In CVPR , pages 4413–4421, 2018. 6, 7\\n[4] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,\\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\\nancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-\\nmodal dataset for autonomous driving. In CVPR , 2020. 1,\\n2\\n[5] Anh-Quan Cao and Raoul de Charette. Monoscene: Monoc-\\nular 3d semantic scene completion. In CVPR , pages 3991–\\n4001, 2022. 2, 6, 7\\n[6] Rohan Chabra, Jan E Lenssen, Eddy Ilg, Tanner Schmidt,\\nJulian Straub, Steven Lovegrove, and Richard Newcombe.\\nDeep local shapes: Learning local sdf priors for detailed 3d\\nreconstruction. In ECCV , pages 608–625, 2020. 3\\n[7] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Effi-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='cient geometry-aware 3d generative adversarial networks. In\\nCVPR , pages 16123–16133, 2022. 2, 3\\n[8] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\\nHao Su. Tensorf: Tensorial radiance fields. arXiv preprint\\narXiv:2203.09517 , 2022. 2, 3\\n[9] Qi Chen, Sourabh V ora, and Oscar Beijbom. Polarstream:\\nStreaming object detection and segmentation with polar pil-\\nlars. NeurIPS , 34:26871–26883, 2021. 7\\n[10] Xiaokang Chen, Kwan-Yee Lin, Chen Qian, Gang Zeng, and\\nHongsheng Li. 3d sketch-aware semantic scene completion\\nvia semi-supervised structure prior. In CVPR , pages 4193–\\n4202, 2020. 2, 7\\n[11] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning con-\\ntinuous image representation with local implicit image func-\\ntion. In CVPR , pages 8628–8638, 2021. 3\\n[12] Ran Cheng, Ryan Razani, Ehsan Taghavi, Enxu Li, and\\nBingbing Liu. 2-s3net: Attentive feature fusion with adap-\\ntive feature selection for sparse semantic segmentation net-\\nwork. In CVPR , pages 12547–12556, 2021. 2, 7', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[13] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d\\nspatio-temporal convnets: Minkowski convolutional neural\\nnetworks. In CVPR , pages 3075–3084, 2019. 1\\n[14] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\\nnetworks. In ICCV , 2017. 4, 6\\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\\ndatabase. In CVPR , pages 248–255, 2009. 6[16] Whye Kit Fong, Rohit Mohan, Juana Valeria Hurtado, Lub-\\ning Zhou, Holger Caesar, Oscar Beijbom, and Abhinav Val-\\nada. Panoptic nuscenes: A large-scale benchmark for li-\\ndar panoptic segmentation and tracking. arXiv preprint\\narXiv:2109.03805 , 2021. 5\\n[17] Vitor Guizilini, Igor Vasiljevic, Rares Ambrus, Greg\\nShakhnarovich, and Adrien Gaidon. Full surround\\nmonodepth from multiple cameras. arXiv preprint\\narXiv:2104.00152 , 2021. 1\\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Deep residual learning for image recognition. In CVPR ,\\n2016. 6\\n[19] Anthony Hu, Zak Murez, Nikhil Mohan, Sof ´ıa Dudas, Jef-\\nfrey Hawke, Vijay Badrinarayanan, Roberto Cipolla, and\\nAlex Kendall. Fiery: Future instance prediction in bird’s-\\neye view from surround monocular cameras. In ICCV , pages\\n15273–15282, 2021. 1\\n[20] Junjie Huang, Guan Huang, Zheng Zhu, and Dalong Du.\\nBevdet: High-performance multi-camera 3d object detection\\nin bird-eye-view. arXiv preprint arXiv:2112.11790 , 2021. 1,\\n2\\n[21] Yanqin Jiang, Li Zhang, Zhenwei Miao, Xiatian Zhu, Jin\\nGao, Weiming Hu, and Yu-Gang Jiang. Polarformer: Multi-\\ncamera 3d object detection with polar transformers. arXiv\\npreprint arXiv:2206.15398 , 2022. 3\\n[22] Alex H Lang, Sourabh V ora, Holger Caesar, Lubing Zhou,\\nJiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders\\nfor object detection from point clouds. In CVPR , 2019. 2\\n[23] Jie Li, Kai Han, Peng Wang, Yu Liu, and Xia Yuan.\\nAnisotropic convolutional networks for 3d semantic scene', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='completion. In CVPR , pages 3351–3359, 2020. 2, 7\\n[24] Shijie Li, Xieyuanli Chen, Yun Liu, Dengxin Dai, Cyrill\\nStachniss, and Juergen Gall. Multi-scale interaction for real-\\ntime lidar data segmentation on an embedded platform. RA-\\nL, 7(2):738–745, 2021. 7\\n[25] Yanwei Li, Yilun Chen, Xiaojuan Qi, Zeming Li, Jian Sun,\\nand Jiaya Jia. Unifying voxel-based representation with\\ntransformer for 3d object detection. In Advances in Neural\\nInformation Processing Systems , 2022. 2, 3\\n[26] Yinhao Li, Zheng Ge, Guanyi Yu, Jinrong Yang, Zengran\\nWang, Yukang Shi, Jianjian Sun, and Zeming Li. Bevdepth:\\nAcquisition of reliable depth for multi-view 3d object detec-\\ntion. arXiv preprint arXiv:2206.10092 , 2022. 1, 2, 3\\n[27] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-\\nhao Sima, Tong Lu, Qiao Yu, and Jifeng Dai. Bevformer:\\nLearning bird’s-eye-view representation from multi-camera\\nimages via spatiotemporal transformers. In ECCV , 2022. 1,\\n2, 3, 4', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='2, 3, 4\\n[28] Tingting Liang, Hongwei Xie, Kaicheng Yu, Zhongyu Xia,\\nZhiwei Lin, Yongtao Wang, Tao Tang, Bing Wang, and Zhi\\nTang. Bevfusion: A simple and robust lidar-camera fusion\\nframework. arXiv preprint arXiv:2205.13790 , 2022. 1, 2\\n[29] Venice Erin Liong, Thi Ngoc Tho Nguyen, Sergi Wid-\\njaja, Dhananjai Sharma, and Zhuang Jie Chong. Amvnet:\\nAssertion-based multi-view fusion network for lidar seman-\\ntic segmentation. arXiv preprint arXiv:2012.04934 , 2020. 2,\\n7\\n9231\\n[30] Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun.\\nPetr: Position embedding transformation for multi-view 3d\\nobject detection. arXiv preprint arXiv:2203.05625 , 2022. 1\\n[31] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang,\\nHuizi Mao, Daniela Rus, and Song Han. Bevfusion: Multi-\\ntask multi-sensor fusion with unified bird’s-eye view repre-\\nsentation. arXiv preprint arXiv:2205.13542 , 2022. 1, 2, 3\\n[32] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='bastian Nowozin, and Andreas Geiger. Occupancy networks:\\nLearning 3d reconstruction in function space. In CVPR ,\\npages 4460–4470, 2019. 3\\n[33] B Mildenhall, PP Srinivasan, M Tancik, JT Barron, R Ra-\\nmamoorthi, and R Ng. Nerf: Representing scenes as neural\\nradiance fields for view synthesis. In ECCV , 2020. 3\\n[34] Jeong Joon Park, Peter Florence, Julian Straub, Richard\\nNewcombe, and Steven Lovegrove. Deepsdf: Learning con-\\ntinuous signed distance functions for shape representation.\\nInCVPR , pages 165–174, 2019. 3\\n[35] Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding\\nimages from arbitrary camera rigs by implicitly unprojecting\\nto 3d. In ECCV , pages 194–210, 2020. 1, 2\\n[36] Cody Reading, Ali Harakeh, Julia Chae, and Steven L\\nWaslander. Categorical depth distribution network for\\nmonocular 3d object detection. In CVPR , 2021. 2\\n[37] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\\nGeiger. Kilonerf: Speeding up neural radiance fields with', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='thousands of tiny mlps. In ICCV , pages 14335–14345, 2021.\\n3\\n[38] Luis Roldao, Raoul de Charette, and Anne Verroust-Blondet.\\nLmscnet: Lightweight multiscale 3d semantic completion.\\nIn2020 International Conference on 3D Vision (3DV) , pages\\n111–119. IEEE, 2020. 2, 7\\n[39] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model\\nscaling for convolutional neural networks. In ICML , pages\\n6105–6114, 2019. 6\\n[40] Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin,\\nHanrui Wang, and Song Han. Searching efficient 3d architec-\\ntures with sparse point-voxel convolution. In ECCV , pages\\n685–702, 2020. 2, 3, 7\\n[41] Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin.\\nFcos3d: Fully convolutional one-stage monocular 3d object\\ndetection. In ICCV , 2021. 6\\n[42] Yi Wei, Linqing Zhao, Wenzhao Zheng, Zheng Zhu, Yong-\\nming Rao, Guan Huang, Jiwen Lu, and Jie Zhou. Surround-\\ndepth: Entangling surrounding views for self-supervised\\nmulti-camera depth estimation. In CoRL , 2022. 1', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[43] Xu Yan, Jiantao Gao, Jie Li, Ruimao Zhang, Zhen Li, Rui\\nHuang, and Shuguang Cui. Sparse single sweep lidar point\\ncloud segmentation via learning contextual shape priors from\\nscene completion. In AAAI , volume 35, pages 3101–3109,\\n2021. 2, 7\\n[44] Dongqiangzi Ye, Zixiang Zhou, Weijia Chen, Yufei Xie, Yu\\nWang, Panqu Wang, and Hassan Foroosh. Lidarmultinet:\\nTowards a unified multi-task network for lidar perception.\\narXiv preprint arXiv:2209.09385 , 2022. 2, 7\\n[45] Maosheng Ye, Rui Wan, Shuangjie Xu, Tongyi Cao, and\\nQifeng Chen. Drinet++: Efficient voxel-as-point point cloudsegmentation. arXiv preprint arXiv: 2111.08318 , 2021. 2,\\n3, 7\\n[46] Yunpeng Zhang, Wenzhao Zheng, Zheng Zhu, Guan Huang,\\nJie Zhou, and Jiwen Lu. A simple baseline for multi-camera\\n3d object detection. arXiv preprint arXiv:2208.10035 , 2022.\\n1\\n[47] Yang Zhang, Zixiang Zhou, Philip David, Xiangyu Yue, Ze-\\nrong Xi, Boqing Gong, and Hassan Foroosh. Polarnet: An', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='improved grid representation for online lidar point clouds se-\\nmantic segmentation. In CVPR , pages 9601–9610, 2020. 7\\n[48] Yunpeng Zhang, Zheng Zhu, Wenzhao Zheng, Junjie Huang,\\nGuan Huang, Jie Zhou, and Jiwen Lu. Beverse: Unified per-\\nception and prediction in birds-eye-view for vision-centric\\nautonomous driving. arXiv preprint arXiv:2205.09743 ,\\n2022. 1, 2\\n[49] Yin Zhou and Oncel Tuzel. V oxelnet: End-to-end learning\\nfor point cloud based 3d object detection. In CVPR , pages\\n4490–4499, 2018. 2\\n[50] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\\nand Jifeng Dai. Deformable detr: Deformable transformers\\nfor end-to-end object detection. In ICLR , 2021. 4\\n[51] Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, Yuexin\\nMa, Wei Li, Hongsheng Li, and Dahua Lin. Cylindrical and\\nasymmetrical 3d convolution networks for lidar segmenta-\\ntion. In CVPR , pages 9939–9948, 2021. 2, 6, 7\\n9232', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Rethinking Federated Learning with Domain Shift: A Prototype View\\nWenke Huang1, Mang Ye1,2*, Zekun Shi1,H e L i1,B o D u1,2∗\\n1National Engineering Research Center for Multimedia Software, Institute of Artiﬁcial Intelligence,\\nHubei Key Laboratory of Multimedia and Network Communication Engineering,\\nSchool of Computer Science, Wuhan University, Wuhan, China.\\n2Hubei Luojia Laboratory, Wuhan, China.\\nhttps://github.com/WenkeHuang/RethinkFL\\nAbstract\\nFederated learning shows a bright promise as a privacy-\\npreserving collaborative learning technique. However ,\\nprevalent solutions mainly focus on all private data sampledfrom the same domain. An important challenge is that whendistributed data are derived from diverse domains. The pri-\\nvate model presents degenerative performance on other do-\\nmains (with domain shift). Therefore, we expect that theglobal model optimized after the federated learning pro-\\ncess stably provides generalizability performance on mul-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tiple domains. In this paper , we propose Federated Proto-types Learning (FPL) for federated learning under domainshift. The core idea is to construct cluster prototypes and\\nunbiased prototypes, providing fruitful domain knowledgeand a fair convergent target. On the one hand, we pull thesample embedding closer to cluster prototypes belongingto the same semantics than cluster prototypes from distinct\\nclasses. On the other hand, we introduce consistency reg-ularization to align the local instance with the respective\\nunbiased prototype. Empirical results on Digits and Ofﬁce\\nCaltech tasks demonstrate the effectiveness of the proposed\\nsolution and the efﬁciency of crucial modules.\\n1. Introduction\\nFederated learning is a privacy-preserving paradigm [ 47,\\n83], which reaches collaborative learning without leaking\\nprivacy. The cornerstone solution, FedAvg [ 47], aggregates', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='parameters from participants and then distributes the globalmodel (averaged parameters) back for further training,which aims to learn a high-quality model without central-izing private data. However, an inherent challenge in feder-\\nated learning is data heterogeneity [ 26,39,69,87]. Speciﬁ-\\ncally, the private data is collected from distinct sources with\\ndiverse preferences and presents non-iid (independently and\\n*Corresponding Author: Mang Ye, Bo Du', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='\\x08\\x07\\x08 \\x01\\n\\x02\\n \\x08\\x0b\\x03\\x06 \\x01\\x08\\x0b\\x02\\n \\x08\\x0c\\x06 \\x01\\x08\\x0c\\x02\\n \\x05\\x06\\x04\\x08\\t \\x01\\x05\\x02\\n\\x03\\x05\\n\\x11\\x07\\n\\x01\\x07\\x05\\x0e\\x0b\\n\\x0b\\x08\\n\\x04\\x0e\\x0c\\n\\x0c\\x0f\\x07\\x06\\n\\x02\\x07\\x10\\t\\x0c\\x06\\n\\x02\\x01\\x02\\x04\\x02 \\x01 \\x02\\x03\\x05 \\x02\\x01 \\x02\\x03\\x06\\nFigure 1. Illustration of heterogeneous federated learning . The\\nfeature visualization on inter domains ( →represents testing on tar-\\nget domain i.e., M→SV means that local dataset is from MNIST\\nand test model on SVHN). The toprow indicates that local train-\\ning results in domain shift. The bottom row shows that our method\\nacquires generalizable performance on different domains.\\nidentically distributed) distribution [ 87]. Each participant\\noptimizes toward the local empirical risk minimum, whichis inconsistent with the global direction. Therefore, the av-\\neraged global model unavoidably faces a slow convergence\\nspeed [ 40] and achieves limited performance improvement.\\nA mainstream of subsequent efforts delves into introduc-\\ning a variety of global signals to regulate private model\\n[13,28,38,40,51,66,70]. These methods focus on la-\\nbel skew, where distributed data are from the same do-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='main , and simulate data heterogeneity via imbalanced sam-\\npling, e.g., Dirichlet strategy [ 32] to generate different la-\\nbel distributions. Nonetheless, another noticeable data het-\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n16312\\nerogeneous property in federated learning is domain shift\\n[21,22,41,55,57]. In particular, private data is derived from\\nvarious domains , leading to distinct feature distributions.\\nIn this scenario, we argue that naive learning on private databrings poor generalizable ability in Fig. 1. Speciﬁcally, the\\nprivate model fails to provide discrimination on other do-\\nmains because it overﬁts local domain distribution. Theaforementioned methods mainly regulate the private modelvia global knowledge ( i.e., the average signals from partici-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='pants). Therefore, these algorithms share a common weak-ness: the global information is insufﬁcient to describe di-\\nverse domain knowledge , which is magniﬁed under the do-\\nmain shift and thus hinders the improvement of generaliz-\\nability. An intuitive solution is to preserve multiple modelsfor distilling respective domain knowledge. However, it in-\\ncurs a high cost of both communication and computation.\\nTaking into account both the effectiveness and efﬁciency,\\nwe rethink the prototype [ 11,36,67,82,91], which is the\\nmean value of features with identical semantics. It rep-\\nresents class-wise characteristics and is vector type [ 90].\\nGiven the enormous participant scale in federated learning,\\nit is not efﬁcient and feasible to maintain all prototypes.However, directly averaging all prototypes to get global pro-totypes would arise the same impediment as global mod-\\nels because averaging operation weakens the domain diver-sity. Besides, global prototypes probably yield biased to the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='dominant domain due to the unknown of private domainsproportion, which results in disadvantageous performance\\non minority domains. Driven by these two issues, on the\\none hand, we ﬁnd representative prototypes by clustering\\nall prototypes. Therefore, each class is abstracted by a setof diverse prototypes, capturing rich domain variance. On\\nthe other hand, we generate unbiased prototypes based oncluster prototypes to construct fair and stable global signals,\\nwhich avoid optimizing toward the underlying primary do-main and thus ensure stability on different domains. Com-\\npared with original feature vectors, cluster and unbiased\\nprototypes are privacy-friendly because it experiences twiceand third times averaging operation [ 70]. Hence, it is less\\nfeasible to disentangle each raw representation and subse-\\nquently reconstruct private data. We analyze the superiority\\nof cluster prototypes and unbiased prototypes in Sec. 3.2.\\nIn this paper, we propose Federated Prototype Learning', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='(FPL), which consists of two components. First ,i no r -\\nder to improve the generalizability on the premise of dis-\\ncriminability. We introduce Cluster Prototypes ContrastiveLearning (CPCL), which leverages cluster prototypes to\\nconstruct contrastive learning [ 7,19,79,84,85]. CPCL adap-\\ntively enforces the query embedding to be more similar to\\ncluster prototypes from the same class than other prototypes\\nwith different semantics. In particular, such an objective en-\\ncourages instance feature to be close to representative proto-types in the same semantic and separates it away from otherclass prototypes, which incorporates diverse domain knowl-\\nedge and maintains a clear decision boundary. Second ,w e', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='utilize unbiased prototypes to provide a fair and stable con-vergence point and propose Unbiased Prototypes Consis-tent Regularization (UPCR). Speciﬁcally, we average clus-ter prototypes to acquire unbiased prototypes. The localinstance is required to minimize the feature-level distancewith the corresponding unbiased prototype. Therefore, thelocal model would not be biased toward dominant domainsand exhibits stable performance on inferior domains. Weconjecture that these two components together make FPLa competitive method for federated learning with domainshift. The main contributions are summarized below.\\n• We focus on heterogeneous federated learning with do-\\nmain shift and identify that the inherent limitation of ex-isting methods is that global regularization signal is in-sufﬁcient to depict diverse domain knowledge and biasedtoward major domain among participants.\\n• We propose a simple yet effective strategy to learn a well', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='generalizable global model in federated learning with do-main shift. Inspired by the success of prototype learn-ing, we introduce cluster prototypes to provide rich do-\\nmain knowledge and further construct unbiased proto-\\ntypes based on the average of cluster prototypes to further\\noffer fair and stable objective signal.\\n• We conduct extensive experiments on Digits [ 23,33,52,\\n61] and Ofﬁce Caltech [ 16] tasks. Accompanied with a\\nset of ablative studies, promising results validate the efﬁ-\\ncacy of FPL and the indispensability of each module.\\n2. Related Work\\n2.1. Data Heterogeneous Federated Learning\\nFederated learning is proposed to handle privacy con-\\ncerns in the distributed learning environment. A pioneer-\\ning federated method, FedAvg [ 47] trains a global model by\\naggregating local model parameters. However, its perfor-\\nmance is impeded due to decentralized data, which poses\\nnon-i.i.d distribution (called data heterogeneity). Based', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='on FedAvg, existing methods of tackling data heterogene-ity problem mainly leverage global penalty term. Fed-\\nProx [ 40], FedCurv [ 66], pFedME [ 68], and FedDyn [ 1]\\ncalculate global parameter stiffness to control discrepan-\\ncies. Besides, MOON [ 38], FedUFO [ 86], FedProto [ 70],\\nand FedProc [ 51] maximize feature-level agreement of lo-\\ncal model and global model. Moreover, SCAFFOLD [ 28]\\nand FedDC [ 13] leverage global gradient calibration to con-\\ntrol local drift. The major limitation of these methods is thatthey focus on single domain performance under label skew\\nscenario and overlook the problem of domain shift, lead-ing to an unsatisfying generalizable performance on multi-ple domains. Closely related methods such as FedBN [ 41],\\nADCOL [ 37], FCCL [ 22] focus on personalized models\\nrather than a shared global model. Besides, the latter two\\n16313\\nmethods require additional discriminator and public data,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='which incurs a heavy burden for either the participant or theserver side. In this paper, we introduce cluster prototypes(diverse domain knowledge) and unbiased prototypes (con-sistent optimization direction) to learn a generalizable and\\nstable global model during the federated learning process.\\n2.2. Clustering Federated Learning\\nClustered federated learning involves grouping clients\\nwith similar data distributions into clusters, such that eachclient is uniquely associated with a particular data distri-\\nbution and contributes to the training of a model tailored\\nto that distribution [ 64,80]. Existing methods can mainly\\nleverage four types of clustering signals: model parameters[5,43], gradient information [ 10,64], training loss [ 15,46]\\nand exogenous information [ 2,42]. However, we leverage\\nthe clustering strategy to select representative prototypes in\\norder to address the federated learning with domain shift.\\n2.3. Prototype Learning', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Prototype refers to the mean vector of the instances be-\\nlonging to the identical class [ 67]. Due to its exemplar-\\ndriven nature and simpler inductive bias, it has boostedgreat potential in a variety of tasks. For example, in super-vised classiﬁcation tasks, it labels testing images via cal-culating its distance with prototypes of each class, which\\nis considered to be more robust and stabler [ 81] in han-\\ndling few-shot [ 48,67,75,81], zero-shot [ 25]. Moreover,\\nit also has been a surge of interest in semantic segmentationtask [ 35,77,90], unsupervised learning [ 18,36,79,84,85]\\nand so on. As for federated learning, prototypes can pro-\\nvide diverse abstract knowledge while adhering to privacy\\nprotocols. There exist a few works incorporating pro-totypes to handle data heterogeneous federated learning.\\nPGFL [ 49] leverages prototypes to construct weight atten-\\ntion parameter aggregation. FedProc [ 51] and FedProto\\n[70] aim to reach feature-wise alignment with global pro-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='totypes. CCVR [ 44] generates virtual feature based on ap-\\nproximated Gaussian Mixture Model. FedPCL [ 71] focuses\\non personalized federated learning and utilizes prototypesto learn personalized models. However, these methods fo-cus on single-domain performance. In domain shift, it is vi-tal to consider generalization on diverse domains. Our work\\nsheds light on leveraging cluster and unbiased prototypes to\\nachieve this goal in federated learning.\\n2.4. Contrastive Learning\\nContrastive learning has recently become a promising\\ndirection in the self-supervised learning ﬁeld, achieving\\ncompetitive performance as supervised learning. The clas-\\nsic methods [ 7,19,53,79,84,85] mainly construct a pos-\\nitive pair and a negative pair for each instance and lever-age InfoNCE [ 54] to contrast positiveness against neg-ativeness. A major branch of subsequent research fo-\\ncuses on elaborating the selection of the informative pos-itive pairs [ 3,12,30,31,34,56,59,65] and negative pairs', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[8,14,24,27,50,60,72,88]. Another line explicitly inves-\\ntigates the semantic structure and introduces unsupervised\\nclustering methods to construct fruitful prototypes as rep-resentative embeddings for groups of semantically similarinstances [ 6,18,36,73,89]. Differently, in this work, Clus-\\nter Prototypes Contrastive Learning (CPCL) is designed for\\nproviding generalization ability in federated learning withdomain shift. We leverage the unsupervised clustering al-\\ngorithm to select representative prototypes for each class\\nand then seek to attract each instance to cluster prototypes\\nin the same semantics while pushing away other cluster pro-totypes from different classes, which brings both generaliz-\\nable and discriminative ability.\\n3. Methodology\\n3.1. Preliminaries\\nFollowing the typical federated learning [ 40,47], there\\nareMparticipants (indexed by m) with respective private\\ndata,Dm={xi,yi}Nm\\ni=1, whereNmdenotes the local data\\nscale. Under heterogeneous federated learning, the condi-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tional feature distribution P(x|y)varies across participants\\neven ifP(y)is consistent, resulting in domain shift :\\n•Domain shift :Pm(x|y)/negationslash=Pn(x|y)(Pm(y)=\\nPn(y)). There exists domain shift among private data.\\nSpeciﬁcally, for the same label space, distinctive fea-ture distribution exists among different participants.\\nBesides, participants agree on sharing a model with the\\nsame architecture. We regard the model with two modules:feature extractor and uniﬁed classiﬁer. The feature extractorf:X→Z , encodes sample xinto a compact ddimen-\\nsional feature vector z=f(x)∈R\\ndin the feature space\\nZ. A uniﬁed classiﬁer g:Z→R|I|, maps feature zinto\\nlogits output l=g(z), whereImeans the classiﬁcation\\ncategories. The optimization direction is to learn a gener-\\nalizable global model to present favorable performance on\\nmultiple domains, through the federated learning process.\\n3.2. Prototypes Meet Federated Learning\\nMotivation . Each prototype ck∈Rdis calculated by the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='mean vector of the features belonging to same class:\\nck=1\\n|Sk|/summationdisplay\\n(xi,yi)∈Skf(xi),(1)\\nwhereSkmeans the set of samples annotated with class\\nk. Prototypes are typical for respective semantic informa-\\ntion. Besides, it carries the speciﬁc domain style informa-\\ntion because the prototypes are not consistent on differentdomains. Therefore, it motivates us to leverage prototypesfrom different domains to learn a generalizable model with-\\nout leaking privacy information. We further deﬁne the k\\nth\\n16314\\n\\x01\\t\\x10\\x0e\\x0f\\x07\\n\\x01\\t\\x10\\x0e\\x0f\\x07\\n\\x01\\t\\n\\x01\\x08\\n\\x01\\x04\\n\\x01\\t\\x10\\x0e\\x0f\\x07\\n\\x02\\n\\x0b\\x0f\\x0b\\x0f\\x11\\x0c\\x07\\x0e\\x03\\n\\x05\\x08\\x04\\x0e\\x07\\x06\\n\\x02\\n\\x0b\\x0f\\x0b\\x0f\\x11\\x0c\\x07\\x0e\\n\\x06\\x04\\x01\\x01\\x05\\x01\\x07\\x02\\x03\\x02\\n\\x02\\x01\\x05\\x01\\x03\\n\\x02\\x07\\x05\\x01\\x06\\x02\\x01\\x02\\x03\\x07\\x0b\\n\\x07\\x0b\\x01\\x03\\x08\\x06\\x07 \\x02\\x04\\x0b\\x0c\\x08\\x05\\x08\\n\\x04\\t\\x0c\\x01\\x03\\x08\\x06\\x07\\nEq. ( 11)\\nEq. ( 8)\\nEq. ( 10)\\nFigure 2. Architecture illustration of Federated Prototypes Learning (FPL). Participants upload local prototypes to server. Based on these\\nprototypes, we introduce cluster prototypes ( ) to construct Cluster Prototypes Contrastive Learning (CPCL in Sec. 3.3.1 ), bringing', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='diverse domain information. Besides, we acquire unbiased prototypes ( ) and propose Unbiased Prototypes Consistent Regularization\\n(UPCR in Sec. 3.3.2 ) to provide a stable consistency signal. Best viewed in color. Zoom in for details.\\nclass prototype from the mthparticipant as:\\nck\\nm=1\\n|Skm|/summationdisplay\\n(xi,yi)∈Skmfm(xi)\\nOm=[c1\\nm,...,ckm,...,c|I|\\nm]∈R|I|×d,(2)\\nwhereSk\\nm={xi,yi|yi=k}Nk\\nm\\ni=1⊂Dmrepresents the\\nprivate dataset Dmof thekthclass for the mthparticipant.\\nGlobal Prototypes . Considering that number of partici-\\npants is large-scale in federated learning, the straightfor-\\nward solution to leverage prototypes is the global proto-\\ntypes (G) via directly averaging operation akin to the global\\nmodel. Hence, the global prototypes are formulated into:\\nGk=1\\nNN/summationdisplay\\nm=1ck\\nm∈Rd\\nG=[G1,...,Gk,...,G|I|].(3)\\nHowever, global prototypes mainly suffer from two notable\\nproblems. \\x82Global prototype unavoidably faces the same', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='dilemma as the global model. In detail, it depicts each class\\nsignal by only one prototype, bearing no domain variationunder heterogeneous federated learning with domain shift.\\n\\x83Moreover, due to the unknown of participants data dis-\\ntribution in federated learning, global prototypes would be\\nbiased toward the dominant domain distribution, leading toa skewed optimization objective during federated process.\\nCluster Prototypes . Inspired by such limitations, we ﬁrst\\npropose cluster prototypes. Compared with global proto-types, we select representative prototypes rather than sin-\\ngle one via unsupervised clustering method, FINCH [ 63].\\nCompared with well-known clustering techniques such as\\nKmeans [ 4,45] and HAC [ 78], FINCH is parameter-free\\nand thus suitable for federated learning with uncertain par-\\nticipants scale. Speciﬁcally, FINCH views that the nearest\\nneighbor of each sample is a sufﬁcient support for grouping.It implicitly picks characteristic prototypes because proto-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='types from different domains are less likely to be the ﬁrst\\nneighbor. Therefore, prototypes from different domainsprobably fail to merge together, while prototypes from sim-\\nilar domains fall into the same group, conversely. Speciﬁ-cally, we leverage cosine similarity to evaluate the distancebetween any two prototypes and view the prototype with\\nminimum distance as its ’neighbor’, sorted into the same\\nset. We deﬁne the k\\nthclass prototype adjacency matrix as:\\nAk(m,n )=/braceleftBigg\\n1,ifn=vk\\nmorm=vk\\nnorvk\\nm=vk\\nn;\\n0,otherwise ,(4)\\nwherevk\\nmdenotes the ﬁrst neighbor (largest cosine similar-\\nity) of the class kprototype from the mthparticipant, ck\\nm.\\nThen, we select several representative prototypes in the em-\\nbedding space based on the clustering results via Eq. ( 4).\\nThus, the cluster prototypes ( P) are denoted as:\\nPk={ck\\nm}Nm=1Cluster−−−−→{ck\\nm}Jm=1∈RJ×d\\nP={P1,...,Pk,...,P|I|}.(5)\\nWe cluster Nprototypes into Jrepresentatives of class k,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='which effectively addresses the aforementioned problem \\x82.\\nUnbiased Prototypes . Nevertheless, the scale of cluster\\nprototypes is variant because the unsupervised clusteringmethods generate them after each communication, which\\ncan not ensure a stable and fair convergent point. Thus, we\\nfurther average cluster prototypes to get a consistent signal:unbiased prototypes ( U), which is calculated as follows:\\nUk=1\\nJ/summationdisplay\\nck∈Pkck∈Rd\\nU=[U1,...,Uk,...,U|I|].(6)\\nNote that compared with global prototypes ( G), unbiased\\nprototypes ( U) largely avoid being biased toward dominant\\ndomains in heterogeneous federated learning and provide astable optimization target. Thus, we hypothesize that unbi-\\n16315\\nGlobal Prototype\\n Cluster Prototype\\n Unbiased Prototype\\nFigure 3. Illustration of different prototypes . Global prototype\\n() fails to describe diverse domains information and is biased\\ntoward the underlying dominant domain. Cluster prototype (', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content=') and unbiased prototype ( ) carry multiple domain knowledge\\nand stable optimization signal. See details in Sec. 3.2.\\nased prototypes could depict the considerably fair conver-\\ngent point and further leverage them to conduct consistentregularization, which productively handles the problem \\x83.\\nDiscussion . We further explain the difference of these three\\nkinds of prototypes in Fig. 3. Global prototypes inherently\\npresent limited domain knowledge and show skewed fea-\\nture space toward the potentially dominant domains in het-\\nerogeneous federated learning. Cluster and unbiased proto-types complementarily handle these two problems because\\nthe former provides fruitful domain knowledge and the lat-ter represents an ideal optimization target, collaborativelyensuring both generalization and stability. Compared with\\nexisting methods that leverage the global model to constructregularization term, prototypes are substantially smaller in', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='size than model parameters, which bring less computationcost for participants. Besides, cluster prototypes and un-biased prototypes are privacy-safe because they experiencetwice and third times averaging operations through unsu-\\npervised clustering. Therefore, leveraging these two kindsof prototypes: cluster prototypes and unbiased prototypesis not only a computation-friendly media but also a privacy-\\npreserving solution in heterogeneous federated learning.\\n3.3. Federated Prototypes Learning\\nFor generalizability and stability in heterogeneous fed-\\nerated learning with domain shift, we leverage cluster pro-totypes and unbiased prototypes to obtain fruitful domain\\nknowledge and stable consistency signal. The proposed\\nmethod comprises two key components: Cluster Prototypes\\nContrastive Learning (CPCL in Sec. 3.3.1 ) and Unbiased\\nPrototypes Consistent Regularization (UPCR in Sec. 3.3.2 ).\\n3.3.1 Cluster Prototypes Contrastive Learning\\nWe deem that a well-generalizable representation should', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='not only be discriminative to provide a clear decisional\\nboundary for different classes but also be as invariant as\\npossible to diverse domain distortions that are applied to\\nthis sample. Speciﬁcally, for instance (x\\ni,yi)∈Dm,w e\\nfeed it into network and acquire its feature vector zi=f(xi). Then, we enforce the instance feature to be simi-\\nlar to respective semantic prototypes ( Pk) and dissimilar to\\ndifferent semantic prototypes ( Nk=P−Pk). We deﬁne\\nthe similarity of the query sample embedding ziwith corre-\\nsponding cluster prototypes c∈P as follows:\\ns(zi,c)=zi·c\\n||zi||×||c||/τ, (7)\\nwhere temperature hyper-parameter, τcontrols the concen-\\ntration strength of representations [ 76]. Thus, we expect to\\nenlarge the similarity with semantic coincident cluster pro-totypes than other different cluster prototypes, which aims\\nto maintain a clear class-wise decision boundary. In ourwork, we introduce Cluster Prototypes Contrastive Learn-\\ning (CPCL) to contrast cluster prototypes with the same', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='class for each query sample against other remainder of clus-ter prototypes with different semantics. It is natural to de-\\nrive the following optimization objective term:\\nLCPCL =−log/summationtext\\nc∈Pkexp(s(zi,c))/summationtext\\nc∈Pkexp(s(zi,c)) +/summationtext\\nc∈Nkexp(s(zi,c))\\n=log(1+/summationtext\\nc∈Nkexp(s(zi,c))/summationtext\\nc∈Pkexp(s(zi,c))),\\n(8)\\nWe give a detailed optimization direction analysis of Eq. ( 8)\\nand thus reformulate the CPCL loss function as follows:\\nminLCPCL\\n≡log(/summationtext\\nc∈Nkexp(s(zi,c))/summationtext\\nc∈Pkexp(s(zi,c)))\\n≡log(/summationdisplay\\nc∈Nkexp(s(zi,c)))\\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\\nDiscriminative−log(/summationdisplay\\nc∈Pkexp(s(zi,c)))\\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\\nGeneralizable.\\n(9)\\nNote that minimizing Eq. ( 8) equally requires pulling em-\\nbedding vector ziclosely to its assigned positive cluster\\nprototypes ( Pk) and pushing zifar away from others neg-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ative prototypes ( Nk), which not only aims to be invariant\\nto diverse domain distortions but also enhances the seman-tic spread-out property, promising both generalizable anddiscriminative property of feature space and thus acquiring\\nsatisfying generalizable performance in federated learning.\\n3.3.2 Unbiased Prototypes Consistent Regularization\\nAlthough cluster prototypes bring diverse domain knowl-\\nedge for the sake of plasticity under domain shift, the cluster\\nprototypes are dynamically generated at each communica-\\ntion and its scale is changing due to the unsupervised clus-tering method. Therefore, cluster prototypes could not offer\\na stable convergence direction at different communication\\nepochs. We assume that unbiased prototypes ( Uin Eq. ( 6))\\nbased on averaged cluster prototypes, could provide a rela-tively fair and stable optimization point and thus cope withthe problem of convergence instability. Thus, in this paper,\\nwe purpose Unbiased Prototypes Consistent Regularization', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='16316\\n(UPCR) to leverage unbiased prototypes. Speciﬁcally, we\\nutilize a consistency regularization term to pull the featurevectorz\\nicloser to the respective unbiased prototype, Ukas:\\nLUPCR =d/summationdisplay\\nv=1(zi,v−Uk\\nv)2, (10)\\nwherevindexes the dimension of feature output. We expect\\nto achieve feature-level alignment between query embed-\\nding and the corresponding unbiased prototype. Besides,we construct CrossEntropy [ 9] loss and use the logits output\\n(l\\ni=g(zi)) with original annotation signal ( yi) to maintain\\nlocal domain discriminative ability via:\\nLCE=−1yilog(σ(li)), (11)\\nwhereσdenotes softmax . Finally, we carry out the fol-\\nlowing optimization objective in local updating phase:\\nL=LCPCL +LUPCR +LCE. (12)\\nThe overall federated learning algorithm is shown in Al-\\ngorithm 1. In each communication epoch, the server dis-\\ntributes the cluster prototypes and unbiased prototypes to\\nparticipants. In local updating, each participant optimizes', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='on local data, while the objective is deﬁned in Eq. ( 12).\\n4. Experiments\\n4.1. Experimental Setup\\nDatasets . We evaluate methods on two classiﬁcation tasks:\\n• Digits [ 23,33,52,61] includes four domains: MNIST\\n(M), USPS (U), SVHN (SV) and SYN (SY) with 10\\ncategories (digit number from 0 to 9).\\n• Ofﬁce Caltech [ 16] consists four domains: Caltech\\n(C), Amazon (A), Webcam (W) and DSLR (D), whichis formed of ten overlapping classes between Ofﬁce31\\n[62] and Caltech-256 [ 17].\\nWe initialize 20 and 10 participants for Digits and Ofﬁce\\nCaltech tasks and randomly allocate domains for partici-\\npants. In detail, the Digits task is MNIST: 3, USPS: 7,\\nSVHN: 6 and SYN: 4. The Ofﬁce Caltech is Caltech: 3,Amazon: 2, Webcam: 1 and DSLR: 4. For each participant,\\nlocal data is randomly selected from these domains with dif-\\nferent proportions ( i.e., 1%in Digits and 20 %in Ofﬁce\\nCaltech), based on the difﬁculty and scale of the tasks. We\\nﬁx the seed to ensure reproduction of our results.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Model . For these two classiﬁcation tasks, we conduct ex-\\nperiment with ResNet-10 [ 20]. The feature vector dimen-\\nsion is 512. Note that all methods use the same network\\narchitecture for fair comparison in different tasks.\\nCounterparts . We compare ours against several sota\\nfederated methods focusing on learning a shared global\\nmodel : FedAvg (AISTATS’17 [ 47]), FedProx (arXiv’18\\n[40]), MOON (CVPR’21 [ 38]), FedDyn (ICLR’21 [ 1]), Fe-\\ndOPT (ICLR’21 [ 58]), FedProc (arXiv’21 [ 51]) and Fed-\\nProto (AAAI’22 [ 70] with parameter averaging).Algorithm 1: FPL\\nInput: Communication epochs E, local rounds T,\\nnumber of participants M,mthparticipant private\\ndataDm(x,y), private model θm\\nOutput: The ﬁnal global model θE\\nfore=1,2,...,E do\\nParticipant Side ;\\nform=1,2,...,N in parallel do\\nθe\\nm,Om←LocalUpdating (θe,P,U)\\nServer Side ;\\nθe+1←/summationtextN\\nm=1|Dm|\\n|D|θe\\nm\\n/*Cluster prototypes */\\nPk={ck\\nm}Nm=1Cluster−−−−→{ck\\nm}Jm=1via Eq. ( 5)\\n/*Unbiased prototypes */\\nUk=1\\nJ/summationtext', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='J/summationtext\\nck∈Pkckby Eq. ( 6)\\nLocalUpdating (θe,P,U):\\nθe\\nm←θe; // Distribute global parameter\\nfort=1,2,...,T do\\nfor(xi,yi)∈Dmdo\\nzi=fe\\nm(xi)\\nli=ge\\nm(zi)\\n/*Cluster Prototypes Contrastive\\nLearning */\\nLCPCL←(zi,P)in Eq. ( 8);// Sec. 3.3.1\\n/*Unbiased Prototypes Consistent\\nRegularization */\\nLUPCR←(zi,U)in Eq. ( 10);// Sec. 3.3.2\\nLCE←(li,yi)in Eq. ( 11)\\nL=LCPCL +LUPCR +LCE\\nθe\\nm←θe\\nm−η∇L\\nOm={}; // Initialize local prototypes\\n/*Local prototypes */\\nfork=1,2,...,|I|do\\nSk\\nm={xi,yi|yi=k}Nk\\nm⊂Dm\\nck\\nm=1\\nNkm/summationtext\\n(xi,yi)∈Skmfm(xi)\\nOm=Om∪{ck\\nm}in Eq. ( 2)\\nreturnθe\\nm,Om\\nImplement Details . To enable a fair comparison, we fol-\\nlow the same setting in [ 22,38]. We conduct communication\\nepoch for E= 100 and local updating round T=1 0 , where\\nall federated learning approaches have little or no accuracygain with more communications. We use the SGD opti-\\nmizer with the learning rate lr=0.01for all approaches.\\nThe corresponding weight decay is 1e−5and momentum', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='is0.9. The training batch size is 64. The hyper-parameter\\nsetting for FPL presents in the next Sec. 4.2.\\nEvaluation Metric . Following [ 40,47], Top-1 accuracy is\\nadopted for fair evaluation in these two classiﬁcation tasks.\\nWe conduct experiments for three times and utilize the lastﬁve communication epochs accuracy as ﬁnal performance.\\n16317\\n\\x05\\x04\\x01\\x02\\x03\\n\\x08\\x02\\x08\\x03\\x08\\x04\\x08\\x05\\x08\\x06\\n\\x02\\x01\\x02\\x03\\x07 \\x02\\x01\\x02\\x04 \\x02\\x01\\x02\\x04\\x07 \\x02\\x01\\x02\\x05 \\x02\\x01\\x02\\x05\\x07 \\x02\\x01\\x02\\x06 \\x02\\x01\\x02\\x06\\x07 \\x02\\x01\\x02\\x07 \\x02\\x01\\x03 \\x02\\x01\\x04\\x07\\t\\x08\\n\\x06\\n\\x0c\\x0b\\nFigure 4. Analysis of FPL with different temperature (Eq. ( 7)).\\n”Base” denotes FedAvg. See details in Sec. 4.2.\\nDigitsCPCL UPCRMNIST USPS SVHN SYN AVG\\n98.14 90.85 76.56 55.01 80.14\\n\\x13 98.03 91.13 79.76 56.84 81.44\\n\\x13 98.23 93.12 81.18 55.40 81.98\\n\\x13 \\x13 98.31 92.71 80.27 61.20 83.12\\nOfﬁce CaltechCPCL UPCRCaltech Amazon Webcam DSLR AVG\\n60.15 75.44 45.86 36.00 54.36\\n\\x13 61.65 78.16 43.62 45.33 57.19\\n\\x13 64.26 79.54 48.39 44.67 59.21\\n\\x13 \\x13 63.39 79.26 55.86 48.00 61.63\\nTable 1. Ablation study of key components of our method in', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Digits and Ofﬁce Caltech task. Please see Sec. 4.2for details.\\n\\x03\\x05\\x04\\x05\\x05\\x05\\x06\\x05\\x07\\x05\\x08\\x05\\nCaltech Amazon Webcam DSLR\\x04\\x08\\x08\\x0b\\x08\\x06\\x07\\x01\\x02\\x05\\x0c\\x0e\\x07\\x06\\nCluster\\nGlobal\\n\\x05\\x01\\x06\\x01\\x07\\x01\\x08\\x01\\t\\x01\\x02\\x01\\x01\\nMNIST USPS SVHN SYN\\x03\\x0b\\t\\x0b\\x0e\\nDigits Ofﬁce CaltechPrototypeAVG /triangle AVG /triangle\\nGlobalG 80.82 - 56.37 -\\nClusterP 81.44 +0.62 58.45 +2.08\\nFigure 5. Comparison of cluster ( Pin Eq. ( 5)) and global pro-\\ntotypes (Gin Eq. ( 3)) for CPCL (Sec. 3.3.1 )on each domain (Top\\nRow) and overall performance (Bottom Row) in Digits and OfﬁceCaltech tasks with τ=0.02. See details in Sec. 4.2.1 .\\n4.2. Diagnostic Analysis\\nFor thoroughly analyzing the efﬁcacy of essential mod-\\nules in our approach, we perform an ablation study on\\nDigits and Ofﬁce Caltech to investigate: Cluster Proto-types Contrastive Learning (CPCL) and Unbiased Proto-\\ntypes Consistent Regularization (UPCR). We ﬁrstly present', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the overall performance with different contrastive tempera-ture (τin Eq. ( 8)). The Fig. 4reveals that a smaller temper-ature beneﬁts training more than higher ones, but extremely\\nlow temperatures are harder to train due to numerical insta-bility (L=NaN in Eq. ( 12) whenτ=0.01), corroborat-\\ning relevant observations reported in [ 7,29,74,76]. Specif-\\nically, the accuracy progressively increases as τenlarges,\\nand the amelioration becomes marginal when τ=0.02.\\nHence, we choose τ=0.02by default. We further give a\\nquantitative result on these two components in Tab. 1. The\\nﬁrst row refers to the FedAvg which directly averages modelparameter without extra operation. Three crucial conclu-sions can be drawn. First , CPCL leads to signiﬁcant perfor-\\nmance improvements against the baseline on different tasks.This evidences that CPCL strategy is able to produce gener-\\nalizable feature space. Second , we notice gains by incorpo-\\nrating UPCR into the baseline. This proves the importance', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='of considering consistent regularization. Third , combining\\nCPCL and UPCR achieves better performance, which sup-\\nports our motivation of exploiting joint generalization andstability in heterogeneous federated learning.\\n4.2.1 Cluster Prototypes Contrastive Learning\\nTo prove the superiority of cluster prototypes ( Pin Eq. ( 5))\\nin providing generalizable and discriminative ability, we\\ncompare them with global prototypes ( Gin Eq. ( 3)) on Of-\\nﬁce Caltech task under contrastive temperature τ=0.02in\\nFig.5. The results reveal that leveraging cluster prototypes\\nperforms better than global prototypes and thus conﬁrm our\\nmotivation of leveraging multiple prototypes to capture di-verse domain knowledge. For example, in Ofﬁce Caltech\\ntask, cluster prototypes achieve 2.08% overall performance\\ngain compared with global prototypes.\\n4.2.2 Unbiased Prototypes Consistent Regularization\\nNote that both global prototypes ( G) and unbiased proto-\\ntypes (U) are able to offer consistent regularization. We', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='conduct experiments on Digits and Ofﬁce Caltech in Tab.\\n3.\\nThese results conﬁrm the superiority of utilizing unbiased\\nprototypes to offer consistent signal. As seen, the unbiased\\nprototypes provide a better convergence signal than globalprototypes and present the increased performance on differ-\\nent tasks i.e., Digits ( +0.82) and Ofﬁce Caltech ( +0.73).\\n4.3. Comparison to State-of-the-Arts\\nThe Tab. 2plots the ﬁnal accuracy metric by the end of\\nfederated learning process with popular sota methods. Itclearly depicts that our method performs signiﬁcantly better\\nthan counterparts, which conﬁrms that FPL can acquire wellgeneralizable ability and thus effectively boost performance\\non different domains. Take the result of Ofﬁce Caltech as an\\nexample, our method outperforms the best counterpart with\\na gap of 4.59%. We visualize the t-SNE visualization anal-\\nysis of FPL at different communication epochs in Fig. 7,\\n16318', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='16318\\nDigits Ofﬁce CaltechMethodsMNIST USPS SVHN SYN AVG /triangle Caltech Amazon Webcam DSLR AVG /triangle\\nFedAvg [ASTAT17] [47] 98.14 90.85 76.56 55.01 80.14 - 60.15 75.44 45.86 36.00 54.36 -\\nFedProx [arXiv18] [40] 98.11 90.24 77.01 56.66 80.50 +0.36 60.21 77.44 48.62 37.33 55.90 +1.54\\nMOON [CVPR21] [38] 97.44 92.15 77.62 38.79 76.50 -3.64 56.19 71.54 41.04 30.22 49.74 -4.62\\nFedDyn [ICLR21] [1] 98.01 91.00 78.95 54.22 80.54 +0.40 61.64 75.54 48.28 35.56 55.25 +0.89\\nFedOPT [ICLR21] [58] 96.23 91.80 73.03 57.85 79.72 -0.42 56.31 56.74 63.33 48.89 56.31 +1.95\\nFedProc [arXiv21] [51] 97.86 88.99 78.90 45.84 77.89 -2.25 58.07 73.65 42.76 30.22 51.17 -3.19\\nFedProto [AAAI22] [70] 98.30 92.44 80.35 53.58 81.16 +1.02 64.02 79.37 50.17 40.33 58.47 +4.11\\nFPL 98.31 92.71 80.27 61.20 83.12 +2.98 63.39 79.26 55.86 48.00 61.63 +7.27\\nTable 2. Comparison with the sota methods on Digits and Ofﬁce Caltech tasks. A VG denotes average accuracy calculated on all', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='domains. See details in Sec. 4.3. Best in bold and second with underline. These notes are the same to others.\\n\\x05\\x04\\x01\\x03\\x02\\x05\\x04\\x01\\x02\\x03\\n\\x01\\x04\\x02\\x04\\x03\\x04\\x04\\x04\\x05\\x04\\x06\\x04\\x07\\x04\\n\\x02 \\x02 \\x02\\x03 \\x02\\x04 \\x02\\x05 \\x02\\x06 \\x02\\x07 \\x02\\x08 \\x02\\t \\x02\\n \\x02\\x02 \\x01 \\x02\\x01\\x03\\x02\\x03\\x05\\x04\\n\\x02\\x04\\x01\\x03\\x02\\x04\\x02\\x01\\x04\\x03\\n\\x05\\x01\\x05\\x02\\x05\\x03\\x05\\x04\\x05\\x05\\x05\\x06\\x05\\n\\x02 \\x02 \\x02\\x03 \\x02\\x04 \\x02\\x05 \\x02\\x06 \\x02\\x07 \\x02\\x08 \\x02\\t \\x02\\n \\x02\\x02 \\x01 \\x02\\x03\\x07\\x07\\x07\\t\\x05\\x06\\x01\\x02\\x04\\n\\x0b\\x06\\x05\\x08\\n\\x03\\x0c\\x0b\\x01\\x12\\n\\x03\\x0c\\x0b\\x08\\x10\\x0f\\x13\\n\\x05\\x07\\x07\\x06\\n\\x03\\x0c\\x0b\\x02\\x14\\x0e\\n\\x03\\x0c\\x0b\\x07\\x08\\t\\n\\x03\\x0c\\x0b\\x08\\x10\\x0f\\n\\x03\\x0c\\x0b\\x08\\x10\\x0f\\x11\\x0f\\n\\x03\\x08\\x04\\nFigure 6. Comparison of average accuracy on different communication epochs with counterparts on Digits and Ofﬁce Caltech tasks.\\nPlease see details in Sec. 4.3.\\nDigitsPrototypeMNIST USPS SVHN SYN AVG/triangle\\nG 98.30 92.44 80.35 53.58 81.16 -\\nU 98.23 93.12 81.18 55.40 81.98 +0.82\\nOfﬁce Caltech\\nCaltech Amazon Webcam DSLR AVG/triangle\\nG 64.02 79.37 50.17 40.33 58.47 -\\nU 64.26 79.54 48.39 44.67 59.21 +0.73\\nTable 3. Comparison of consistent regularization with global\\nprototypes ( Gin Eq. ( 3)) and unbiased prototypes ( Uin Eq. ( 6))\\non Digits and Ofﬁce Caltech tasks. See details in Sec. 4.2.2 .\\nE:5(90.3%) E:1 5 (96.9%) E:5 0 (97.7%) E:7 5 (97.9%)', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='E:5(38.0%) E:1 5 (76.03%) E:5 0 (78.8%) E:7 5 (80.0%)\\nFigure 7. t-SNE Visualization of FPL at different communica-\\ntion epoch on randomly participants from MNIST (Top Row) and\\nSVHN (Bottom Row). Please refer to Sec. 4.3for details.\\nwhich depicts that FPL is feasible to learn a generalizable\\ndecision boundary. We draw the the average accuracy met-ric in each communication epoch during training phase in\\nFig.6. We observe that FPL presents faster and stabler con-\\nvergence speed than other methods in these two tasks.\\n5. Conclusion\\nIn this paper, we explore the generalizability and stabil-\\nity problem under domain shift in heterogeneous federated\\nlearning. Our work introduces a simple yet effective fed-erated learning algorithm, Federated Prototypes Learning\\n(FPL). We leverage prototypes (class prototypical represen-\\ntation) to tackle these two problems by enjoying the com-plementary advantages of cluster prototypes and unbiasedprototypes: diverse domain knowledge and stable conver-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='gence signal. The effectiveness of FPL has been thoroughly\\nvalidated with many popular counterparts over various clas-siﬁcation tasks. We wish this work to pave the way for fu-\\nture research on heterogeneous federated learning.\\nAcknowledgement. This work is partially supported by\\nNational Natural Science Foundation of China under Grant(62176188, 62225113), the Key Research and Development\\nProgram of Hubei Province (2021BAA187), Zhejiang lab\\n(NO.2022NF0AB01), CCF-Huawei Populus Grove Fund\\n(CCF-HuaweiTC2022003), the Special Fund of Hubei Lu-\\nojia Laboratory (220100015) and the Science and Technol-\\nogy Major Project of Hubei Province (Next-Generation AI\\nTechnologies) under Grant (2019AEA170).\\n16319\\nReferences\\n[1] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew\\nMattina, Paul Whatmough, and Venkatesh Saligrama. Fed-erated learning based on dynamic regularization. In ICLR ,\\n2021. 2,6,8\\n[2] Mohammed H Alsharif, Abdullah Alrashoudi, Abdullah Al-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='abdulwahab, Saleh A Alshebeili, and Usman Tariq. Collabo-rative federated learning for healthcare: Multi-modal covid-19 diagnosis at the edge. IEEE ITJ , 2021. 3\\n[3] Anonymous. Soft neighbors are positive supporters in con-\\ntrastive visual representation learning. In ICLR , 2023. 3\\n[4] David Arthur and Sergei Vassilvitskii. k-means++: The ad-\\nvantages of careful seeding. In ACM-SIAM , 2006. 4\\n[5] Christopher Briggs, Zhong Fan, and Peter Andras. Federated\\nlearning with hierarchical clustering of local updates to im-prove training on non-iid data. In IJCNN , pages 1–9, 2020.\\n3\\n[6] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-\\notr Bojanowski, and Armand Joulin. Unsupervised learn-ing of visual features by contrasting cluster assignments. InNeurIPS , pages 9912–9924, 2020. 3\\n[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\\noffrey Hinton. A simple framework for contrastive learning\\nof visual representations. In ICML , pages 1597–1607, 2020.\\n2,3,7', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='2,3,7\\n[8] Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, An-\\ntonio Torralba, and Stefanie Jegelka. Debiased contrastivelearning. In NeurIPS , pages 8765–8775, 2020. 3\\n[9] Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and\\nReuven Y Rubinstein. A tutorial on the cross-entropymethod. Ann. Oper . Res. , pages 19–67, 2005. 6\\n[10] Moming Duan, Duo Liu, Xinyuan Ji, Renping Liu, Liang\\nLiang, Xianzhang Chen, and Yujuan Tan. Fedgroup: Ef-ﬁcient clustered federated learning via decomposed data-driven measure. In ISPA , 2021. 3\\n[11] Richard O Duda, Peter E Hart, et al. Pattern classiﬁcation\\nand scene analysis , volume 3. Wiley New York, 1973. 2\\n[12] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre\\nSermanet, and Andrew Zisserman. With a little help from\\nmy friends: Nearest-neighbor contrastive learning of visual\\nrepresentations. In ICCV , pages 9588–9597, 2021. 3\\n[13] Liang Gao, Huazhu Fu, Li Li, Yingwen Chen, Ming Xu, and', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Cheng-Zhong Xu. Feddc: Federated learning with non-iiddata via local drift decoupling and correction. In CVPR ,\\n2022. 1,2\\n[14] Songwei Ge, Shlok Mishra, Chun-Liang Li, Haohan Wang,\\nand David Jacobs. Robust contrastive learning using negativesamples with diminished semantics. In NeurIPS , 2021. 3\\n[15] Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ram-\\nchandran. An efﬁcient framework for clustered federatedlearning. In NeurIPS , pages 19586–19597, 2020. 3\\n[16] Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman.\\nGeodesic ﬂow kernel for unsupervised domain adaptation.InCVPR , pages 2066–2073, 2012. 2,6\\n[17] Gregory Grifﬁn, Alex Holub, and Pietro Perona. Caltech-256\\nobject category dataset. 2007. 6[18] Yuanfan Guo, Minghao Xu, Jiawen Li, Bingbing Ni, Xuanyu\\nZhu, Zhenbang Sun, and Yi Xu. Hcsc: Hierarchical con-\\ntrastive selective coding. In CVPR , pages 9706–9715, 2022.\\n3\\n[19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Girshick. Momentum contrast for unsupervised visual rep-resentation learning. In CVPR , pages 9729–9738, 2020. 2,\\n3\\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In CVPR ,\\npages 770–778, 2016. 6\\n[21] Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip\\nGibbons. The non-iid data quagmire of decentralized ma-chine learning. In ICML , pages 4387–4398, 2020.\\n[22] Wenke Huang, Mang Ye, and Bo Du. Learn from others and\\nbe yourself in heterogeneous federated learning. In CVPR ,\\n2022. 2,6\\n[23] Jonathan J. Hull. A database for handwritten text recognition\\nresearch. IEEE PAMI , pages 550–554, 1994. 2,6\\n[24] Tri Huynh, Simon Kornblith, Matthew R Walter, Michael\\nMaire, and Maryam Khademi. Boosting contrastive self-\\nsupervised learning with false negative cancellation. In\\nWACV , pages 2785–2795, 2022. 3\\n[25] Saumya Jetley, Bernardino Romera-Paredes, Sadeep Jaya-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='sumana, and Philip Torr. Prototypical priors: From im-proving classiﬁcation to zero-shot learning. arXiv preprint\\narXiv:1512.01192 , 2015. 3\\n[26] Peter Kairouz, H Brendan McMahan, Brendan Avent,\\nAur´elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista\\nBonawitz, Zachary Charles, Graham Cormode, Rachel Cum-mings, et al. Advances and open problems in federated learn-\\ning. arXiv preprint arXiv:1912.04977 , 2019. 1\\n[27] Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion,\\nPhilippe Weinzaepfel, and Diane Larlus. Hard negative\\nmixing for contrastive learning. In NeurIPS , pages 21798–\\n21809, 2020. 3\\n[28] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,\\nSashank J Reddi, Sebastian U Stich, and Ananda TheerthaSuresh. Scaffold: Stochastic controlled averaging for on-device federated learning. In ICML , 2020. 1,2\\n[29] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,\\nYonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Dilip Krishnan. Supervised contrastive learning. In NeurIPS ,\\n2020. 7\\n[30] Sungnyun Kim, Gihun Lee, Sangmin Bae, and Se-Young\\nYun. Mixco: Mix-up contrastive learning for visual repre-\\nsentation. In NeurIPS Workshop , 2020. 3\\n[31] Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and\\nHamed Pirsiavash. Mean shift for self-supervised learning.InCVPR , pages 10326–10335, 2021. 3\\n[32] Samuel Kotz, Narayanaswamy Balakrishnan, and Norman L\\nJohnson. Continuous multivariate distributions, V olume 1:\\nModels and applications , volume 1. John Wiley & Sons,\\n2004. 1\\n[33] Yann LeCun, L ´eon Bottou, Yoshua Bengio, and Patrick\\nHaffner. Gradient-based learning applied to document recog-nition. Proceedings of the IEEE\\n, pages 2278–2324, 1998. 2,\\n6\\n16320\\n[34] Kibok Lee, Yian Zhu, Kihyuk Sohn, Chun-Liang Li, Jinwoo\\nShin, and Honglak Lee. i-mix: A domain-agnostic strategy\\nfor contrastive representation learning. In ICLR , 2021. 3\\n[35] Gen Li, Varun Jampani, Laura Sevilla-Lara, Deqing Sun,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Jonghyun Kim, and Joongkyu Kim. Adaptive prototypelearning and allocation for few-shot segmentation. In CVPR ,\\npages 8334–8343, 2021. 3\\n[36] Junnan Li, Pan Zhou, Caiming Xiong, and Steven CH Hoi.\\nPrototypical contrastive learning of unsupervised representa-tions. In ICLR , 2021. 2,3\\n[37] Qinbin Li, Bingsheng He, and Dawn Song. Adversarial col-\\nlaborative learning on non-iid features. arXiv , 2021. 2\\n[38] Qinbin Li, Bingsheng He, and Dawn Song. Model-\\ncontrastive federated learning. In CVPR , pages 10713–\\n10722, 2021. 1,2,6,8\\n[39] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia\\nSmith. Federated learning: Challenges, methods, and futuredirections. IEEE SPM , pages 50–60, 2020. 1\\n[40] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar San-\\njabi, Ameet Talwalkar, and Virginia Smith. Federatedoptimization in heterogeneous networks. arXiv preprint\\narXiv:1812.06127 , 2020. 1,2,3,6,8\\n[41] Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and Qi Dou. Fed {bn}: Federated learning on non- {iid}fea-\\ntures via local batch normalization. In ICLR , 2021. 2\\n[42] Niklaus Liu, Zhaonan Liang, Junyang Lin, and Yang Liu.\\nPatient clustering improves efﬁciency of federated machine\\nlearning to predict mortality and hospital stay time using dis-tributed electronic medical records. Journal of biomedical\\ninformatics , 99:103291, 2019. 3\\n[43] Guodong Long, Ming Xie, Tao Shen, Tianyi Zhou, Xi-\\nanzhi Wang, and Jing Jiang. Multi-center federated learn-\\ning: clients clustering for better personalization. WWW ,\\n26(1):481–500, 2023. 3\\n[44] Mi Luo, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang, and\\nJiashi Feng. No fear of heterogeneity: Classiﬁer calibrationfor federated learning with non-iid data. In NeurIPS , 2021.\\n3\\n[45] James MacQueen et al. Some methods for classiﬁcation and\\nanalysis of multivariate observations. In BSMSP , pages 281–\\n297, 1967. 4\\n[46] Yishay Mansour, Mehryar Mohri, Jae Theertha Suresh', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Ro, and Ananda. Three approaches for personalization\\nwith applications to federated learning. arXiv preprint\\narXiv:2002.10619 , 2020. 3\\n[47] Brendan McMahan, Eider Moore, Daniel Ramage, Seth\\nHampson, and Blaise Aguera y Arcas. Communication-efﬁcient learning of deep networks from decentralized data.InAISTATS , pages 1273–1282, 2017. 1,2,3,6,8\\n[48] Pascal Mettes, Elise van der Pol, and Cees Snoek. Hyper-\\nspherical prototype networks. Advances in neural informa-\\ntion processing systems , 32, 2019. 3\\n[49] Umberto Michieli and Mete Ozay. Prototype guided feder-\\nated learning of visual feature representations. arXiv preprint\\narXiv:2105.08982 , 2021. 3\\n[50] Jovana Mitrovic, Brian McWilliams, Jacob Walker, Lars\\nBuesing, and Charles Blundell. Representation learning viainvariant causal mechanisms. In ICLR , 2021. 3[51] Xutong Mu, Yulong Shen, Ke Cheng, Xueli Geng, Jiaxuan\\nFu, Tao Zhang, and Zhiwei Zhang. Fedproc: Prototypicalcontrastive federated learning on non-iid data. arXiv preprint', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='arXiv:2109.12273 , 2021. 1,2,3,6,8\\n[52] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis-\\nsacco, Bo Wu, and Andrew Y Ng. Reading digits in natu-ral images with unsupervised feature learning. In NeurIPS\\nWorkshop , 2011. 2,6\\n[53] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\\nsentation learning with contrastive predictive coding. arXiv\\npreprint arXiv:1807.03748 , 2018. 3\\n[54] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\\nsentation learning with contrastive predictive coding. arXiv\\npreprint arXiv:1807.03748 , 2018. 3\\n[55] Sinno Jialin Pan and Qiang Yang. A survey on transfer learn-\\ning. IEEE TKDE , pages 1345–1359, 2009. 2\\n[56] Xiangyu Peng, Kai Wang, Zheng Zhu, Mang Wang, and\\nYang You. Crafting better contrastive views for siamese rep-\\nresentation learning. In CVPR , pages 16031–16040, 2022.\\n3\\n[57] Joaquin Qui ˜nonero-Candela, Masashi Sugiyama, Neil D\\nLawrence, and Anton Schwaighofer. Dataset Shift in Ma-\\nchine Learning . Mit Press, 2009. 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[58] Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary\\nGarrett, Keith Rush, Jakub Kone ˇcn`y, Sanjiv Kumar, and\\nH Brendan McMahan. Adaptive federated optimization. InICLR , 2021. 6,8\\n[59] Sucheng Ren, Huiyu Wang, Zhengqi Gao, Shengfeng He,\\nAlan Yuille, Yuyin Zhou, and Cihang Xie. A simple datamixing prior for improving self-supervised learning. InCVPR , pages 14595–14604, 2022. 3\\n[60] Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Ste-\\nfanie Jegelka. Contrastive learning with hard negative sam-ples. In ICLR\\n, 2021. 3\\n[61] Prasun Roy, Subhankar Ghosh, Saumik Bhattacharya, and\\nUmapada Pal. Effects of degradations on deep neural net-\\nwork architectures. arXiv preprint arXiv:1807.10108 , 2018.\\n2,6\\n[62] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell.\\nAdapting visual category models to new domains. In ECCV ,\\npages 213–226, 2010. 6\\n[63] M. Saquib Sarfraz, Vivek Sharma, and Rainer Stiefelhagen.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Efﬁcient parameter-free clustering using ﬁrst neighbor rela-tions. In CVPR , pages 8934–8943, 2019. 4\\n[64] Felix Sattler, Klaus-Robert M ¨uller, and Wojciech Samek.\\nClustered federated learning: Model-agnostic distributedmultitask optimization under privacy constraints. IEEE\\nTNNLS , 32(8):3710–3722, 2020. 3\\n[65] Zhiqiang Shen, Zechun Liu, Zhuang Liu, Marios Savvides,\\nTrevor Darrell, and Eric Xing. Un-mix: Rethinking imagemixtures for unsupervised visual representation learning. InAAAI , pages 2216–2224, 2022. 3\\n[66] Neta Shoham, Tomer Avidor, Aviv Keren, Nadav Israel,\\nDaniel Benditkis, Liron Mor-Yosef, and Itai Zeitak. Over-\\ncoming forgetting in federated learning on non-iid data. InNeurIPS Workshop , 2019. 1,2\\n[67] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical\\nnetworks for few-shot learning. In NeurIPS , 2017. 2,3\\n16321\\n[68] Canh T. Dinh, Nguyen Tran, and Josh Nguyen. Personal-\\nized federated learning with moreau envelopes. In NeurIPS ,\\npages 21394–21405, 2020. 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[69] Yue Tan, Yixin Liu, Guodong Long, Jing Jiang, Qinghua Lu,\\nand Chengqi Zhang. Federated learning on non-iid graphsvia structural knowledge sharing. In AAAI , 2023. 1\\n[70] Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu,\\nJing Jiang, and Chengqi Zhang. Fedproto: Federated proto-type learning across heterogeneous clients. In AAAI , 2022.\\n1,2,3,6,8\\n[71] Yue Tan, Guodong Long, Jie Ma, Lu Liu, Tianyi Zhou, and\\nJing Jiang. Federated learning from pre-trained models: A\\ncontrastive learning approach. In NeurIPS , 2022. 3\\n[72] Ajinkya Tejankar, Soroush Abbasi Koohpayegani, Vipin\\nPillai, Paolo Favaro, and Hamed Pirsiavash. Isd: Self-supervised learning by iterative similarity distillation. InICCV , 2021. 3\\n[73] Yonglong Tian, Olivier J Henaff, and A ¨aron van den Oord.\\nDivide and contrast: Self-supervised learning from uncu-\\nrated data. In ICCV , pages 10063–10074, 2021. 3\\n[74] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con-\\ntrastive multiview coding. In ECCV , pages 776–794.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Springer, 2020. 7\\n[75] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenen-\\nbaum, and Phillip Isola. Rethinking few-shot image classiﬁ-cation: a good embedding is all you need? In ECCV , 2020.\\n3\\n[76] Feng Wang and Huaping Liu. Understanding the behaviour\\nof contrastive loss. In CVPR , pages 2495–2504, 2021. 5,7\\n[77] Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou,\\nand Jiashi Feng. Panet: Few-shot image semantic segmenta-tion with prototype alignment. In CVPR , pages 9197–9206,\\n2019. 3\\n[78] Joe H Ward Jr. Hierarchical grouping to optimize an objec-\\ntive function. JASA , pages 236–244, 1963. 4\\n[79] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\\nUnsupervised feature learning via non-parametric instancediscrimination. In CVPR , pages 3733–3742, 2018. 2,3\\n[80] Chenhao Xu, Youyang Qu, Yong Xiang, and Longxiang Gao.\\nAsynchronous federated learning on heterogeneous devices:\\nA survey. arXiv preprint arXiv:2109.04269 , 2021. 3', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[81] Wenjia Xu, Yongqin Xian, Jiuniu Wang, Bernt Schiele, and\\nZeynep Akata. Attribute prototype network for zero-shot\\nlearning. In NeurIPS , pages 21969–21980, 2020. 3\\n[82] Hong-Ming Yang, Xu-Yao Zhang, Fei Yin, and Cheng-\\nLin Liu. Robust classiﬁcation with convolutional prototypelearning. In CVPR , pages 3474–3482, 2018. 2\\n[83] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.\\nFederated machine learning: Concept and applications. ACM\\nTIST , pages 1–19, 2019. 1\\n[84] Mang Ye, Jianbing Shen, Xu Zhang, Pong C Yuen, and Shih-\\nFu Chang. Augmentation invariant and instance spreading\\nfeature for softmax embedding. IEEE PAMI , 2020. 2,3\\n[85] Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang. Un-\\nsupervised embedding learning via invariant and spreadinginstance feature. In CVPR , pages 6210–6219, 2019. 2,3[86] Lin Zhang, Yong Luo, Yan Bai, Bo Du, and Ling-Yu Duan.\\nFederated learning for non-iid data via uniﬁed feature learn-\\ning and optimization objective alignment. In ICCV , pages', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='4420–4428, 2021. 2\\n[87] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon\\nCivin, and Vikas Chandra. Federated learning with non-iiddata. arXiv preprint arXiv:1806.00582 , 2018. 1\\n[88] Mingkai Zheng, Shan You, Fei Wang, Chen Qian, Chang-\\nshui Zhang, Xiaogang Wang, and Chang Xu. Ressl: Rela-tional self-supervised learning with weak augmentation. InNeurIPS , pages 2543–2555, 2021. 3\\n[89] Huasong Zhong, Jianlong Wu, Chong Chen, Jianqiang\\nHuang, Minghua Deng, Liqiang Nie, Zhouchen Lin, andXian-Sheng Hua. Graph contrastive clustering. In CVPR ,\\npages 9224–9233, 2021. 3\\n[90] Tianfei Zhou, Wenguan Wang, Ender Konukoglu, and Luc\\nVan Gool. Rethinking semantic segmentation: A prototypeview. In CVPR , 2022. 2,3\\n[91] Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, and Cheng-\\nLin Liu. Prototype augmentation and self-supervision forincremental learning. In CVPR , pages 5871–5880, 2021. 2\\n16322', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior\\nWenjing Bian Zirui Wang Kejie Li Jia-Wang Bian\\nVictor Adrian Prisacariu\\nActive Vision Lab, University of Oxford\\n{wenjing, ryan, kejie, jiawang, victor }@robots.ox.ac.uk\\nOurs BARF\\nNeRFmm SC-NeRF\\nFigure 1. Novel view synthesis comparison . We propose NoPe-NeRF for joint pose estimation and novel view synthesis. Our method\\nenables more robust pose estimation and renders better novel view synthesis than previous state-of-the-art methods.\\nAbstract\\nTraining a Neural Radiance Field (NeRF) without pre-\\ncomputed camera poses is challenging. Recent advances\\nin this direction demonstrate the possibility of jointly opti-\\nmising a NeRF and camera poses in forward-facing scenes.\\nHowever, these methods still face difficulties during dra-\\nmatic camera movement. We tackle this challenging prob-\\nlem by incorporating undistorted monocular depth priors.\\nThese priors are generated by correcting scale and shift', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='parameters during training, with which we are then able\\nto constrain the relative poses between consecutive frames.\\nThis constraint is achieved using our proposed novel loss\\nfunctions. Experiments on real-world indoor and outdoor\\nscenes show that our method can handle challenging cam-\\nera trajectories and outperforms existing methods in terms\\nof novel view rendering quality and pose estimation ac-\\ncuracy. Our project page is https://nope-nerf.\\nactive.vision .\\n1. Introduction\\nThe photo-realistic reconstruction of a scene from a\\nstream of RGB images requires both accurate 3D geometryreconstruction and view-dependent appearance modelling.\\nRecently, Neural Radiance Fields (NeRF) [24] have demon-\\nstrated the ability to build high-quality results for generating\\nphoto-realistic images from novel viewpoints given a sparse\\nset of images.\\nAn important preparation step for NeRF training is the\\nestimation of camera parameters for the input images. A', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='current go-to option is the popular Structure-from-Motion\\n(SfM) library COLMAP [35]. Whilst easy to use, this pre-\\nprocessing step could be an obstacle to NeRF research and\\nreal-world deployments in the long term due to its long pro-\\ncessing time and its lack of differentiability. Recent works\\nsuch as NeRFmm [46], BARF [18] and SC-NeRF [12] pro-\\npose to simultaneously optimise camera poses and the neu-\\nral implicit representation to address these issues. Neverthe-\\nless, these methods can only handle forward-facing scenes\\nwhen no initial parameters are supplied, and fail in dramatic\\ncamera motions, e.g.a casual handheld captured video.\\nThis limitation has two key causes. First, all these meth-\\nods estimate a camera pose for each input image individ-\\nually without considering relative poses between images.\\nLooking back to the literature of Simultaneous localisation\\nand mapping (SLAM) and visual odometry, pose estimation\\ncan significantly benefit from estimating relative poses be-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n4160\\ntween adjacent input frames. Second, the radiance field is\\nknown to suffer from shape-radiance ambiguity [55]. Esti-\\nmating camera parameters jointly with NeRF adds another\\ndegree of ambiguity, resulting in slow convergence and un-\\nstable optimisation.\\nTo handle the limitation of large camera motion, we seek\\nhelp from monocular depth estimation [22, 28, 29, 51]. Our\\nmotivation is threefold: First, monocular depth provides\\nstrong geometry cues that are beneficial to constraint shape-\\nradiance ambiguity. Second, relative poses between ad-\\njacent depth maps can be easily injected into the training\\npipeline via Chamfer Distance. Third, monocular depth is\\nlightweight to run and does not require camera parameters', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='as input, in contrast to multi-view stereo depth estimation.\\nFor simplicity, we use the term mono-depth from now on.\\nUtilising mono-depth effectively is not straightforward\\nwith the presence of scale and shift distortions. In other\\nwords, mono-depth maps are not multi-view consistent.\\nPrevious works [9, 17, 47] simply take mono-depth into\\na depth-wise loss along with NeRF training. Instead, we\\npropose a novel and effective way to thoroughly integrate\\nmono-depth into our system. First, we explicitly optimise\\nscale and shift parameters for each mono-depth map dur-\\ning NeRF training by penalising the difference between\\nrendered depth and mono-depth. Since NeRF by itself is\\ntrained based on multiview consistency, this step transforms\\nmono-depth maps to undistorted multiview consistent depth\\nmaps. We further leverage these multiview consistent depth\\nmaps in two loss terms: a) a Chamfer Distance loss between\\ntwo depth maps of adjacent images, which injects relative', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='pose to our system; and b) a depth-based surface rendering\\nloss, which further improves relative pose estimation.\\nIn summary, we propose a method to jointly optimise\\ncamera poses and a NeRF from a sequence of images with\\nlarge camera motion. Our system is enabled by three contri-\\nbutions. First , we propose a novel way to integrate mono-\\ndepth into unposed-NeRF training by explicitly modelling\\nscale and shift distortions. Second , we supply relative poses\\nto the camera-NeRF joint optimisation via an inter-frame\\nloss using undistorted mono-depth maps. Third , we further\\nregularise our relative pose estimation with a depth-based\\nsurface rendering loss.\\nAs a result, our method is able to handle large camera\\nmotion, and outperforms state-of-the-art methods by a sig-\\nnificant margin in terms of novel view synthesis quality and\\ncamera trajectory accuracy.\\n2. Related Work\\nNovel View Synthesis. While early Novel View Synthe-\\nsis (NVS) approaches applied interpolations between pix-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='els [3], later works often rendered images from 3D recon-\\nstruction [1, 6]. In recent years, different representations\\nof the 3D scene are used, e.g.meshes [30, 31], Multi-PlaneImages [41, 59], layered depth [42] etc. Among them,\\nNeRF [24] has become a popular scene representation for\\nits photorealistic rendering.\\nA number of techniques are proposed to improve NeRF’s\\nperformance with additional regularisation [13, 26, 54],\\ndepth priors [7,32,47,53], surface enhancements [27,43,49]\\nor latent codes [40, 44, 52]. Other works [2, 10, 25, 34] have\\nalso accelerated NeRF training and rendering. However,\\nmost of these approaches require pre-computed camera pa-\\nrameters obtained from SfM algorithms [11, 35].\\nNeRF With Pose Optimisation. Removing camera pa-\\nrameter preprocessing is an active line of research recently.\\nOne category of the methods [33, 38, 60] use a SLAM-style\\npipeline, that either requires RGB-D inputs or relies on ac-\\ncurate camera poses generated from the SLAM tracking', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='system. Another category of works optimises camera poses\\nwith the NeRF model directly. We term this type of method\\nasunposed-NeRF in this paper. iNeRF [50] shows that\\nposes for novel view images can be estimated using a recon-\\nstructed NeRF model. GNeRF [21] combines Generative\\nAdversarial Networks with NeRF to estimate camera poses\\nbut requires a known sampling distribution for poses. More\\nrelevant to our work, NeRFmm [46] jointly optimises both\\ncamera intrinsics and extrinsics alongside NeRF training.\\nBARF [18] proposes a coarse-to-fine positional encoding\\nstrategy for camera poses and NeRF joint optimisation. SC-\\nNeRF [12] further parameterises camera distortion and em-\\nploys a geometric loss to regularise rays. GARF [4] shows\\nthat using Gaussian-MLPs makes joint pose and scene opti-\\nmisation easier and more accurate. Recently, SiNeRF [48]\\nuses SIREN [36] layers and a novel sampling strategy to al-\\nleviate the sub-optimality of joint optimisation in NeRFmm.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Although showing promising results on the forward-facing\\ndataset like LLFF [23], these approaches face difficulties\\nwhen handling challenging camera trajectories with large\\ncamera motion. We address this issue by closely integrat-\\ning mono-depth maps with the joint optimisation of camera\\nparameters and NeRF.\\n3. Method\\nWe tackle the challenge of handling large camera mo-\\ntions in unposed-NeRF training. Given a sequence of im-\\nages, camera intrinsics, and their mono-depth estimations,\\nour method recovers camera poses and optimises a NeRF\\nsimultaneously. We assume camera intrinsics are available\\nin the image meta block, and we run an off-the-shelf mono-\\ndepth network DPT [7] to acquire mono-depth estimations.\\nWithout repeating the benefit of mono-depth, we unroll this\\nsection around the effective integration of monocular depth\\ninto unposed-NeRF training.\\nThe training is a joint optimisation of the NeRF, cam-\\nera poses, and distortion parameters of each mono-depth\\n4161', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='4161\\n \\nalign to globalKey Components Training Pipeline\\n : Point cloud from depth : Monocular depth map : Ground truth image\\n : Renderd image\\n : Renderd depth : Camera pose  \\n : Depth distortion - scale\\n : Depth distortion - shift : NeRF parameters\\nNeRF\\nNeRFLegend\\nBack-projectionLearnable\\nObjects\\nOperations : Undistorted depthFigure 2. Method Overview. Our method takes a sequence of images as input to reconstruct NeRF and jointly estimates the camera poses\\nof the frames. We first generate monocular depth maps from a mono-depth estimation network and reconstruct the point clouds. We then\\noptimise NeRF, camera poses, and depth distortion parameters jointly with inter-frame and NeRF losses.\\nmap. The distortion parameters are supervised by minimis-\\ning the discrepancies between the mono-depth maps and\\ndepth maps rendered from the NeRF, which are multiview\\nconsistent. The undistorted depth maps in return effec-\\ntively mediate the shape-radiance ambiguity, which eases', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the training of NeRF and camera poses.\\nSpecifically, the undistorted depth maps enable two con-\\nstraints. We constrain global pose estimation by sup-\\nplying relative pose between adjacent images. This is\\nachieved via a Chamfer-Distance-based correspondence be-\\ntween two point clouds, back-projected from undistorted\\ndepth maps. Further, we regularise relative pose estima-\\ntion with a surface-based photometric consistency where we\\ntreat undistorted depth as surface.\\nWe detail our method in the following sections, start-\\ning from NeRF in Sec. 3.1 and unposed-NeRF training in\\nSec. 3.2, looking into mono-depth distortions in Sec. 3.3,\\nfollowed by our mono-depth enabled loss terms in Sec. 3.4,\\nand finishing with an overall training pipeline Sec. 3.5.\\n3.1. NeRF\\nNeural Radiance Field (NeRF) [24] represents a scene as\\na mapping function FΘ: (x,d)→(c, σ)that maps a 3D lo-\\ncation x∈R3and a viewing direction d∈R3to a radiance\\ncolour c∈R3and a volume density value σ. This map-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ping is usually implemented with a neural network parame-\\nterised by FΘ. Given Nimages I={Ii|i= 0. . . N−1}\\nwith their camera poses Π ={πi|i= 0. . . N−1},\\nNeRF can be optimised by minimising photometric errorLrgb=PN\\ni∥Ii−ˆIi∥2\\n2between synthesised images ˆIand\\ncaptured images I:\\nΘ∗= arg min\\nΘLrgb(ˆI | I,Π), (1)\\nwhere ˆIiis rendered by aggregating radiance colour on\\ncamera rays r(h) =o+hdbetween near and far bound\\nhnandhf. More concretely, we synthesise ˆIiwith a volu-\\nmetric rendering function\\nˆIi(r) =Zhf\\nhnT(h)σ(r(h))c(r(h),d)dh, (2)\\nwhere T(h) = exp( −Rh\\nhnσ(r(s))ds)is the accumulated\\ntransmittance along a ray. We refer to [24] for further de-\\ntails.\\n3.2. Joint Optimisation of Poses and NeRF\\nPrior works [12, 18, 46] show that it is possible to esti-\\nmate both camera parameters and a NeRF at the same time\\nby minimising the above photometric error Lrgbunder the\\nsame volumetric rendering process in Eq. (2).\\nThe key lies in conditioning camera ray casting on vari-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='able camera parameters Π, as the camera ray ris a function\\nof camera pose. Mathematically, this joint optimisation can\\nbe formulated as:\\nΘ∗,Π∗= arg min\\nΘ,ΠLrgb(ˆI,ˆΠ| I), (3)\\nwhere ˆΠdenotes camera parameters that are updated during\\noptimising. Note that the only difference between Eq. (1)\\n4162\\nand Eq. (3) is that Eq. (3) considers camera parameters as\\nvariables.\\nIn general, the camera parameters Πinclude camera in-\\ntrinsics, poses, and lens distortions. We only consider es-\\ntimating camera poses in this work, e.g., camera pose for\\nframe Iiis a transformation Ti= [Ri|ti]with a rotation\\nRi∈SO(3)and a translation ti∈R3.\\n3.3. Undistortion of Monocular Depth\\nWith an off-the-shelf monocular depth network, e.g.,\\nDPT [28], we generate mono-depth sequence D=\\n{Di|i= 0. . . N−1}from input images. Without sur-\\nprise, mono-depth maps are not multi-view consistent so we\\naim to recover a sequence of multi-view consistent depth\\nmaps, which are further leveraged in our relative pose loss\\nterms.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='terms.\\nSpecifically, we consider two linear transformation pa-\\nrameters for each mono-depth map, resulting in a se-\\nquence of transformation parameters for all frames Ψ =\\n{(αi, βi)|i= 0. . . N−1}, where αiandβidenote a scale\\nand a shift factor. With multi-view consistent constraint\\nfrom NeRF, we aim to recover a multi-view consistent depth\\nmapD∗\\niforDi:\\nD∗\\ni=αiDi+βi, (4)\\nby joint optimising αiandβialong with a NeRF. This joint\\noptimisation is mostly achieved by enforcing the consis-\\ntency between an undistorted depth map D∗\\niand a NeRF\\nrendered depth map ˆDivia a depth loss:\\nLdepth =NX\\ni', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='D∗\\ni−ˆDi', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content=', (5)\\nwhere\\nˆDi(r) =Zhf\\nhnT(h)σ(r(h))dh (6)\\ndenotes a volumetric rendered depth map from NeRF.\\nIt is important to note that both NeRF and mono-depth\\nbenefit from Eq. (5). On the one hand, mono-depth provides\\nstrong geometry prior for NeRF training, reducing shape-\\nradiance ambiguity. On the other hand, NeRF provides\\nmulti-view consistency so we can recover a set of multi-\\nview consistent depth maps for relative pose estimations.\\n3.4. Relative Pose Constraint\\nAforementioned unposed-NeRF methods [12,18,46] op-\\ntimise each camera pose independently, resulting in an over-\\nfit to target images with incorrect poses. Penalising incor-\\nrect relative poses between frames can help to regularise the\\njoint optimisation towards smooth convergence, especially\\nin a complex camera trajectory. Therefore, we propose two\\nlosses that constrain relative poses.Point Cloud Loss. We back-project the undistorted depth\\nmapsD∗using the known camera intrinsics, to point clouds\\nP∗={P∗', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='P∗={P∗\\ni|i= 0. . . N−1}and optimise the relative\\npose between consecutive point clouds by minimising a\\npoint cloud loss Lpc:\\nLpc=X\\n(i,j)lcd(P∗\\nj,TjiP∗\\ni), (7)\\nwhere Tji=TjT−1\\nirepresents the related pose that trans-\\nforms point cloud P∗\\nitoP∗\\nj, tuple (i, j)denotes indices of a\\nconsecutive pair of instances, and lcddenotes Chamfer Dis-\\ntance:\\nlcd(Pi, Pj) =X\\npi∈Pimin\\npj∈Pj∥pi−pj∥2+X\\npj∈Pjmin\\npi∈Pi∥pi−pj∥2.\\n(8)\\nSurface-based Photometric Loss. While the point cloud\\nlossLpcoffers supervision in terms of 3D-3D matching, we\\nobserve that a surface-based photometric error can alleviate\\nincorrect matching. With the photometric consistency as-\\nsumption, this photometric error penalises the differences\\nin appearance between associated pixels. The association is\\nestablished by projecting the point cloud P∗\\nionto images Ii\\nandIj.\\nThe surface-based photometric loss can then be defined\\nas:\\nLrgb−s=X\\n(i,j)∥Ii⟨KiP∗\\ni⟩ −Ij⟨KjTjT−1\\niP∗\\ni⟩∥,(9)\\nwhere ⟨·⟩represents the sampling operation on the image', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='andKidenotes a projection matrix for ithcamera.\\n3.5. Overall Training Pipeline\\nAssembling all loss terms, we get the overall loss func-\\ntion:\\nL=Lrgb+λ1Ldepth +λ2Lpc+λ3Lrgb−s, (10)\\nwhere λ1, λ2, λ3are the weighting factors for respective\\nloss terms. By minimising the combined of loss L:\\nΘ∗,Π∗,Ψ∗= arg min\\nΘ,Π,ΨL(ˆI,ˆD,ˆΠ,ˆΨ| I,D), (11)\\nour method returns the optimised NeRF parameters Θ, cam-\\nera poses Π, and distortion parameters Ψ.\\n4. Experiments\\nWe begin with a description of our experimental setup in\\nSec. 4.1. In Sec. 4.2, we compare our method with pose-\\nunknown methods. Next, we compare our method with the\\nCOLMAP-assisted NeRF baseline in Sec. 4.3. Lastly, we\\nconduct ablation studies in Sec. 4.4.\\n4163\\n(a) BARF(b) NeRFmm(c) SC-NeRF(d) Ours(e) Ground Truth\\nFigure 3. Qualitative results of novel view synthesis and depth prediction on Tanks and Temples. We visualise the synthesised images', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and the rendered depth maps (top left of each image) for all methods. NoPe-NeRF is able to recover details for both colour and geometry.\\n4.1. Experimental Setup\\nDatasets. We conduct experiments on two datasets Tanks\\nand Temples [15] and ScanNet [5]. Tanks and Temples :\\nwe use 8 scenes to evaluate pose accuracy and novel view\\nsynthesis quality. We chose scenes captured at both indoor\\nand outdoor locations, with different frame sampling rates\\nand lengths. All images are down-sampled to a resolution\\nof960×540. For the family scene, we sample 200 images\\nand take 100 frames with odd frame ids as training images\\nand the remaining 100 frames for novel view synthesis, in\\norder to analyse the performance under smooth motion. For\\nthe remaining scenes, following NeRF [24], 1/8 of the im-\\nages in each sequence are held out for novel view synthesis,\\nunless otherwise specified. ScanNet : we select 4 scenes for\\nevaluating pose accuracy, depth accuracy, and novel view', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='synthesis quality. For each scene, we take 80-100 consec-\\nutive images and use 1/8 of these images for novel view\\nsynthesis. For evaluation, we employ depth maps and poses\\nprovided by ScanNet as ground truth. ScanNet images are\\ndown-sampled to 648×484. We crop images with dark\\norders during preprocessing.\\nMetrics. We evaluate our proposed method in three as-\\npects. For novel view synthesis , we follow previous meth-\\nods [12,18,46], and use standard evaluation metrics, includ-\\ning Peak Signal-to-Noise Ratio (PSNR), Structural Similar-\\nity Index Measure (SSIM) [45] and Learned Perceptual Im-\\nage Patch Similarity (LPIPS) [56]. For pose evaluation, We\\nuse standard visual odometry metrics [16, 37, 57], includ-\\ning the Absolute Trajectory Error (ATE) and Relative Pose\\nError (RPE). ATE measures the difference between the esti-\\nmated camera positions and the ground truth positions. RPE', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='measures the relative pose errors between pairs of images,which consists of relative rotation error (RPE r) and relative\\ntranslation error (RPE t). The estimated trajectory is aligned\\nwith the ground truth using Sim(3) with 7 degrees of free-\\ndom. We use standard depth metrics [8,19,20,39] (Abs Rel,\\nSq Rel, RMSE, RMSE log, δ1,δ2andδ3) for depth evalu-\\nation. For further detail, please refer to the supplementary\\nmaterial. To recover the metric scale, we follow Zhou et\\nal. [58] and match the median value between rendered and\\nground truth depth maps.\\nImplementation Details. Our model architecture is based\\non NeRF [24] with a few modifications: a) replacing\\nReLU activation function with Softplus and b) sampling\\n128 points along each ray uniformly with noise, between\\na predefined range (0.1,10). We use 2 separate Adam op-\\ntimisers [14] for NeRF and other parameters. The initial\\nlearning rate for NeRF is 0.001 and for the pose and dis-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tortion is 0.0005. Camera rotations are optimised in axis-\\nangle representation ϕi∈so(3). We first train the model\\nwith all losses with constant learning rates until the inter-\\nframe losses converge. Then, we remove the inter-frame\\nlosses and depth loss to refine the model with the RGB loss\\nonly. We decay the learning rates with different schedulers\\nto refine for 10,000 epochs. We balance the loss terms with\\nλ1= 0.04,λ2= 1.0andλ3= 1.0. For each training step,\\nwe randomly sample 1024 pixels (rays) from each input im-\\nage and 128 samples per ray. More details are provided in\\nthe supplementary material.\\n4.2. Comparing With Pose-Unknown Methods\\nWe compare our method with pose-unknown baselines,\\nincluding BARF [18], NeRFmm [46] and SC-NeRF [12].\\nView Synthesis Quality. To obtain the camera poses of\\n4164\\n(a) BARF (b) NeRFmm (d) Ours (c) SC-NeRFMuseum Ballroom', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Figure 4. Pose Estimation Comparison. We visualise the trajectory (3D plot) and relative rotation errors RPE r(bottom colour bar) of each\\nmethod on Ballroom andMuseum . The colour bar on the right shows the relative scaling of colour. More results are in the supplementary.\\nscenesOurs BARF NeRFmm SC-NeRF\\nPSNR ↑ SSIM ↑ LPIPS ↓ PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPSScanNet0079 00 32.47 0.84 0.41 32.31 0.83 0.43 30.59 0.81 0.49 31.33 0.82 0.46\\n0418 00 31.33 0.79 0.34 31.24 0.79 0.35 30.00 0.77 0.40 29.05 0.75 0.43\\n0301 00 29.83 0.77 0.36 29.31 0.76 0.38 27.84 0.72 0.45 29.45 0.77 0.35\\n0431 00 33.83 0.91 0.39 32.77 0.90 0.41 31.44 0.88 0.45 32.57 0.90 0.40\\nmean 31.86 0.83 0.38 31.41 0.82 0.39 29.97 0.80 0.45 30.60 0.81 0.41Tanks and TemplesChurch 25.17 0.73 0.39 23.17 0.62 0.52 21.64 0.58 0.54 21.96 0.60 0.53\\nBarn 26.35 0.69 0.44 25.28 0.64 0.48 23.21 0.61 0.53 23.26 0.62 0.51\\nMuseum 26.77 0.76 0.35 23.58 0.61 0.55 22.37 0.61 0.53 24.94 0.69 0.45', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Family 26.01 0.74 0.41 23.04 0.61 0.56 23.04 0.58 0.56 22.60 0.63 0.51\\nHorse 27.64 0.84 0.26 24.09 0.72 0.41 23.12 0.70 0.43 25.23 0.76 0.37\\nBallroom 25.33 0.72 0.38 20.66 0.50 0.60 20.03 0.48 0.57 22.64 0.61 0.48\\nFrancis 29.48 0.80 0.38 25.85 0.69 0.57 25.40 00.69 0.52 26.46 0.73 0.49\\nIgnatius 23.96 0.61 0.47 21.78 0.47 0.60 21.16 0.45 0.60 23.00 0.55 0.53\\nmean 26.34 0.74 0.39 23.42 0.61 0.54 22.50 0.59 0.54 23.76 0.65 0.48\\nTable 1. Novel view synthesis results on ScanNet and Tanks and Temples . Each baseline method is trained with its public code under\\nthe original settings and evaluated with the same evaluation protocol.\\ntest views for rendering, we minimise the photometric error\\nof the synthesised images while keeping the NeRF model\\nfixed, as in NeRFmm [46]. Each test pose is initialised with\\nthe learned pose of the training frame that is closest to it.\\nWe use the same pre-processing for all baseline approaches,\\nwhich results in higher accuracy than their original imple-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='mentations. More details are provided in the supplemen-\\ntary material. Our method outperforms all the baselines\\nby a large margin. The quantitative results are summarised\\nin Tab. 1, and qualitative results are shown in Fig. 3.\\nWe recognised that because the test views, which are\\nsampled from videos, are close to the training views, good\\nresults may be obtained due to overfitting to the training\\nimages. Therefore, we conduct an additional qualitativeevaluation on more novel views. Specifically, we fit a\\nbezier curve from the estimated training poses and sam-\\nple interpolated poses for each method to render novel view\\nvideos. Sampled results are shown in Fig. 5, and the ren-\\ndered videos are in the supplementary material. These re-\\nsults show that our method renders photo-realistic images\\nconsistently, while other methods generate visible artifacts.\\nCamera Pose. Our method significantly outperforms\\nother baselines in all metrics. The quantitative pose eval-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='uation results are shown in Tab. 2. For ScanNet, we use\\nthe camera poses provided by the dataset as ground truth.\\nFor Tanks and Temples, not every video comes with ground\\ntruth poses, so we use COLMAP estimations for reference.\\nOur estimated trajectory is better aligned with the ground\\n4165\\nscenesOurs BARF NeRFmm SC-NeRF\\nRPEt↓ RPEr↓ ATE↓ RPEt RPEr ATE RPE t RPEr ATE RPE t RPEr ATEScanNet0079 00 0.752 0.204 0.023 1.110 0.480 0.062 1.706 0.636 0.100 2.064 0.664 0.115\\n0418 00 0.455 0.119 0.015 1.398 0.538 0.020 1.402 0.460 0.013 1.528 0.502 0.016\\n0301 00 0.399 0.123 0.013 1.316 0.777 0.219 3.097 0.894 0.288 1.133 0.422 0.056\\n0431 00 1.625 0.274 0.069 6.024 0.754 0.168 6.799 0.624 0.496 4.110 0.499 0.205\\nmean 0.808 0.180 0.030 2.462 0.637 0.117 3.251 0.654 0.224 2.209 0.522 0.098Tanks and TemplesChurch 0.034 0.008 0.008 0.114 0.038 0.052 0.626 0.127 0.065 0.836 0.187 0.108\\nBarn 0.046 0.032 0.004 0.314 0.265 0.050 1.629 0.494 0.159 1.317 0.429 0.157', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Museum 0.207 0.202 0.020 3.442 1.128 0.263 4.134 1.051 0.346 8.339 1.491 0.316\\nFamily 0.047 0.015 0.001 1.371 0.591 0.115 2.743 0.537 0.120 1.171 0.499 0.142\\nHorse 0.179 0.017 0.003 1.333 0.394 0.014 1.349 0.434 0.018 1.366 0.438 0.019\\nBallroom 0.041 0.018 0.002 0.531 0.228 0.018 0.449 0.177 0.031 0.328 0.146 0.012\\nFrancis 0.057 0.009 0.005 1.321 0.558 0.082 1.647 0.618 0.207 1.233 0.483 0.192\\nIgnatius 0.026 0.005 0.002 0.736 0.324 0.029 1.302 0.379 0.041 0.533 0.240 0.085\\nmean 0.080 0.038 0.006 1.046 0.441 0.078 1.735 0.477 0.123 1.890 0.489 0.129\\nTable 2. Pose accuracy on ScanNet and Tanks and Temples . Note that we use COLMAP poses in Tanks and Temples as the “ground\\ntruth”. The unit of RPE ris in degrees, ATE is in the ground truth scale and RPE tis scaled by 100.\\nAbs Rel ↓Sq Rel ↓RMSE ↓RMSE log ↓δ1↑δ2↑δ3↑\\nOurs 0.141 0.137 0.568 0.176 0.828 0.970 0.987\\nBARF 0.376 0.684 0.990 0.401 0.490 0.751 0.884\\nNeRFmm 0.590 1.721 1.672 0.587 0.316 0.560 0.743', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='SC-NeRF 0.417 0.642 1.079 0.476 0.362 0.658 0.832\\nDPT 0.197 0.246 0.751 0.226 0.747 0.934 0.975\\nTable 3. Depth map evaluation on ScanNet . Our depth\\nestimation is more accurate than baseline models BARF [18],\\nNeRFmm [46] and SC-NeRF [12]. Compared with DPT [58], we\\nshow our depth is more accurate after undistortion.\\ntruth than other methods, and our estimated rotation is two\\norders of magnitudes more accurate than others. We visu-\\nalise the camera trajectories and rotations in Fig. 4.\\nDepth. We evaluate the accuracy of the rendered depth\\nmaps on ScanNet, which provides the ground-truth depths\\nfor evaluation. Our rendered depth maps achieve superior\\naccuracy over the previous alternatives. We also compare\\nwith the mono-depth maps estimated by DPT. Our rendered\\ndepth maps, after undistortion using multiview consistency\\nin the NeRF optimisation, outperform DPT by a large mar-\\ngin. The results are summarised in Tab. 3, and sampled\\nqualitative results are illustrated in Fig. 3.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='4.3. Comparing With COLMAP Assisted NeRF\\nWe make a comparison of pose estimation accuracy be-\\ntween our method and COLMAP against ground truth poses\\nin ScanNet. We achieve on-par accuracy with COLMAP, as\\nshown in Tab. 4. We further analyse the novel view syn-\\nthesis quality of the NeRF model trained with our learned\\nposes to COLMAP poses on ScanNet and Tanks and Tem-\\nples. The original NeRF training contains two stages, find-\\ning poses using COLMAP and optimising the scene repre-\\nsentation. In order to make our comparison fairer, in this\\nsection only, we mimic a similar two-stage training as the\\noriginal NeRF [24]. In the first stage, we train our method\\nwith all losses for camera pose estimation, i.e., mimicking\\nthe COLMAP processing. Then, we fix the optimised poses\\nOurs\\nBARF\\nNeRFmm\\nSC-NeRF1 438 261\\n(b) BARF (c) NeRFmm (a) Ours (d) SC-NeRF\\nView 1 View 2\\n View 3\\nFigure 5. Sampled frames from rendered novel view videos.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='For each method, we fit the learned trajectory with a bezier curve\\nand uniformly sample new viewpoints for rendering. Our method\\ngenerates significantly better results than previous methods, which\\nshow visible artifacts. The full rendered videos and details about\\ngenerating novel views are provided in the supplementary.\\nand train a NeRF model from scratch, using the same set-\\ntings and loss as the original NeRF. This evaluation enables\\nus to compare our estimated poses to the COLMAP poses\\nindirectly, i.e., in terms of contribution to view synthesis.\\nOur two-stage method outperforms the COLMAP-\\nassisted NeRF baseline, which indicates a better pose es-\\ntimation for novel view synthesis. The results are sum-\\nmarised in Tab. 5.\\nAs is commonly known, COLMAP performs poorly in\\nlow-texture scenes and sometimes fails to find accurate\\ncamera poses. Fig. 6 shows an example of a low-texture\\nscene where COLMAP provides inaccurate pose estimation', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='that causes NeRF to render images with visible artifacts. In\\ncontrast, our method renders high-quality images, thanks to\\nrobust optimisation of camera pose.\\n4166\\nscenesOurs COLMAP\\nRPEt↓RPEr↓ATE↓ RPEtRPErATE\\n0079 00 0.752 0.204 0.023 0.655 0.221 0.012\\n0418 00 0.455 0.119 0.015 0.491 0.124 0.016\\n0301 00 0.399 0.123 0.013 0.414 0.136 0.009\\n0431 00 1.625 0.274 0.069 1.292 0.249 0.051\\nmean 0.808 0.180 0.030 0.713 0.182 0.022\\nTable 4. Comparison of pose accuracy with COLMAP on\\nScanNet.\\n(a) COLMAP+NeRF (b) Ours\\n (c) Ground T ruth\\nFigure 6. COLMAP failure case. On a rotation-dominant se-\\nquence with low-texture areas, COLMAP fails to estimate correct\\nposes, which results in artifacts in synthesised images.\\nscenesOurs Ours-r COLMAP+NeRF\\nPSNR ↑SSIM↑LPIPS ↓PSNR SSIM LPIPS PSNR SSIM LPIPSScanNet0079 00 32.47 0.84 0.41 33.12 0.85 0.40 31.98 0.83 0.43\\n0418 00 31.33 0.79 0.34 30.49 0.77 0.40 30.60 0.78 0.40\\n0301 00 29.83 0.77 0.36 30.05 0.78 0.34 30.01 0.78 0.36', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='0431 00 33.83 0.91 0.39 33.86 0.91 0.39 33.54 0.91 0.39\\nmean 31.86 0.83 0.38 31.88 0.83 0.38 31.53 0.82 0.40Tanks and TemplesChurch 25.17 0.73 0.39 26.74 0.78 0.32 25.72 0.75 0.37\\nBarn 26.35 0.69 0.44 26.58 0.71 0.42 26.72 0.71 0.42\\nMuseum 26.77 0.76 0.35 26.98 0.77 0.36 27.21 0.78 0.34\\nFamily 26.01 0.74 0.41 26.21 0.75 0.40 26.61 0.77 0.39\\nHorse 27.64 0.84 0.26 28.06 0.84 0.26 27.02 0.82 0.29\\nBallroom 25.33 0.72 0.38 25.53 0.73 0.38 25.47 0.73 0.38\\nFrancis 29.48 0.80 0.38 29.73 0.81 0.38 30.05 0.81 0.38\\nIgnatius 23.96 0.61 0.47 23.98 0.62 0.46 24.08 0.61 0.47\\nmean 26.34 0.74 0.39 26.73 0.75 0.37 26.61 0.75 0.38\\nTable 5. Comparison to NeRF with COLMAP poses. Our two-\\nstage method (Ours-r) outperforms both COLMAP+NeRF and our\\none-stage method (Ours).\\nInterestingly, this experiment also reveals that the two-\\nstage method shows higher accuracy than the one-stage\\nmethod. We hypothesise that the joint optimisation (from\\nrandomly initialised poses) in the one-stage approach', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='causes the NeRF optimisation to be trapped in a local min-\\nimum, potentially due to the bad pose initialisation. The\\ntwo-stage approach circumvents this issue by re-initialising\\nthe NeRF and re-training with well-optimised poses, result-\\ning in higher performance.\\n4.4. Ablation Study\\nIn this section, we analyse the effectiveness of the param-\\neters and components that have been added to our model.\\nThe results of ablation studies are shown in Tab. 6.\\nEffect of Distortion Parameters. We find that ignoring\\ndepth distortions ( i.e., setting scales to 1 and shifts to 0 as\\nconstants) leads to a degradation in pose accuracy, as in-NVS Pose\\nPSNR↑SSIM↑LPIPS ↓RPEt↓RPEr↓ATE↓\\nOurs 31.86 0.83 0.38 0.801 0.181 0.031\\nOurs w/o α, β 31.46 0.82 0.39 1.929 0.321 0.066\\nOurs w/o Lpc 31.73 0.82 0.38 2.227 0.453 0.101\\nOurs w/o Lrgb−s31.05 0.81 0.41 1.814 0.401 0.156\\nOurs w/o Ldepth 31.20 0.81 0.40 1.498 0.383 0.089\\nTable 6. Ablation study results on ScanNet.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='consistent distortions of depth maps introduce errors to the\\nestimation of relative poses and confuse NeRF for geometry\\nreconstruction.\\nEffect of Inter-frame Losses. We observe that the inter-\\nframe losses are the major contributor to improving relative\\nposes. When removing the pairwise point cloud loss Lpc\\nor the surface-based photometric loss Lrgb−s, there is less\\nconstraint between frames, and thus the pose accuracy be-\\ncomes lower.\\nEffect of NeRF Losses. When the depth loss Ldepth is\\nremoved, the distortions of input depth maps are only opti-\\nmised locally through the inter-frame losses. We find that\\nthis can lead to drift and degradation in pose accuracy.\\n4.5. Limitations\\nOur proposed method optimises camera pose and the\\nNeRF model jointly and works on challenging scenes where\\nother baselines fail. However, the optimisation of the model\\nis also affected by non-linear distortions and the accuracy of\\nthe mono-depth estimation, which we did not consider.\\n5. Conclusion', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='5. Conclusion\\nIn this work, we present NoPe-NeRF, an end-to-end\\ndifferentiable model for joint camera pose estimation and\\nnovel view synthesis from a sequence of images. We\\ndemonstrate that previous approaches have difficulty with\\ncomplex trajectories. To tackle this challenge, we use\\nmono-depth maps to constrain the relative poses between\\nframes and regularise the geometry of NeRF, which leads\\nto better pose estimation. We show the effectiveness and\\nrobustness of NoPe-NeRF on challenging scenes. The im-\\nproved pose estimation leads to better novel view synthesis\\nquality and geometry reconstruction compared with other\\napproaches. We believe our method is an important step\\ntowards applying the unknown-pose NeRF models to large-\\nscale scenes in the future.\\nAcknowledgements\\nWe thank Theo Costain, Michael Hobley, Shuai Chen\\nand Xinghui Li for their helpful proofreading and dis-\\ncussions. Wenjing Bian is supported by the China\\nScholarship Council - University of Oxford Scholar-\\nship.\\n4167', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ship.\\n4167\\nReferences\\n[1] Chris Buehler, Michael Bosse, Leonard McMillan, Steven\\nGortler, and Michael Cohen. Unstructured lumigraph ren-\\ndering. In SIGGRAPH , 2001. 2\\n[2] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\\nHao Su. Tensorf: Tensorial radiance fields. In ECCV , 2022.\\n2\\n[3] Shenchang Eric Chen and Lance Williams. View interpola-\\ntion for image synthesis. In SIGGRAPH , 1993. 2\\n[4] Shin-Fang Chng, Sameera Ramasinghe, Jamie Sherrah, and\\nSimon Lucey. Garf: Gaussian activated radiance fields for\\nhigh fidelity reconstruction and pose estimation. arXiv e-\\nprints , 2022. 2\\n[5] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\\nber, Thomas Funkhouser, and Matthias Nießner. Scannet:\\nRichly-annotated 3d reconstructions of indoor scenes. In\\nCVPR , 2017. 5\\n[6] Paul E Debevec, Camillo J Taylor, and Jitendra Malik. Mod-\\neling and rendering architecture from photographs: A hybrid\\ngeometry-and image-based approach. In SIGGRAPH , 1996.\\n2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='2\\n[7] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra-\\nmanan. Depth-supervised nerf: Fewer views and faster train-\\ning for free. In CVPR , 2022. 2\\n[8] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-\\nmanghelich, and Dacheng Tao. Deep ordinal regression net-\\nwork for monocular depth estimation. In CVPR , 2018. 5\\n[9] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang.\\nDynamic view synthesis from dynamic monocular video. In\\nICCV , 2021. 2\\n[10] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie\\nShotton, and Julien Valentin. Fastnerf: High-fidelity neural\\nrendering at 200fps. In ICCV , 2021. 2\\n[11] Richard Hartley and Andrew Zisserman. Multiple view ge-\\nometry in computer vision . 2003. 2\\n[12] Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Anima\\nAnandkumar, Minsu Cho, and Jaesik Park. Self-calibrating\\nneural radiance fields. In ICCV , 2021. 1, 2, 3, 4, 5, 7\\n[13] Mijeong Kim, Seonguk Seo, and Bohyung Han. Infonerf:\\nRay entropy minimization for few-shot neural volume ren-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='dering. In CVPR , 2022. 2\\n[14] Diederik P. Kingma and Jimmy Ba. Adam: A method for\\nstochastic optimization. In Yoshua Bengio and Yann LeCun,\\neditors, ICLR , 2015. 5\\n[15] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen\\nKoltun. Tanks and temples: Benchmarking large-scale scene\\nreconstruction. ACM Transactions on Graphics , 2017. 5\\n[16] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust\\nconsistent video depth estimation. In CVPR , 2021. 5\\n[17] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\\nNeural scene flow fields for space-time view synthesis of dy-\\nnamic scenes. In CVPR , 2021. 2\\n[18] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si-\\nmon Lucey. Barf: Bundle-adjusting neural radiance fields.\\nInICCV , 2021. 1, 2, 3, 4, 5, 7[19] Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid.\\nLearning depth from single monocular images using deep\\nconvolutional neural fields. TPAMI , 2015. 5\\n[20] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and Johannes Kopf. Consistent video depth estimation. ToG.\\n5\\n[21] Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su,\\nLan Xu, Xuming He, and Jingyi Yu. Gnerf: Gan-based\\nneural radiance field without posed camera. In Proceedings\\nof the IEEE/CVF International Conference on Computer Vi-\\nsion, 2021. 2\\n[22] S Mahdi H Miangoleh, Sebastian Dille, Long Mai, Sylvain\\nParis, and Yagiz Aksoy. Boosting monocular depth estima-\\ntion models to high-resolution via content-adaptive multi-\\nresolution merging. In CVPR , 2021. 2\\n[23] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,\\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\\nAbhishek Kar. Local light field fusion: Practical view syn-\\nthesis with prescriptive sampling guidelines. ACM Transac-\\ntions on Graphics (TOG) , 2019. 2\\n[24] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\\nRepresenting scenes as neural radiance fields for view syn-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='thesis. Communications of the ACM , 2021. 1, 2, 3, 5, 7\\n[25] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-\\nder Keller. Instant neural graphics primitives with a multires-\\nolution hash encoding. ACM Trans. Graph. 2\\n[26] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall,\\nMehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Reg-\\nnerf: Regularizing neural radiance fields for view synthesis\\nfrom sparse inputs. In CVPR , 2022. 2\\n[27] Michael Oechsle, Songyou Peng, and Andreas Geiger.\\nUnisurf: Unifying neural implicit surfaces and radiance\\nfields for multi-view reconstruction. In ICCV , 2021. 2\\n[28] Ren ´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\\nsion transformers for dense prediction. In ICCV , 2021. 2,\\n4\\n[29] Ren ´e Ranftl, Katrin Lasinger, David Hafner, Konrad\\nSchindler, and Vladlen Koltun. Towards robust monocular\\ndepth estimation: Mixing datasets for zero-shot cross-dataset\\ntransfer. TPAMI , 2020. 2\\n[30] Gernot Riegler and Vladlen Koltun. Free view synthesis. In', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ECCV , 2020. 2\\n[31] Gernot Riegler and Vladlen Koltun. Stable view synthesis.\\nInCVPR , 2021. 2\\n[32] Barbara Roessle, Jonathan T Barron, Ben Mildenhall,\\nPratul P Srinivasan, and Matthias Nießner. Dense depth pri-\\nors for neural radiance fields from sparse input views. In\\nCVPR , 2022. 2\\n[33] Antoni Rosinol, John J Leonard, and Luca Carlone. Nerf-\\nslam: Real-time dense monocular slam with neural radiance\\nfields. arXiv preprint arXiv:2210.13641 , 2022. 2\\n[34] Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong\\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\\nRadiance fields without neural networks. In CVPR , 2022. 2\\n[35] Johannes L Schonberger and Jan-Michael Frahm. Structure-\\nfrom-motion revisited. In CVPR , 2016. 1, 2\\n4168\\n[36] Vincent Sitzmann, Julien Martel, Alexander Bergman, David\\nLindell, and Gordon Wetzstein. Implicit neural representa-\\ntions with periodic activation functions. NeurIPS , 2020. 2\\n[37] J ¨urgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Burgard, and Daniel Cremers. A benchmark for the eval-\\nuation of rgb-d slam systems. In IROS . IEEE, 2012. 5\\n[38] Edgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew Davi-\\nson. iMAP: Implicit mapping and positioning in real-time.\\nInICCV , 2021. 2\\n[39] Libo Sun, Jia-Wang Bian, Huangying Zhan, Wei Yin,\\nIan Reid, and Chunhua Shen. Sc-depthv3: Robust self-\\nsupervised monocular depth estimation for dynamic scenes.\\narXiv preprint arXiv:2211.03660 , 2022. 5\\n[40] Alex Trevithick and Bo Yang. Grf: Learning a general ra-\\ndiance field for 3d representation and rendering. In ICCV ,\\n2021. 2\\n[41] Richard Tucker and Noah Snavely. Single-view view syn-\\nthesis with multiplane images. In CVPR , 2020. 2\\n[42] Shubham Tulsiani, Richard Tucker, and Noah Snavely.\\nLayer-structured 3d scene inference via view synthesis. In\\nECCV , 2018. 2\\n[43] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\\nKomura, and Wenping Wang. Neus: Learning neural implicit', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='surfaces by volume rendering for multi-view reconstruction.\\nNeurIPS , 2021. 2\\n[44] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P\\nSrinivasan, Howard Zhou, Jonathan T Barron, Ricardo\\nMartin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibr-\\nnet: Learning multi-view image-based rendering. In CVPR ,\\n2021. 2\\n[45] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-\\nmoncelli. Image quality assessment: from error visibility to\\nstructural similarity. IEEE transactions on image processing ,\\n2004. 5\\n[46] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and\\nVictor Adrian Prisacariu. NeRF −−: Neural radiance\\nfields without known camera parameters. arXiv preprint\\narXiv:2102.07064 , 2021. 1, 2, 3, 4, 5, 6, 7\\n[47] Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu,\\nand Jie Zhou. Nerfingmvs: Guided optimization of neural\\nradiance fields for indoor multi-view stereo. In ICCV , 2021.\\n2\\n[48] Yitong Xia, Hao Tang, Radu Timofte, and Luc Van Gool.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Sinerf: Sinusoidal neural radiance fields for joint pose esti-\\nmation and scene reconstruction. 2022. 2\\n[49] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. V ol-\\nume rendering of neural implicit surfaces. In NeurIPS , 2021.\\n2\\n[50] Lin Yen-Chen, Pete Florence, Jonathan T Barron, Alberto\\nRodriguez, Phillip Isola, and Tsung-Yi Lin. inerf: Inverting\\nneural radiance fields for pose estimation. In IROS , 2021. 2\\n[51] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Si-\\nmon Chen, Yifan Liu, and Chunhua Shen. Towards accurate\\nreconstruction of 3d scene shape from a single monocular\\nimage. TPAMI , 2022. 2\\n[52] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\\npixelnerf: Neural radiance fields from one or few images. In\\nCVPR , 2021. 2[53] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sat-\\ntler, and Andreas Geiger. Monosdf: Exploring monocu-\\nlar geometric cues for neural implicit surface reconstruction.\\nNeurIPS , 2022. 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='NeurIPS , 2022. 2\\n[54] Jian Zhang, Yuanqing Zhang, Huan Fu, Xiaowei Zhou,\\nBowen Cai, Jinchi Huang, Rongfei Jia, Binqiang Zhao, and\\nXing Tang. Ray priors through reprojection: Improving neu-\\nral radiance fields for novel view extrapolation. In CVPR ,\\n2022. 2\\n[55] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\\nKoltun. Nerf++: Analyzing and improving neural radiance\\nfields. arXiv preprint arXiv:2010.07492 , 2020. 2\\n[56] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\\nand Oliver Wang. The unreasonable effectiveness of deep\\nfeatures as a perceptual metric. In CVPR , 2018. 5\\n[57] Zichao Zhang and Davide Scaramuzza. A tutorial on quanti-\\ntative trajectory evaluation for visual (-inertial) odometry. In\\nIROS . IEEE, 2018. 5\\n[58] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G\\nLowe. Unsupervised learning of depth and ego-motion from\\nvideo. In CVPR , 2017. 5, 7\\n[59] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and Noah Snavely. Stereo magnification: Learning view syn-\\nthesis using multiplane images. 2018. 2\\n[60] Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hu-\\njun Bao, Zhaopeng Cui, Martin R. Oswald, and Marc Polle-\\nfeys. Nice-slam: Neural implicit scalable encoding for slam.\\nInCVPR , 2022. 2\\n4169', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='HGFormer: Hierarchical Grouping Transformer for Domain Generalized\\nSemantic Segmentation\\nJian Ding1,2,3, Nan Xue1, Gui-Song Xia1,2*, Bernt Schiele3, Dengxin Dai3\\n1NERCMS, School of Computer Science, Wuhan University, China\\n2State Key Lab. LIESMARS, Wuhan University, China\\n3Max Planck Institute for Informatics, Saarland Informatics Campus, Germany\\n{jian.ding, xuenan, guisong.xia }@whu.edu.cn, {schiele, ddai }@mpi-inf.mpg.de\\nAbstract\\nCurrent semantic segmentation models have achieved\\ngreat success under the independent and identically dis-\\ntributed (i.i.d.) condition. However, in real-world appli-\\ncations, test data might come from a different domain than\\ntraining data. Therefore, it is important to improve model\\nrobustness against domain differences. This work stud-\\nies semantic segmentation under the domain generalization\\nsetting, where a model is trained only on the source domain\\nand tested on the unseen target domain. Existing works', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='show that Vision Transformers are more robust than CNNs\\nand show that this is related to the visual grouping property\\nof self-attention. In this work, we propose a novel hierarchi-\\ncal grouping transformer (HGFormer) to explicitly group\\npixels to form part-level masks and then whole-level masks.\\nThe masks at different scales aim to segment out both parts\\nand a whole of classes. HGFormer combines mask clas-\\nsiﬁcation results at both scales for class label prediction.\\nWe assemble multiple interesting cross-domain settings by\\nusing seven public semantic segmentation datasets. Exper-\\niments show that HGFormer yields more robust semantic\\nsegmentation results than per-pixel classiﬁcation methods\\nand ﬂat-grouping transformers, and outperforms previous\\nmethods signiﬁcantly. Code will be available at https:\\n//github.com/dingjiansw101/HGFormer .\\n1. Introduction\\nResearch in semantic image segmentation has leaped for-\\nward in the past years due to the development of deep neural', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='network. However, most of these models assume that the\\ntraining and testing data follow the same distribution. In the\\nreal-world, we frequently encounter testing data that is out\\nof distribution. The generalization ability of models under\\ndistribution shift is crucial for applications related to safety,\\n*Corresponding author\\nWith Gaussian Noise\\nPart-level masks (Ours) Whole-level masks (Ours)Whole-level masks (Mask2former) Semantic results (Mask2former)\\nSemantic results (Ours)\\nWith Snow\\nPart-level masks (Ours) Whole-level masks (Ours)Whole-level masks (Mask2former) Semantic results (Mask2former)\\nSemantic results (Ours)Figure 1. Semantic segmentation can be considered as partition-\\ning an image into classiﬁcation units (regions), then classifying the\\nunits. The units can range from pixels to large masks. Intuitively,\\nmask classiﬁcation is more robust than per-pixel classiﬁcation, as\\nmasks allow to aggregate features over large image regions of the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='same class to predict a ‘global’ label. Despite this promise, the\\nprocess of grouping pixels into whole-level masks directly from\\npixels is very challenging under the distribution shift ( e.g., Gaus-\\nsian Noise). In order to tackle this problem, we present a hierar-\\nchical grouping paradigm to group pixels to part-level masks ﬁrst\\nand then to group part-level masks towhole-level masks to get re-\\nliable masks. Then we combine both part-level and whole-level\\nmask classiﬁcation for robust semantic segmentation, given that\\nthe masks at the two levels capture complementary information.\\nsuch as self-driving. In domain generalization setting, mod-\\nels are trained only on source domains and tested on tar-\\nget domains, where the distributions of source domains and\\ntarget domains are different. Unlike the domain adapta-\\ntion [ 25,56], target data is not accessible / needed during\\ntraining, making the task challenging but practically useful.\\nRecently, Vision Transformers have been shown to be', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='signiﬁcantly more robust than traditional CNNs in the out-\\nof-distribution generalization [ 21,25,42,58,60,70]. Some\\nworks interpret self-attention as a kind of visual group-\\n1\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n15413\\ning[7,38], and believe that it is related to robustness [70].\\nHowever, these works mainly focus on classiﬁcation. Al-\\nthough FAN [ 70] and Segformer [ 60] have been evaluated\\non segmentation, they do not explicitly introduce visual\\ngrouping in their networks. Since grouping is naturally\\naligned with the task of semantic segmentation, we would\\nlike to ask the question: can we improve the robustness of\\nsemantic segmentation by introducing an explicit grouping\\nmechanism into semantic segmentation networks?\\nMost deep learning based segmentation models directly', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='conduct per-pixel classiﬁcation without the process of\\ngrouping. Some recent segmentation models introduced ﬂat\\ngrouping [15,67] into the segmentation decoder, where pix-\\nels are grouped into a set of binary masks directly and clas-\\nsiﬁcation on masks is used to make label prediction. By us-\\ning a one-to-one matching similar to DETR [ 5], the loss be-\\ntween predicted masks and ground truth masks is computed.\\nTherefore the network is trained to directly predict whole-\\nlevel masks , as shown in Fig. 1. Intuitively, if whole-level\\nmasks are accurate, mask classiﬁcation will be more robust\\nthan per-pixel classiﬁcation due to its information aggrega-\\ntion over regions of the same class. But we ﬁnd that using\\ntheﬂat grouping to generate whole-level masks is suscepti-\\nble to errors, especially under cross-domain settings. This\\nis shown by the example in Fig. 1- bottom.\\nDifferent from the ﬂat grouping works [ 14,67], we pro-\\npose a hierarchical grouping in the segmentation decoder,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='where the pixels are ﬁrst grouped into part-level masks , and\\nthen grouped into whole-level masks . Actually, the hierar-\\nchical grouping is inspired by the pioneer works of image\\nsegmentation [ 2,13,49] and is further supported by strong\\npsychological evidence that humans parse scenes into part-\\nwhole hierarchies [ 24]. We ﬁnd that grouping pixels to part-\\nlevel masks and then to whole-level masks is more robust\\nthan grouping pixels directly to whole-level masks. Part-\\nlevel masks and whole-level masks segment images at dif-\\nferent scales such as parts anda whole of classes. There-\\nfore, part-level and whole-level masks are complementary,\\nand combining mask classiﬁcation results at those different\\nscales improves the overall robustness.\\nTo instantiate a hierarchical grouping idea, we propose\\na hierarchical grouping transformer (HGFormer) in the de-\\ncoder of a segmentation model. The diagram is shown in\\nFig. 2. We ﬁrst send the feature maps to the part-level', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='grouping module. In the part-level grouping module, the\\ninitialization of cluster centers is down sampled from fea-\\nture maps. Then we compute the pixel-center similarities\\nand assign pixels to cluster centers according to the simi-\\nlarities. To get the part-level masks, we only compute the\\nsimilarities between each pixel feature and its nearby cen-\\nter features. We then aggregate information of the part-\\nlevel masks and generate whole-level masks by using cross-\\nattention, similar to how previous methods aggregate pixelsinformation to generate whole-level masks [ 14,67]. Finally,\\nwe classify masks at different levels, and average the se-\\nmantic segmentation results of all the scales.\\nWe evaluate the method under multiple settings, which\\nare assembled by using seven challenging semantic seg-\\nmentation datasets. In each of the setting, we train the\\nmethods on one domain and test them on other domains.\\nExtensive experiments show that our model is signiﬁcantly', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='better than previous per-pixel classiﬁcation based , and\\nwhole-level mask based segmentation models for out-of-\\ndistribution generalization.\\nTo summarize, our contributions are: 1) We present a hi-\\nerarchical grouping paradigm for robust semantic segmen-\\ntation; 2) based on the hierarchical grouping paradigm, we\\npropose a hierarchical grouping transformer (HGFormer),\\nwhere the pixels are ﬁrst grouped into part-level masks,\\nand then grouped into whole-level masks. Final semantic\\nsegmentation results are obtained by making classiﬁcations\\non all masks; 3) HGFormer outperforms previous seman-\\ntic segmentation models on domain generalized semantic\\nsegmentation across various experimental settings. We also\\ngive detailed analyses of the robustness of grouping-based\\nmethods under distribution shift.\\n2. Related Work\\n2.1. Semantic Segmentation\\nSemantic segmentation is a classic and fundamental\\nproblem in computer vision. It aims to segment the objects', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and scenes in images and give their classiﬁcations. In the\\ndeep learning era, semantic segmentation is usually formu-\\nlated as a pixel-level classiﬁcation problem [ 9–12,36,60]\\nsince FCN [ 36]. Recently, it is becoming popular to use\\nwhole-level mask classiﬁcation to formulate the seman-\\ntic segmentation problem [ 14,15,54,66,67]. In contrast\\nto pixel-level and mask-level classiﬁcation, to our best\\nknowledge, there are very few works on learning part-\\nlevel masks [ 27,62], and using part-level mask classiﬁca-\\ntion [ 20,69] for semantic segmentation in deep learning\\nera. Among them, SSN [ 27] and super pixel FCN [ 62]\\nmainly focus on part-level mask learning instead of part-\\nlevel classiﬁcation for the semantic segmentation results.\\nBI [20] is not an end-to-end model, which needs extra part-\\nlevel masks as input. RegProxy [ 69] is a recent work that\\ncloses to our work, which uses convolutions to learn part-\\nlevel masks, and is only evaluated on the i.i.d. condition.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='In contrast, we use similarity-based grouping to learn part-\\nlevel masks, and are the ﬁrst to validate the effectiveness\\nof using part-level mask classiﬁcation for domain general-\\nized semantic segmentation. Besides, Regproxy [ 69] is cus-\\ntomized with plain ViT [ 51], while our work is applicable\\nto pyramid transformers [ 35,57] and CNNs. Our work is\\nalso different for the hierarchical segmentation design.\\n2\\n15414\\n2.2. Domain Generalization\\nDomain generalization (DG) assumes that the target data\\n(even unlabelled) is not accessible during training. Methods\\nfor DG in classiﬁcation includes domain alignment [ 31,37],\\nmeta-learning [ 3,30], data augmentation [ 52,61], ensemble\\nlearning [ 8,34,63], self-supervised learning [ 4,6], and regu-\\nlarization strategies [ 26,53]. The ensemble of mask classiﬁ-\\ncation at different levels is related to the ensemble methods\\nfor domain generalization. The drawback of the previous\\nensemble-based methods [ 29,64] is that they will largely in-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='crease the runtime. Some ensemble methods [ 45,59] focus\\non the averaging of model weights, which do not increase\\nthe runtime, but increase the training time. Our method does\\nnot introduce extra FLOPS due to the efﬁcient hierarchical\\ngrouping design, and does not introduce extra training time.\\nWhile the DG in classiﬁcation is widely studied in the\\nprevious works, there are only several works that study\\nthe DG in semantic segmentation. The previous meth-\\nods for DG in semantic segmentation includes: (1) Do-\\nmain Randomization [ 44,68] and (2) Normalization and\\nWhitening [ 16,40,41,43]. Although not designed specif-\\nically for domain generalization tasks, the Vision Trans-\\nformers [ 18] have shown their robustness [ 70] in the out-of-\\ndistribution setting. The robustness of Vision Transformer\\nwas explained to be related to the grouping property of self-\\nattention [ 70]. However, there are no works that study the\\neffect of explicit grouping in the segmentation decoder for', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='semantic segmentation in DG. Motivated by these results,\\nwe study the different levels of grouping and mask classiﬁ-\\ncation for semantic segmentation in DG.\\n3. Methods\\nGiven an image I∈RH×W×3, an image partition is\\ndeﬁned as S={R1,...,RN}, such that ∪N\\ni=1Ri= Ω and\\nRi∩Rj= Ø, ifi̸=j. After mapping each region Rito a\\nclass byLi, we getC={L1(R1),...,LN(RN)}. Semantic\\nsegmentation can then be deﬁned as:\\nY={S,C}. (1)\\nAccording to the scales, we can roughly divide the image\\npartitions into three levels: pixels, part-level masks, and\\nwhole-level masks. The pixel partition, where each element\\ninSis a pixel, is widely used by all per-pixel classiﬁcation\\nmethods [ 10,36]. The whole-level masks partition, where\\neach element in Srepresents a whole mask of a class, is\\nused by a few recent approaches [ 14,15]. The part-level\\nmask partition, where each element aims to cover a class\\npart, is proposed by this work and used along with whole-\\nlevel masks to enhance robustness.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Intuitively, mask classiﬁcation is more robust than per-\\npixel classiﬁcation, as masks allow to aggregate features\\nover large image regions of the same class to predict aAlgorithm 1 Part-level grouping\\nRequire: Pixel feature map K∈R(H×W)×d, classiﬁca-\\ntion feature map V∈R(H×W)×d\\n1:Initialize the cluster center featuresQ1∈RNp×dby\\ndown sampling K\\n2:fort= 1,···,Ldo\\n3: Compute assignment matrix AtbyQtandK\\n4: Update the cluster center features Qt+1=At×K\\n5: Update the part-level tokens Zt=At×V\\n6:end for\\n‘global’ label. Despite of this promise, generating whole-\\nlevel masks directly from pixels is a very challenging task.\\nWhile SOTA methods [ 14,15] can generate reasonable\\nwhole-level masks directly from pixels, the ﬂat grouping\\nmethods used are not robust to domain changes – when\\ntested on a different domain, the generated masks are of\\npoor quality, leading to low semantic segmentation perfor-\\nmance (see Fig. 1). In order to tackle this problem, this', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='work proposes using hierarchical grouping in the segmen-\\ntation transformer architecture to group pixels to part-level\\nmasks ﬁrst and then to group part-level masks towhole-\\nlevel masks . The advantages are 1) the grouping task be-\\ncomes less challenging; 2) mask classiﬁcation can be per-\\nformed at both scales and their results can be combined for\\nmore robust label prediction, given that the masks at the\\ntwo scales capture complementary information; 3) global\\nself-attention to aggregate long-range context information\\ncan be performed directly at both part-level and whole-level\\nnow, which is not possible at pixel-level due to the huge\\ncomputation cost.\\n3.1. HGFormer\\nTo efﬁciently implement a model which can predict se-\\nmantic segmentation at different scales, we adopt a hierar-\\nchical grouping process, which consists of two stages. The\\nﬁrst is to group pixels into part-level masks by similarity-\\nbased local clustering. The second is to group part-level', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='masks into whole-level masks by cross-attention. Then we\\nmake classiﬁcation on partitions at different scales. The\\nframework can be seen in Fig. 2. We will introduce the\\ndetails in the following.\\nPart-level grouping. The goal of part-level grouping is\\nto compute a partition Sm={R1,...,RNp}, withNpthe\\nnumber of part-level masks. Smcan be represented by\\nahard assignment matrix˜A∈ {0,1}Np×(HW)such that\\n˜Aij= 1 if thej-th pixel is assigned to mask iand 0 oth-\\nerwise. Since the hard assignment is not differentiable, we\\ncompute a soft assignment matrixA∈[0,1]Np×(HW)such\\nthat/summationtextNp\\ni=1Aij= 1.Aijrepresents the probability of as-\\nsigning the j-th pixel to mask i.\\nTo compute A, we perform an iterative grouping al-\\n3\\n15415\\nPart-level tokens\\nMLPMLP𝑳 ×Part-level mask grouping\\nWhole-level mask grouping\\nWhole-level queries\\n…Whole-level tokens\\n…Class\\n…\\nWhole-level masks\\n……Class\\n…Part-level masks\\n…', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='…Part-level masks\\n…\\nInputFigure 2. The pipeline of our proposed method. We ﬁrst pass an image to a backbone network and get feature maps at different resolutions.\\nThe largest feature map K0is projected to Kfor part-level grouping. The other three feature maps are fused to form a new feature map\\nVfor part-level mask feature extraction used for later classiﬁcation. The details of part-level grouping can be seen in Algorithm 1. The\\ngrouping process is repeated Literations. At the end of each iteration, there are Nppart-level masks, and their tokens. Combining part-\\nlevel classiﬁcations and part-level masks, we can get the semantic segmentation results O1. The part-level tokens from the last iteration\\nof part-level grouping are aggregated to whole-level masks by whole-level grouping (which are actually cross-attention layers). Similarly,\\nthere are also Literations in the whole-level grouping. At the end of each iteration, there are Nowhole-level tokens. Whole-level masks', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='are computed by a matrix multiplication between K0and projected whole-level mask tokens. Similarly, we can get semantic segmentation\\nresultsO2by combining whole-level masks and their classiﬁcations. The ﬁnal results Oare the sum of O1andO2.\\ngorithm (see Algorithm 1). It takes a feature map K∈\\nR(H×W)×das input to compute assignment matrix . We par-\\ntition an image feature map Kinto regular grid cells with\\nsize(r×r)as the initialization of part-level masks, which is\\na common strategy in super pixel learning [ 1,27,62]. Then\\nwe average the features inside regular grid cells to get the\\nfeatures of part-level masks (or called cluster center) fea-\\nturesQ∈RNp×d, whereNp=H/r×W/r . Then we com-\\npute the cosine similarities between pixel-center pairs and\\ngetD∈RNp×(HW). For efﬁciency, we do not compute the\\nsimilarities between all pixel-center pairs. Instead, we only\\ncompute the similarities between pixels and their 9 nearby\\ncenters (see Fig. 3). As a result, we get D′∈R9×(HW).', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='But for the convenience of describing, we still use the Din\\nthe following. Due to the local constraint, each cluster cen-\\nter can only aggregate the nearby pixels, so we can get the\\npart-level masks .\\nThe similarities between the i-th center feature and j-th\\npixel feature are written as:\\nDi,j=/braceleftbiggf(Qi,Kj)ifi∈Nj\\n−∞ ifi /∈Nj,(2)\\nwhereQi∈Rdis thei-th cluster center feature, and\\nKj∈Rdis thej-th pixel feature. f(x,y) =1\\nτx·y\\n|x|·|y|com-\\nputes the cosine similarity between xandy, whereτis the\\ntemperature to adjust the scale of similarities. Njis the\\nset of nearby regular grid cells of j-th pixel, which can be\\nviewed in Fig. 3. Then we can compute the soft assignment\\nmatrix as:\\nAi,j=softmax(D)(i,j) =exp( Di,j)\\n/summationtextNp\\ni=1exp( Di,j),(3)\\nFigure 3. Explanation of the similarities between pixel features\\nand its nearby center features. The grouping process is to assign\\neach pixel to one of Npcenter features. However, due to the com-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='putation cost of the global comparisons, we only compute the sim-\\nilarities between pixels and their nearby center features to perform\\nlocal comparisons. For example, we only assign each pixel in the\\ngreen box to one of its 9 nearby center features.\\nThen we can update the cluster center features by\\nQnew=A×K. (4)\\nAfter we get the new center features, we can compute the\\nnew assignment matrix by using updated center features\\nQnewand feature map K. The process is repeated Ltimes,\\nas shown in Algorithm 1. To get the part-level mask tokens\\nfor classiﬁcation, we use the assignment matrix to extract\\npart-level tokens from another feature map Vby\\nZ=A×V. (5)\\nTo strengthen the part-level mask features, we pass Zto a\\nself-attention layer and a feed forward network (FFN) layer\\nand getZ′. Then we use a linear classiﬁer layer and a\\nsoftmax activation layer to map Z′∈RNp×dto part-level\\nclass predictions Pm∈RNp×K, whereKis the number of\\nclasses.\\n4\\n15416', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='classes.\\n4\\n15416\\nNote that we use different feature maps for part-level\\ngrouping and classiﬁcation to decouple these two kinds of\\ntasks, since the shallow layers are usually used for localiza-\\ntion while the deep layers beneﬁt the classiﬁcation [ 33].\\nWhole-level grouping. The aim of this stage is to group\\npart-level masks into whole-level masks. Our framework\\nis agnostic to the detailed grouping method. But for a fair\\ncomparison, we use the transformer decoders for grouping,\\nfollowing [ 14]. Each transformer decoder layer consists of\\none multi-head cross-attention, one self-attention, and an\\nFFN layer. Firstly, we perform cross-attention between No\\nlearnable positional embeddings E∈RNo×dand part-level\\ntokensZ′∈RNp×dto get the output features:\\nEout=Softmax((EWq)×(Z′Wk)T)×(Z′Wv),(6)\\nwhereWq∈Rd×d,Wk∈Rd×d,Wv∈Rd×dare pro-\\njection heads for queries, keys, and values, respectively.\\nFor simplicity, the multi-head mechanism is ignored in the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='formulation. The cross-attention layer is followed by self-\\nattention, and Eoutand FFN layers. Similar to the pro-\\ncess in part-level mask learning, the transformer decoder\\noperation is also repeated Ltimes. At the end of each\\ntransformer decoder layer, there are two MLP layers. The\\nﬁrst MLP layer maps the out features Eout∈RN×dto\\nmask embedding ε∈RN×d. Then the whole-level masks\\nM∈[0,1]N×(H0×W0)can be computed as\\nM=σ(ε×KT\\n0), (7)\\nwhereKT\\n0∈R(H×W)×dis a feature map before K\\n(see Fig. 2). The second MLP layer maps the out features to\\nclass logits. Then we apply a softmax activation on the class\\nlogits to get the probability predictions Ph∈RN×(K+1),\\nwhereKis the number of classes. There is an extra dimen-\\nsion representing the “no object” category ( ∅).\\nThe difference between our work and the previous coun-\\nterparts [ 14,67] is: the keys andvalues in previous works\\nare pixel features, while our keys andvalues are features of\\npart-level masks. Our hierarchical grouping design reduces', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the computation complexity, since the number of part-level\\nmasks is much smaller than the pixels.\\nMulti-scale semantic segmentation. We conduct clas-\\nsiﬁcation at two levels: part-level mask classiﬁcation and\\nwhole-level mask classiﬁcation. The semantic segmenta-\\ntion results from the part-level mask classiﬁcation can be\\ncomputed as\\nO1=PT\\nm×A (8)\\nThe semantic segmentation results form the whole-level\\nmask classiﬁcation can be computed as\\nO2=PT\\nh×M (9)\\nThe ﬁnal semantic segmentation result is an ensemble of\\ntwo results by a simple addition.Loss design. The challenge in part-level mask learning is\\nthat we do not have the ground truth for the part-level par-\\ntition. The partition at the part-level stages is not unique.\\nTherefore, we design two kinds of losses. First, we di-\\nrectly add a cross-entropy lossLpart,cls onO1. Given that the\\ncross-entropy loss is also affected by classiﬁcation accuracy,\\nwhich does not have a strong constraint on the mask quality,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='we also add a pixel-cluster contrastive loss . The core idea\\nof the contrastive loss is to learn more discriminative feature\\nmaps, which can the be used for similarity-based part-level\\ngrouping. Given ground truth masks MG∈Rg×(H×W),\\nwheregis the number of masks in an image, we ﬁrst av-\\nerage the features within each ground truth mask and get\\nT∈Rg×d. Then the contrastive loss [ 22,50,54] for each\\npixel inKis computed as:\\nLi\\ncontrast=−log/summationtextK\\nj=1MG,j,iexp(f(Ki,Tj))\\n/summationtextK\\nj=1exp(f(Ki,Tj)),(10)\\nwheref(x,y)is a function to measure the similarity be-\\ntween two feature vectors. The losses for the whole-level\\nlearning mainly follow previous works [ 14]. There is a one-\\nto-one matching between predictions and ground truths. For\\nthe matched prediction, a dice loss Ldice, a mask loss Lmask,\\nand a mask classiﬁcation loss Lmask,cls are computed.\\n3.2. Implementation Details\\nWe set the weights for Lpart,cls ,Lcontrast ,Ldice,Lmask, and', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Lmask,cls to 2, 6, 5, 5, and 2, respectively. The temperature\\nτin contrastive loss is 0.1, and the down sample rate ris\\n4. We set the number of reﬁning stages Lin part-level and\\nwhole-level grouping to 6. We use the deformable attention\\nTransformer (MSDeformAttn) [ 71] layers to strengthen the\\nfeatures before we project the feature maps to KandV.\\nThe output strides for KandVare 1/8. We conduct a multi-\\nscale augmentation and then crop a 512×1,024patch for\\ntraining. During testing, we send the images with the origi-\\nnal size to models. By default, models are trained with 20k\\niterations with a batch size of 16. We use the ADAMW\\nas our optimizer with an initial learning rate of 0.0001 and\\n0.05 weight decay.\\n4. Experiments\\n4.1. Datasets\\nReal-world datasets. Cityscapes contains 5,000 urban\\nscene images collected from 50 cities primarily in Germany.\\nThe image size of Cityscapes images is 2,048×1,024.\\nBDD is another real-world dataset, which contains 7,000', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='images for training, and 1,000 images for testing. The im-\\nages of BDD is mainly collected from US. The image size\\nof BDD is 1,280×720. Mapillary [ 39] is a large-scale\\ndataset, which contains 18,000 images for training, 2,000\\n5\\n15417\\nimages for validation, and 5,000 images for testing. The\\nimages of Mapillary are captured from all over the world,\\nat various conditions regarding weather and season, which\\nmakes the dataset very diverse. ACDC [ 48] collects the im-\\nages with a resolution of 1,920×1,080under adverse con-\\nditions, including night, fog, rain, and snow. ACDC con-\\ntains 1,600 images for training, 406 images for validation,\\nand 2,000 images for testing.\\nSynthetic datasets. GTA V is a synthetic dataset collected\\nfrom GTA V game, which contains 12,403, 6,382, and 6,181\\nimages with a resolution of 1,914×1,052for training, val-\\nidation, and testing, respectively. SYNTHIA [ 47] is another\\nsynthetic dataset, which consists of 9,400 photo-realistic\\nimages with a size of 1,280×760.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Common corruption dataset. We follow the previous\\nworks [ 28,60] to expand the Cityscapes validation set with\\n16 type of generated corruptions. The corruptions can be\\ndivided into 4 categories: noise, blur, weather, and digital.\\nThere are 5 severity levels for each kind of corruption.\\n4.2. Main Results\\nWe evaluate models on 4 kinds of generalization settings.\\nNormal-to-adverse generalization. In this experimental\\nsetting, all the models are trained on Cityscapes [ 17] (im-\\nages at normal conditions) and tested on ACDC [ 48] (im-\\nages at 4 kinds of adverse conditions). Although not specif-\\nically designed for domain generalization, transformer-\\nbased methods have been shown to be more robust than\\ntraditional CNN methods. Therefore, we compare our\\nproposed HGFormer with two representative transformer-\\nbased segmentation methods in Tab. 1. Among them, all\\nCNN-based methods and Segformer [ 60] are based on per-\\npixel classiﬁcation. Mask2former [ 14] is based on whole-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='level classiﬁcation. We can see that our method outperforms\\nthe previous CNN-based methods by a large margin, and\\nalso signiﬁcantly outperforms the competitive transformer-\\nbased segmentation models.\\nCityscapes-to-other datasets generalization. In this ex-\\nperimental setting, models are trained on Cityscapes [ 17]\\nand tested on BDD [ 65], Mapillary [ 39], GTA V [ 46], and\\nSynthia [ 47]. The results are shown in Tab. 2. In the\\nﬁrst block of Tab. 2, we compare all the methods with a\\nResNet-50 [ 23] backbone. We can see that the grouping-\\nbased method Mask2Former [ 14] is already comparable to\\nthe previous domain generalization methods, which indi-\\ncates the effectiveness of grouping-based model for gener-\\nalization. Our HGFormer outperforms Mask2Former [ 14]\\nby 1.5 points, showing that our hierarchical grouping-based\\nmodel is better than the ﬂatgrouping-based model for do-\\nmain generalized semantic segmentation.\\nMapillary-to-other datasets generalization. Here, mod-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='els are trained on Mapillary [ 39] and tested on BDD [ 65],\\nMapillary [ 39], GTA V [ 46], and Synthia [ 47]. We canTable 1. Cityscapes-to-ACDC generalization. The models are\\ntrained on Cityscapes [ 17] only, and tested on ACDC [ 48]. The\\nresults of Mask2former [ 14], Segformer [ 60], and HGFormer are\\nimplemented by us. Others are from ACDC paper [ 48]. The results\\nof models trained by us are an average of 3 times. The results of\\nSegformer [ 60] are obtained by their ofﬁcially released model.\\nMethod backbone Fog Night Rain Snow All\\nReﬁneNet [ 32] R101 46.4 29 52.6 43.3 43.7\\nDeepLabv2 [ 10] R101 33.5 30.1 44.5 40.2 38\\nDeepLabv3+ [ 12] R101 45.7 25 50 42 41.6\\nDANet [ 19] DA101 34.7 19.1 41.5 33.3 33.1\\nHRNet [ 55] HR-w48 38.4 20.6 44.8 35.1 35.3\\nMask2former [ 14] R50 54.1 36.5 53.1 50.6 49.8\\nHGFormer ( ours ) R50 56.5 35.8 57.7 56.2 53.0\\nMask2former [ 14] Swin-T 56.4 39.1 58.9 58.2 54.6\\nSegformer [ 60] B2 59.2 38.9 62.5 58.2 56.2\\nHGFormer ( ours ) Swin-T 58.5 43.3 62.0 58.3 56.7', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Segformer [ 60] B5 63.2 47.8 66.4 63.7 62.0\\nMask2former [ 14] Swin-L 69.1 53.1 68.3 65.2 65.0\\nHGFormer ( ours ) Swin-L 69.9 52.7 72.0 68.6 67.2\\nTable 2. Cityscapes to other datasets generalization. Models\\nare trained on Cityscapes and tested on BDD (B), Mapillary (M),\\nGTA V (G), and Synthia (S). Results of IBN, SW, DRPC, GTR,\\nISW, and SAN-SAW are from paper [ 43]. Others are implemented\\nby us. Our results are an average of 3 times.\\nMethod backbone B M G S Average\\nIBN [ 40] R50 48.6 57.0 45.1 26.1 44.2\\nSW [ 41] R50 48.5 55.8 44.9 26.1 43.8\\nDRPC [ 68] R50 49.9 56.3 45.6 26.6 44.6\\nGTR [ 44] R50 50.8 57.2 45.8 26.5 45.0\\nISW [ 16] R50 50.7 58.6 45 26.2 45.1\\nSAN-SAW [ 43] R50 53.0 59.8 47.3 28.3 47.1\\nMask2former [ 14] R50 46.8 61.6 48.0 31.2 46.9\\nHGFormer ( ours ) R50 51.5 61.6 50.4 30.1 48.4\\nMask2former [ 14] Swin-T 51.3 65.3 50.6 34 50.3\\nHGFormer ( ours ) Swin-T 53.4 66.9 51.3 33.6 51.3\\nMask2former [ 14] Swin-L 60.1 72.2 57.8 42.4 58.1\\nHGFormer ( ours ) Swin-L 61.5 72.1 59.4 41.3 58.6', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Table 3. Mapillary-to-other datasets generalization. The mod-\\nels are trained on Mapillary, and tested on GTA V (G), Synthia (S),\\nCityscapes (C), and BDD (B).\\nMethod backbone G S C B Average\\nIBN [ 40] R50 30.7 27.0 42.8 31.0 32.9\\nSW [ 41] R50 28.5 27.4 40.7 30.5 31.8\\nDRPC [ 68] R50 33.0 29.6 46.2 32.9 35.4\\nGTR [ 44] R50 32.9 30.3 45.8 32.6 35.4\\nISW [ 16] R50 33.4 30.2 46.4 32.6 35.6\\nSAN-SAW [ 43] R50 34.0 31.6 48.7 34.6 37.2\\nMask2former [ 14] R50 55.8 37.7 65.6 56.4 53.9\\nHGFormer ( ours ) R50 59.2 37.4 67.1 59.1 55.7\\nMask2former [ 14] Swin-T 57.8 40.1 68.2 59.1 56.3\\nHGFormer ( ours ) Swin-T 60.1 39.5 69.3 61.0 57.5\\nMask2former [ 14] Swin-L 64.8 48.4 77.9 64.7 63.9\\nHGFormer ( ours ) Swin-L 66.5 47.7 78.2 66.3 64.7\\nsee that HGFormer is consistently better than Mask2former\\nwith all backbones, as shown in Tab. 3.\\nNormal-to-corruption generalization. In this set-\\n6\\n15418\\nTable 4. Cityscapes-to-Cityscapes-C generalization (level 5).\\nMethod AverageBlur Noise Digital Weather', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Motion Defoc Glass Gauss Gauss Impul Shot Speck Bright Contr Satur JPEG Snow Spatt Fog Frost\\nMask2former-Swin-T [ 14] 41.6 51.5 49.4 38.2 46.2 9.6 9.8 13.5 44.4 74.2 60.0 70.0 23.3 23.7 59.4 65.4 27.3\\nHGFormer-Swin-T ( ours ) 43.9 52.9 53.9 39.0 49.5 12.1 12.3 18.2 46.3 75.0 60.0 71.2 27.2 29.4 60.6 65.0 29.1\\nMask2former-Swin-L [ 14] 58.7 63.5 66.6 62.1 62.3 26.2 35.9 33.2 62.9 80.0 72.6 77.3 52.5 50.5 75.3 75.1 43.0\\nHGFormer-Swin-L ( ours ) 59.4 64.1 67.2 61.5 63.6 27.2 35.7 32.9 63.1 79.9 72.9 78.0 53.6 55.4 75.8 75.5 43.2\\nTable 5. Ablation of iterations in part-level mask classifca-\\ntion. In this ablation, HGFormer with Swin-T [ 35] is trained on\\nCityscapes (C), and tested on Cityscapes, ACDC all (A), GTA V\\n(G), BDD (B), Synthia (S), and Mapillary (M).\\nIter C A G B S M Avg\\n1 76.8 56.1 51.3 52.1 32.1 65.8 55.7\\n2 77.6 56.1 51.4 52.0 32.3 65.9 55.9\\n3 77.9 56.2 51.8 52.6 32.8 66.2 56.2\\n4 77.9 56.5 52.0 52.6 32.6 66.3 56.3\\n5 77.8 56.4 51.7 52.6 32.5 66.3 56.2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='6 77.4 55.4 50.5 52.2 32.3 65.6 55.6\\nFigure 4. Visualization of results on adverse conditions. The\\nmodels are only trained on Cityscapes, and tested on images with\\nadverse conditions. Our method is signiﬁcantly better than the\\nMask2former [ 14] under adverse conditions.\\nting, models are trained on Cityscapes [ 17] and tested on\\nCityscapes-C [ 28] level 5, which includes 16 types of ar-\\ntiﬁcial corruptions at an extreme level. We compare HG-\\nFormer with Mask2former in Tab. 4, showing that HG-\\nFormer is signiﬁcantly better than Mask2former when gen-\\neralizing to extremely corrupted images.\\n4.3. Ablation Studies\\nAblation of iterations in part-level mask classiﬁcation.\\nWe test the results of different iterations of HGFormer and\\nshow the results in Tab. 5. It shows that the ﬁrst itera-\\ntion is much lower than later iterations on in-domain per-\\nformance, and slightly lower than the later stages on out-\\nof-distribution performance. As the iteration increases, the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='performance gradually increases. The performance is satu-\\nrated at iteration 4. The last stage is lower than the second-\\nlast stage. We hypothesize that the last stage is inﬂuencedby the gradients from whole-level grouping since only the\\npart-level tokens of the last stage are taken as the input of\\nwhole-level grouping. When we remove the whole-level\\ngrouping during training, the last stage is slightly higher\\nthan the second-last stage, which veriﬁes our hypothesis.\\nIndividual performance of part-level and whole-level\\nmasks. We report the individual generalization perfor-\\nmance of part-level and whole-level mask classiﬁcation in\\nTab. 6. It shows that the part-level classiﬁcation is sig-\\nniﬁcantly better than the whole-level classiﬁcation in HG-\\nFormer. And the ensemble of part-level and whole-level\\nclassiﬁcation can further improve the performance, which\\nindicates that the part-level and whole-level mask classiﬁ-\\ncation are complementary to each other.\\n4.4. Visualization Analyses', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Visualization comparisons with Mask2former. We\\npresent the visualization results of Mask2forme and HG-\\nFormer, both with Swin-Tiny on ACDC (see Fig. 4) and\\nCityscapes-C (see Fig. 5) to demonstrate the performance of\\nmodels for real-world adverse conditions and for synthetic\\ncorruptions. We choose impulse noise and defocus blur at\\nlevel 5 for visualization. The results on both of the datasets\\nshow that HGFormer makes fewer errors than Mask2former\\nin adverse conditions.\\nMasks at different levels of corruption. We visualize\\nthe part-level and whole-level masks at different levels of\\ncorruption to show how they change as the severity level in-\\ncreases (see Fig. 6). We can see that the whole-level masks\\nare not stable with the increasing of severity levels, and to-\\ntally failed at level 5. In contrast, the part-level masks are\\nmore stable, and can achieve high recall for the boundaries\\nbetween classes.\\nPart-level masks with different model weights. To pro-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='vide more insights about our method, we visualize the part-\\nlevel masks with model weights from random initializa-\\ntion, ImageNet pre-trained weights and Cityscapes trained\\nweights. The results are shown in Fig. 7. We can see that\\neven using the randomly initialized weights and ImageNet\\npre-trained weights, our model can produce reasonable part-\\nlevel masks, which indicates that our model has the poten-\\ntial for unsupervised segmentation and weakly-supervised\\nsegmentation. The results indicate that the part-level group-\\ning structure itself can provide a good prior , which can\\nexplain the generalization from the grouping side. For the\\nCityscapes trained model weights, the boundaries between\\n7\\n15419\\nTable 6. Comparison of part-level classiﬁcation and whole-level classiﬁcation, and their combination. We train HGFormer with\\nSwin-T on Cityscapes, then test the model on other datasets.\\nwhole-level mask part-level mask ACDC (all) GTA V BDD Synthia Mapillary Average', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='✓ 54.5 49.5 51.5 33.8 66.3 51.1\\n✓ 56.2 51.3 53.1 33.3 66.5 52.1\\n✓ ✓ 56.6 51.3 53.4 33.6 66.9 52.4\\nImpulse Noise\\nDefocus BlurMask2former\\nMask2formerHGFormer\\nHGFormerGround truth\\nGround truth\\nFigure 5. Visualization of results on corruptions. We choose two kinds of corruption at level 5 for this visualization: impulse noise and\\ndefocus blur. The models are trained on Cityscapes.\\nClean Level1 Level2 Level3 Level4 Level5\\nFigure 6. Visualization of part-level and whole-level masks at different levels of Gaussian noise. In the ﬁrst row, we visualize the\\nwhole-level masks from Mask2former. In the second row, we visualize the part-level masks from our method. We can see that our part-\\nlevel masks are more robust than the whole-level masks in Mask2Former as the increasing severity level of Gaussian noise.\\nSegmentation annotation trained\\n Randomly initialized\\n ImageNet pre-trained', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Figure 7. Visualization of part-level masks with different weights. We ﬁnd that the randomly initialized weights can also produce some\\nreasonable part-level masks.\\ndifferent categories are more accurate than the randomly\\ninitialized and ImageNet pre-trained weights. It is worth-\\nwhile noting that the boundaries between the same class\\nis not unique, due to no ground truths being used for part-\\nlevel masks. But with a part-level classiﬁcation, all feasible\\npart-level partitioning can be transformed to correct seman-\\ntic segmentation results, if the boundaries between different\\nclasses are correct.\\n5. Conclusion and Future work\\nIn this paper, we propose a hierarchical semantic seg-\\nmentation model, which can efﬁciently generate image par-\\ntitions in a hierarchical structure. Then we perform both\\npart-level mask classiﬁcation and whole-level mask classi-ﬁcation. The ﬁnal semantic segmentation result is an en-\\nsemble of two results. Our method is veriﬁed to be robust', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='in out-of-distribution images. We can explore more com-\\nplicated fusion methods of classiﬁcation at different scales.\\nWe leave them as future works. Since our model can be\\nconsidered a kind of multi-task learning, how to automati-\\ncally balance the loss weights of classiﬁcation at different\\nscales can be studied in the future.\\nAcknowledgement. This work was supported by the Na-\\ntional Nature Science Foundation of China under grants\\nU22B2011 and 62101390. Jian Ding was also supported\\nby the China Scholarship Council. We would like to thank\\nAhmed Abbas for the insightful discussions.\\n8\\n15420\\nReferences\\n[1] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien\\nLucchi, Pascal Fua, and Sabine S ¨usstrunk. Slic superpix-\\nels compared to state-of-the-art superpixel methods. IEEE\\nPAMI , 34(11):2274–2282, 2012. 4\\n[2] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji-\\ntendra Malik. Contour detection and hierarchical image seg-\\nmentation. IEEE TPAMI , 33(5):898–916, 2010. 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[3] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chel-\\nlappa. Metareg: Towards domain generalization using meta-\\nregularization. NeurIPS , 31, 2018. 3\\n[4] Silvia Bucci, Antonio D’Innocente, Yujun Liao, Fabio M\\nCarlucci, Barbara Caputo, and Tatiana Tommasi. Self-\\nsupervised learning across domains. IEEE PAMI ,\\n44(9):5516–5528, 2021. 3\\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\\nto-end object detection with transformers. In ECCV , pages\\n213–229. Springer, 2020. 2\\n[6] Fabio M Carlucci, Antonio D’Innocente, Silvia Bucci, Bar-\\nbara Caputo, and Tatiana Tommasi. Domain generalization\\nby solving jigsaw puzzles. In CVPR , pages 2229–2238,\\n2019. 3\\n[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,\\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\\ning properties in self-supervised vision transformers. In\\nICCV , pages 9650–9660, 2021. 2\\n[8] Junbum Cha, Hancheol Cho, Kyungjae Lee, Seunghyun', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Park, Yunsung Lee, and Sungrae Park. Domain generaliza-\\ntion needs stochastic weight averaging for robustness on do-\\nmain shifts. arXiv:2102.08604 , 3, 2021. 3\\n[9] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\\nKevin Murphy, and Alan L Yuille. Deeplab: Semantic image\\nsegmentation with deep convolutional nets, atrous convolu-\\ntion, and fully connected crfs. IEEE PAMI , 40(4):834–848,\\n2017. 2\\n[10] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\\nKevin Murphy, and Alan L Yuille. Deeplab: Semantic image\\nsegmentation with deep convolutional nets, atrous convolu-\\ntion, and fully connected crfs. IEEE PAMI , 40(4):834–848,\\n2017. 2,3,6\\n[11] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\\nHartwig Adam. Rethinking atrous convolution for semantic\\nimage segmentation. arXiv:1706.05587 , 2017. 2\\n[12] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\\nseparable convolution for semantic image segmentation. In', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ECCV , pages 801–818, 2018. 2,6\\n[13] Yuhua Chen, Dengxin Dai, Jordi Pont-Tuset, and Luc\\nVan Gool. Scale-aware alignment of hierarchical image seg-\\nmentation. In CVPR , 2016. 2\\n[14] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-\\nder Kirillov, and Rohit Girdhar. Masked-attention mask\\ntransformer for universal image segmentation. In CVPR ,\\npages 1290–1299, 2022. 2,3,5,6,7[15] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-\\npixel classiﬁcation is not all you need for semantic segmen-\\ntation. NeurIPS , 34:17864–17875, 2021. 2,3\\n[16] Sungha Choi, Sanghun Jung, Huiwon Yun, Joanne T Kim,\\nSeungryong Kim, and Jaegul Choo. Robustnet: Improving\\ndomain generalization in urban-scene segmentation via in-\\nstance selective whitening. In CVPR , pages 11580–11590,\\n2021. 3,6\\n[17] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\\nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe\\nFranke, Stefan Roth, and Bernt Schiele. The cityscapes', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='dataset for semantic urban scene understanding. In CVPR ,\\npages 3213–3223, 2016. 6,7\\n[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale. arXiv:2010.11929 ,\\n2020. 3\\n[19] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei\\nFang, and Hanqing Lu. Dual attention network for scene\\nsegmentation. In CVPR , pages 3146–3154, 2019. 6\\n[20] Raghudeep Gadde, Varun Jampani, Martin Kiefel, Daniel\\nKappler, and Peter V Gehler. Superpixel convolutional net-\\nworks using bilateral inceptions. In ECCV , pages 597–613.\\nSpringer, 2016. 2\\n[21] Yong Guo, David Stutz, and Schiele Bernt. Improving ro-\\nbustness of vision transformers by reducing sensitivity to\\npatch corruptions. In CVPR , 2023. 1\\n[22] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Girshick. Momentum contrast for unsupervised visual rep-\\nresentation learning. In CVPR , pages 9729–9738, 2020. 5\\n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In CVPR ,\\npages 770–778, 2016. 6\\n[24] Geoffrey Hinton. How to represent part-whole hierarchies in\\na neural network. arXiv:2102.12627 , 2021. 2\\n[25] Lukas Hoyer, Dengxin Dai, and Luc Van Gool. DAFormer:\\nImproving network architectures and training strategies for\\ndomain-adaptive semantic segmentation. In CVPR , 2022. 1\\n[26] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang.\\nSelf-challenging improves cross-domain generalization. In\\nECCV , pages 124–140. Springer, 2020. 3\\n[27] Varun Jampani, Deqing Sun, Ming-Yu Liu, Ming-Hsuan\\nYang, and Jan Kautz. Superpixel sampling networks. In\\nECCV , pages 352–368, 2018. 2,4\\n[28] Christoph Kamann and Carsten Rother. Benchmarking the\\nrobustness of semantic segmentation models. In CVPR ,\\npages 8828–8838, 2020. 6,7', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[29] Balaji Lakshminarayanan, Alexander Pritzel, and Charles\\nBlundell. Simple and scalable predictive uncertainty esti-\\nmation using deep ensembles. NeurIPS , 30, 2017. 3\\n[30] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy\\nHospedales. Learning to generalize: Meta-learning for do-\\nmain generalization. In AAAI , volume 32, 2018. 3\\n[31] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot.\\nDomain generalization with adversarial feature learning. In\\nCVPR , pages 5400–5409, 2018. 3\\n9\\n15421\\n[32] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian\\nReid. Reﬁnenet: Multi-path reﬁnement networks for high-\\nresolution semantic segmentation. In CVPR , pages 1925–\\n1934, 2017. 6\\n[33] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\\nBharath Hariharan, and Serge Belongie. Feature pyramid\\nnetworks for object detection. In CVPR , pages 2117–2125,\\n2017. 5\\n[34] Quande Liu, Qi Dou, Lequan Yu, and Pheng Ann Heng. Ms-\\nnet: multi-site network for improving prostate segmentation', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='with heterogeneous mri data. T-MI , 39(9):2713–2724, 2020.\\n3\\n[35] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\\nHierarchical vision transformer using shifted windows. In\\nICCV , pages 10012–10022, 2021. 2,7\\n[36] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\\nconvolutional networks for semantic segmentation. In\\nCVPR , pages 3431–3440, 2015. 2,3\\n[37] Krikamol Muandet, David Balduzzi, and Bernhard\\nSch¨olkopf. Domain generalization via invariant fea-\\nture representation. In International Conference on Machine\\nLearning , pages 10–18. PMLR, 2013. 3\\n[38] Muhammad Muzammal Naseer, Kanchana Ranasinghe,\\nSalman H Khan, Munawar Hayat, Fahad Shahbaz Khan, and\\nMing-Hsuan Yang. Intriguing properties of vision transform-\\ners.NeurIPS , 34:23296–23308, 2021. 2\\n[39] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and\\nPeter Kontschieder. The mapillary vistas dataset for semantic', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='understanding of street scenes. In ICCV , pages 4990–4999,\\n2017. 5,6\\n[40] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two\\nat once: Enhancing learning and generalization capacities\\nvia ibn-net. In ECCV , pages 464–479, 2018. 3,6\\n[41] Xingang Pan, Xiaohang Zhan, Jianping Shi, Xiaoou Tang,\\nand Ping Luo. Switchable whitening for deep representation\\nlearning. In ICCV , pages 1863–1871, 2019. 3,6\\n[42] Sayak Paul and Pin-Yu Chen. Vision transformers are robust\\nlearners. In AAAI , volume 36, pages 2071–2081, 2022. 1\\n[43] Duo Peng, Yinjie Lei, Munawar Hayat, Yulan Guo, and Wen\\nLi. Semantic-aware domain generalized segmentation. In\\nCVPR , pages 2594–2605, 2022. 3,6\\n[44] Duo Peng, Yinjie Lei, Lingqiao Liu, Pingping Zhang,\\nand Jun Liu. Global and local texture randomization\\nfor synthetic-to-real semantic segmentation. IEEE TIP ,\\n30:6594–6608, 2021. 3,6\\n[45] Alexandre Rame, Matthieu Kirchmeyer, Thibaud Rahier,\\nAlain Rakotomamonjy, Patrick Gallinari, and Matthieu', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Cord. Diverse weight averaging for out-of-distribution gen-\\neralization. arXiv:2205.09739 , 2022. 3\\n[46] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen\\nKoltun. Playing for data: Ground truth from computer\\ngames. In ECCV , pages 102–118. Springer, 2016. 6\\n[47] German Ros, Laura Sellart, Joanna Materzynska, David\\nVazquez, and Antonio M Lopez. The synthia dataset: A large\\ncollection of synthetic images for semantic segmentation of\\nurban scenes. In CVPR , pages 3234–3243, 2016. 6[48] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Acdc:\\nThe adverse conditions dataset with correspondences for se-\\nmantic driving scene understanding. In ICCV , pages 10765–\\n10775, 2021. 6\\n[49] Zhuowen Tu, Xiangrong Chen, Alan L Yuille, and Song-\\nChun Zhu. Image parsing: Unifying segmentation, detec-\\ntion, and recognition. IJCV , 63(2):113–140, 2005. 2\\n[50] Wouter Van Gansbeke, Simon Vandenhende, Stamatios\\nGeorgoulis, and Luc Van Gool. Unsupervised semantic seg-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='mentation by contrasting object mask proposals. In ICCV ,\\npages 10052–10062, 2021. 5\\n[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. NeurIPS , 30, 2017. 2\\n[52] Riccardo V olpi and Vittorio Murino. Addressing model vul-\\nnerability to distributional shifts over image transformation\\nsets. In ICCV , pages 7980–7989, 2019. 3\\n[53] Haohan Wang, Zexue He, Zachary C Lipton, and Eric P\\nXing. Learning robust representations by projecting superﬁ-\\ncial statistics out. arXiv:1903.06256 , 2019. 3\\n[54] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and\\nLiang-Chieh Chen. Max-deeplab: End-to-end panoptic seg-\\nmentation with mask transformers. In CVPR , pages 5463–\\n5474, 2021. 2,5\\n[55] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,\\nChaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui\\nTan, Xinggang Wang, et al. Deep high-resolution rep-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='resentation learning for visual recognition. IEEE PAMI ,\\n43(10):3349–3364, 2020. 6\\n[56] Mei Wang and Weihong Deng. Deep visual domain adapta-\\ntion: A survey. Neurocomputing , 312:135–153, 2018. 1\\n[57] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\\nPyramid vision transformer: A versatile backbone for dense\\nprediction without convolutions. In ICCV , pages 568–578,\\n2021. 2\\n[58] Florian Wenzel, Andrea Dittadi, Peter Vincent Gehler, Carl-\\nJohann Simon-Gabriel, Max Horn, Dominik Zietlow, David\\nKernert, Chris Russell, Thomas Brox, Bernt Schiele, et al.\\nAssaying out-of-distribution generalization in transfer learn-\\ning. arXiv:2207.09239 , 2022. 1\\n[59] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Re-\\nbecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos,\\nHongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Ko-\\nrnblith, et al. Model soups: averaging weights of multi-\\nple ﬁne-tuned models improves accuracy without increas-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ing inference time. In International Conference on Machine\\nLearning , pages 23965–23998. PMLR, 2022. 3\\n[60] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,\\nJose M Alvarez, and Ping Luo. Segformer: Simple and ef-\\nﬁcient design for semantic segmentation with transformers.\\nNeurIPS , 34:12077–12090, 2021. 1,2,6\\n[61] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc\\nNiethammer. Robust and generalizable visual representation\\nlearning via random convolutions. arXiv:2007.13003 , 2020.\\n3\\n10\\n15422\\n[62] Fengting Yang, Qian Sun, Hailin Jin, and Zihan Zhou. Su-\\nperpixel segmentation with fully convolutional networks. In\\nCVPR , pages 13964–13973, 2020. 2,4\\n[63] Teresa Yeo, O ˘guzhan Fatih Kar, and Amir Zamir. Robustness\\nvia cross-domain ensembles. In ICCV , pages 12189–12199,\\n2021. 3\\n[64] Teresa Yeo, O ˘guzhan Fatih Kar, and Amir Zamir. Robustness\\nvia cross-domain ensembles. In ICCV , pages 12189–12199,\\n2021. 3\\n[65] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Dar-\\nrell. Bdd100k: A diverse driving dataset for heterogeneous\\nmultitask learning. In CVPR , pages 2636–2645, 2020. 6\\n[66] Qihang Yu, Huiyu Wang, Dahun Kim, Siyuan Qiao,\\nMaxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille,\\nand Liang-Chieh Chen. Cmt-deeplab: Clustering mask\\ntransformers for panoptic segmentation. In CVPR , pages\\n2560–2570, 2022. 2\\n[67] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins,\\nYukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh\\nChen. k-means mask transformer. In ECCV , pages 288–307.\\nSpringer, 2022. 2,5\\n[68] Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto\\nSangiovanni-Vincentelli, Kurt Keutzer, and Boqing\\nGong. Domain randomization and pyramid consistency:\\nSimulation-to-real generalization without accessing target\\ndomain data. In ICCV , pages 2100–2110, 2019. 3,6\\n[69] Yifan Zhang, Bo Pang, and Cewu Lu. Semantic segmen-\\ntation by early region proxy. In CVPR , pages 1258–1268,\\n2022. 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='2022. 2\\n[70] Daquan Zhou, Zhiding Yu, Enze Xie, Chaowei Xiao, An-\\nimashree Anandkumar, Jiashi Feng, and Jose M Alvarez.\\nUnderstanding the robustness in vision transformers. In In-\\nternational Conference on Machine Learning , pages 27378–\\n27394. PMLR, 2022. 1,2,3\\n[71] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\\nand Jifeng Dai. Deformable detr: Deformable transformers\\nfor end-to-end object detection. arXiv:2010.04159 , 2020. 5\\n11\\n15423', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Exploring Structured Semantic Prior\\nfor Multi Label Recognition with Incomplete Labels\\nZixuan Ding1,4*Ao Wang2,3,4*Hui Chen2,3,†Qiang Zhang1\\nPengzhang Liu5Yongjun Bao5Weipeng Yan5Jungong Han6,7\\n1Xidian University2Tsinghua University3BNRist\\n4Hangzhou Zhuoxi Institute of Brain and Intelligence5JD.com\\n6Department of Computer Science, the University of Sheffield, UK\\n7Centre for Machine Intelligence, the University of Sheffield, UK\\ndingzixuan@stu.xidian.edu.cn wa22@mails.tsinghua.edu.cn qzhang@xidian.edu.cn\\n{jichenhui2012,jungonghan77 }@gmail.com {Paul.yan, baoyongjun, liupengzhang }@jd.com\\nAbstract\\nMulti-label recognition (MLR) with incomplete labels is\\nvery challenging. Recent works strive to explore the image-\\nto-label correspondence in the vision-language model, i.e.,\\nCLIP [22], to compensate for insufficient annotations. In\\nspite of promising performance, they generally overlook the\\nvaluable prior about the label-to-label correspondence. In', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='this paper, we advocate remedying the deficiency of label\\nsupervision for the MLR with incomplete labels by deriving\\na structured semantic prior about the label-to-label corre-\\nspondence via a semantic prior prompter. We then present\\na novel Semantic Correspondence Prompt Network (SCP-\\nNet), which can thoroughly explore the structured semantic\\nprior. A Prior-Enhanced Self-Supervised Learning method\\nis further introduced to enhance the use of the prior. Com-\\nprehensive experiments and analyses on several widely used\\nbenchmark datasets show that our method significantly out-\\nperforms existing methods on all datasets, well demonstrat-\\ning the effectiveness and the superiority of our method.\\nOur code will be available at https://github.com/\\njameslahm/SCPNet .\\n1. Introduction\\nMulti-label recognition (MLR) aims to describe the im-\\nage content with various semantic labels [5, 26, 29, 30]. It\\nencodes the visual information into structured labels, which', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='can benefit the index and fast retrieval of images in broad\\npractical applications, such as the search engine [24,27] and\\nthe recommendation system [2, 33].\\nBenefited from the development of deep learning, MLR\\n*Equal contributions. †Corresponding author.\\nCNNbackbonesoftmaxCNNbackbonesigmoidtransferImageencoderTextencoderImageencoderLabelencodertransfertransferMLRMLRMCCL(a)CNN-based(b)DualCoOpImageencoderLabelencoderMLRN4132(c)ourSCPNetFigure 1. Overview of CNN-based, DualCoOp [26] and our SCP-\\nNet. Like DualCoOp, our SCPNet adopts CLIP as the base model.\\nDifferently, our SCPNet aims to enhance the MLR with the prior\\nabout the label-to-label correspondence. MC means multi-class.\\nCL denotes contrastive learning.\\nhas achieved remarkable progress in recent years. How-\\never, collecting high-quality full annotations becomes very\\nchallenging when the label set scales up, which greatly hin-\\nders the wide usage of MLR in real scenarios. Recently,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='researchers explore more feasible solutions for MLR. For\\nexample, the full label setting is relaxed with a partial label\\nsetting in [3, 21], which merely annotates a few labels for\\neach training image. One more extreme setting with solely\\nonesingle positive label is tackled in [8,16]. These settings\\ncan be unified into a common issue of incomplete labels ,\\nwhich relieves the burden of the full annotation and con-\\nsiderably reduces the annotation cost. Therefore, it draws\\nincreasing attention from both academia and industry.\\nCompared with the full label setting, the incomplete la-\\nbel setting encounters a dilemma of poor supervision, re-\\nsulting in severe performance drops for MLR. Existing\\nmethods strive to regain supervision from missing labels by\\nexhaustively exploring the image-to-label correspondence\\nvia semantic-aware modules [4,21] or loss calibration meth-\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Except for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n3398\\nods [8, 16, 32]. A convolutional neural network (CNN) pre-\\ntrained on the ImageNet is usually leveraged to construct\\nthe MLR model. Its multi-class softmax layer is often re-\\nplaced by a multi-label sigmoid layer (Fig. 1 (a)). Such a re-\\nplacement wipes out prior knowledge about the correspon-\\ndence between images and labels although it is necessary\\nand inevitable.\\nRecently, vision-language pretrained models have ob-\\ntained remarkable success in various vision tasks [26, 34,\\n35]. Thanks to their large-scale pretraining, the vision-\\nlanguage model, e.g., CLIP [22], which is trained with 400\\nmillion image-text pairs, can well bridge the visual-textual\\ngap [26], providing rich prior knowledge for the down-\\nstream tasks. For the MLR task, Sun et al. [26] propose\\na DualCoOp method, which is the first work to employ the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='CLIP as the MLR base model. Through dual prompts, Du-\\nalCoOp directly adopts the text encoder in the CLIP as the\\nmulti-label classification head (Fig. 1 (b)), without aban-\\ndoning the visual-textual prior in the pretrained CLIP.\\nDespite its effectiveness, DualCoOp is still limited in\\nremedying the deficiency of label supervision, which is de-\\nsired for the MLR with incomplete labels. Intuitively, it is\\nconvenient to reason unknown labels from annotated labels\\nby leveraging the correspondence among labels, e.g., tables\\nare likely to appear with chairs, and cars are usually ac-\\ncompanied by roads. Therefore, such a label-to-label cor-\\nrespondence can help survive more label supervision and\\nthus benefit MLR with incomplete labels. Besides, although\\nmost vision-language models do not encourage the con-\\ntrastive learning among texts, they are still abundant in the\\nknowledge about the label-to-label correspondence because\\nof the large-scale cross-modality training. However, such a', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='valuable prior is rarely explored in the existing state-of-the-\\nart method, i.e., DualCoOp [26].\\nIn this paper, we aim to mitigate such deficiency of label\\nsupervision for MLR with incomplete labels by leveraging\\nthe abundant prior about the label-to-label correspondence\\nin the CLIP [22]. We present a structured prior prompter\\nto conveniently derive a structured semantic prior from the\\nCLIP. Then we propose a novel Semantic Correspondence\\nPrompt network (SCPNet) (Fig. 1 (c)), which can prompt\\nthe structured label-to-label correspondence with a cross-\\nmodality prompter. Our SCPNet also equips a semantic as-\\nsociation module to explore high-order relationships among\\nlabels with the guidance of the derived structured semantic\\nprior. A prior-enhanced self-supervised learning method is\\nfurther introduced to comprehensively investigate the valu-\\nable prior. As a result, our method can neatly calibrate its\\npredicted semantic distribution while maintaining the self-\\nconsistency.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='consistency.\\nTo verify the effectiveness of the proposed method for\\nMLR with incomplete labels, we conduct extensive exper-\\niments and analyses on a series of widely used benchmarkdatasets, i.e., MS COCO [19], PASCAL VOC [11], NUS\\nWide [7], CUB [28] and OpenImages [17]. Experimental\\nresults show that our method can significantly outperform\\nstate-of-the-art methods on all datasets with a maximal im-\\nprovement of 6.8%/3.4%mAP for the single positive la-\\nbel setting and the partial label setting, respectively, well\\ndemonstrating its effectiveness and superiority.\\nOverall, our contributions are four folds.\\n• We advocate leveraging a structured semantic prior to\\ndeal with the deficiency of label supervision for MLR\\nwith incomplete labels. To this end, we extract such a\\nprior via a structured prior prompter.\\n• We present a semantic correspondence prompt Net-\\nwork (SCPNet) based on a cross-modality prompter\\nand a semantic association module. The SCPNet can', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='adequately explore the structured prior knowledge,\\nthus boosting MLR with incomplete labels.\\n• We design a prior-enhanced self-supervised learning\\nmethod to further investigate such a structured seman-\\ntic prior, which can enjoy both distribution refinement\\nand self-consistency.\\n• Experimental results show that our method can con-\\nsistently achieve state-of-the-art performance on all\\nbenchmark datasets, revealing the significant effective-\\nness. Thorough analyses also demonstrate the superi-\\nority of our method.\\n2. Related work\\nMulti-label recognition with full annotations. Multi-\\nlabel Recognition has long been a hot topic in the computer\\nvision field [1,21,30]. A generic method is to learn multiple\\nbinary classifiers [8, 16], which usually takes no considera-\\ntion of the label correlation. Recently, the label-to-label cor-\\nrespondence is established through graph neural networks\\nor transformer structures [6, 29]. These methods heavily', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='rely on the quality of label supervision. However, collect-\\ning a large-scale dataset with complete labels is challenging\\nand expensive. In real scenarios, researchers explore much\\nmore practical settings with incomplete labels, i.e., MLR\\nwith partial labels and MLR with a single positive label.\\nMulti-label recognition with incomplete labels. In the\\npartial label setting, only a few labels need to be annotated\\nfor each training image. Durand et al. [9] adopt a curricu-\\nlum learning based model to predict the missing labels dur-\\ning the training procedure. Pu et al. [21] and Chen et al. [4]\\ntransfer predictions of neighboring images via image-image\\ncorrelation. However, their performance is not guaranteed\\nin more severe scenarios, i.e., single positive label setting,\\nin which each image is provided with solely one positive\\nannotation. To tackle the issue of the single positive label,\\n3399\\nSemantic Association Module\\n𝐿\\nText\\nencoder\\n𝐺∙\\nImage\\nencoder\\n𝐹∙𝒗𝟏𝒗𝒎𝒆𝒊\\n+Cross -Modality Prompter\\n…\\n𝒙', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='…\\n𝒙\\nSemantic Correspondence Prompt Network (SCPNet)…Semantic Correspondence Prompt Network ( SCPNet )\\n… …Prior -Enhanced Self -Supervised Learning (PESSL)\\n𝑝(𝑌|𝒙) 𝑝(𝑌|𝒙)\\nSASC(𝑝(𝑌|ω𝒙,𝑨∗)𝑝(𝑌|Ω𝒙) 𝑝(𝑌|ω𝒙)…𝑝(𝑌|𝒙)\\n𝐿𝑑𝑠𝑡𝑙 𝐿𝑐𝑠𝑡𝑨∗𝑝(𝑌|ω(𝒙))\\nWeak  transformantion\\nStrong transformation \\n𝑌𝒙\\nperson\\ntruck\\ncar\\nbus𝑝(𝑦1|𝒙)\\n𝑝(𝑦𝑛|𝒙)\\n…\\n… … …bus… person\\nlabel gt. pred. anno.\\nperson \\uf050\\uf050 1\\ntruck \\uf050\\uf050 0\\ncat ×× -1𝑌 truck\\nΩ(𝒙)ω(𝒙)\\n…structured semantic\\nprior 𝑨∗\\n𝒕𝒊\\n𝒛𝒊∗\\n𝒇Figure 2. An overview of the proposed method. We design a semantic correspondence prompt network to explore the structured semantic\\nprior for MlR with incomplete labels. A prior-enhanced self-supervised learning strategy is used to enhance such exploration.\\nCole et al. [8] propose a regularized online loss via a joint\\noptimization of label estimator and image classifier. Zhang\\net al. [32] adopt a label correction process for the proba-\\nbility exceeding a fixed threshold. Kim et al. [16] propose', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='to reject or correct the large loss samples during training,\\nwhich can prevent over-fitting false negative labels. How-\\never, different from our solution, they usually independently\\ncalibrate the importance of different labels [8, 16, 32], tak-\\ning no consideration of the semantic correspondence among\\nlabels.\\nVision-language models in downstream visual tasks.\\nRadford et al . [22] exploit the contrastive learning with\\nlarge-scale image-text pairs, i.e., about 400 million pairs,\\nending up with a powerful vision-language model, i.e.,\\nCLIP. Such a model shows remarkable generalization ca-\\npability in downstream visual tasks [22]. Therefore, re-\\nsearchers exhaustively explore how to leverage the abun-\\ndant vision-language correspondence [12, 14, 23, 26]. Sun\\net al. [26] also employ CLIP for MLR. They present dual\\nprompts, i.e., a positive prompt and a negative one, to ex-\\nplore the rich image-to-label correspondence in CLIP. How-\\never, different from our motivation, they overlook the rich', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='label-to-label correspondence in CLIP.\\n3. Methodology\\n3.1. Structured Prior Prompter\\nFor MLR with full annotations, existing methods can\\nachieve fruitful outcomes by exploring the semantic corre-\\nspondence between images and labels [6]. However, they\\nrequire abundant label supervision to obtain accurate la-\\nbel co-occurrence information for the estimation of label\\nrelationships. Therefore, in MLR with incomplete labels,\\nthe scarce label supervision greatly hinders their capabil-\\nity to explore the semantic correspondence. Benefited from\\nthe development of large-scale pretrained embeddings, e.g.,\\nGlove [20], or models, e.g., BERT [15] and CLIP [22],we can easily obtain contextual representations for labels,\\nwhich can be directly used to derive such a label-to-label\\ncorrespondence. Such a annotation-free strategy is no-\\ntably appealing when no adequate label supervision is pro-\\nvided. Furthermore, the abundant correspondence prior in', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the pretrained model can help associate the annotated label\\nwith unknown labels, which promisingly alleviates the de-\\nficiency of label supervision. Hence, we introduce a struc-\\ntured prior prompter to explore such a label-to-label corre-\\nspondence in the pretrained model. Considering the popu-\\nlarity and the remarkable performance in the computer vi-\\nsion community, we choose the vision-language model, i.e.,\\nCLIP [22], as the target.\\nSpecifically, in the proposed structured prior prompter,\\nfor a set of to-be-explored labels Y={y0, y1, ..., y n}, we\\nderive the label feature by feeding a prompt template, i.e.,a\\nphoto of a [CLS] , into the text encoder of CLIP. We denote\\nthe label feature as ¯zifor each yi. Then the correlation prior\\namong labels, denoted as A= (aij)n×n, can be derived as:\\naij=sim(¯zi,¯zj) (1)\\nwhere sim (·,·)is the cosine similarity.\\nFor each entry ai, we select the top Kelements and\\nset the rest to zero, ending up with a sparse matrix, A′=\\n(a′\\nij)n×n:\\na′', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='(a′\\nij)n×n:\\na′\\nij=\\x1aaij,ifj∈topK(ai)\\n0, ifj /∈topK(ai)(2)\\nFollowing [6], we mitigate the over-smoothness of graph\\nrepresentation by adjusting the sparse graph A′as follows:\\n¯aij=\\x1a(s/Pn\\ni̸=j′a′\\nij′)×a′\\nij,ifi̸=j\\n1−s, ifi=j(3)\\nwhere sis a hyper-parameter which determines weights as-\\nsigned to a node itself and its neighboring nodes. The label\\n3400\\ncorrespondence graph Gcan be derived as:\\na∗\\nij=I[¯aij̸= 0] exp(¯ aij/τ′)P\\njI[¯aij̸= 0] exp(¯ aij/τ′)(4)\\nwhere τ′controls the distribution smoothness and I [·]is an\\nindicator function. We denote the the adjacency matrix of\\nGasA∗= (a∗\\nij)n×n.\\nWe see that A∗emphasizes the importance of the node\\nitself and weights other nodes according to their relation-\\nships (see Eq. (1)). Therefore, the fruitful label correspon-\\ndence can be encoded in such a structured graph, i.e.,A∗,\\nproviding rich structured semantic prior for MLR models.\\n3.2. Semantic Correspondence Prompt Network\\nAs shown in Fig. 2, the SCPNet consists of a cross-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='modality prompter and a semantic association module.\\nCross-modality prompter (CMP). Previous works [4,\\n16, 21] usually employ a convolutional neural network pre-\\ntrained on ImageNet, e.g., ResNet50. During fine-tuning in\\nthe downstream MLR tasks, the prior knowledge about the\\nimage-to-label correspondence is generally discarded due\\nto the semantic shift, i.e., different label sets between the\\nImageNet and the MLR benchmark datasets. Differently,\\nwe aim to take full use of such an image-label prior during\\nmodel optimization. Similar to [26], we resolve the prob-\\nlem of semantic shift by a cross-modality prompter, based\\non a vision-language model, i.e., CLIP [22].\\nFormally, following [35], given a label set, i.e.,Y=\\n{y0, y1, ..., y n}, we introduce msoft prompt tokens to ex-\\ntract its representation. For ease of explanation, we denote\\nthe prompt as ti={v1,v2, ...,vm,ei}, where vwith a sub-\\nscript denotes a soft prompt token and eiis the embedding', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ofyi. The label feature of yi, denoted as zi, can be derived\\nby the text encoder of CLIP. For an input image x, its vi-\\nsual representation, denoted as f, is extracted by the image\\nencoder of CLIP. The process of feature extraction can be\\ncomputed as follows:\\nf=F(x),zi=G(ti), (5)\\nwhere F(·)andG(·)denote the image encoder and the text\\nencoder in CLIP, respectively.\\nSemantic association module (SAM). As CMP still\\nlacks capturing the label-to-label correspondence, we fur-\\nther equip a semantic association module to capture high-\\norder relationships among labels. Specifically, with guid-\\nance of the structured semantic prior A∗(see Eq. (4)),\\nwe utilize Lgraph convolutional network (GCN) layers to\\nprogressively refine the input features H0=Z, where\\nZ={z0,z1, ...,zn}is a combination of features for Y\\nas in Eq. (5). The l-th GCN layer is updated as follows:\\nHl+1=ρ(A∗HlWl), (6)where Wwith a superscript is a learnable parameter ma-\\ntrix and ρis a non-linear function. l∈[0, L). The fi-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='nal refined label representations can be obtained through a\\nresidual connection, i.e.,Z∗=H0+HL. The likelihood\\np(yi|x)can be computed as:\\np(yi|x) =σ(sim(f,z∗\\ni)/τ), (7)\\nwhere zi∗denotes the refined feature for label yi.\\nBenefited from the GCN, the structured label-to-label\\ncorrespondence in CLIP, which is represented by A∗, can\\nbe progressively refined in the label representation. There-\\nfore, during the semantic matching between the image fea-\\nture and the label feature, i.e., Eq. (7), labels with high cor-\\nrelations will obtain similar likelihoods, enabling a subtle\\nsemantic association.\\n3.3. Prior-Enhanced Self-Supervised Learning\\nThe proposed prior-enhanced self-supervised learning\\nstrategy, dubbed PESSL, aims to make full use of the struc-\\ntured semantic correspondence prior. We endow the pro-\\nposed PESSL with a self-supervised consistency loss and\\na self-distillation objective that is boosted by a structure-\\naware semantic calibration strategy.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Structure-aware semantic calibration. Intuitively, if\\ntwo labels are semantically correlated, they may be ob-\\nserved in one image. For MLR, such a correspondence can\\nhelp decide potential semantic labels for an input image,\\ngiven its predictions. Therefore, we formulate the likeli-\\nhood of p(yi|x)as a weighted combination of likelihoods\\nfor correlated neighboring labels of yi:\\np∗(yi|x) =X\\nyj∈N(yi)w(i, j)×p(yj|x) (8)\\nHere, w(i, j)is a correlation weight indicating the relation-\\nship between yiandyj.N(yi)denotes a correlated neigh-\\nboring set of labels corresponding to yi.\\nFor ease of explanation, we introduce a correlation ma-\\ntrixWto represent the whole correlation among labels, i.e.,\\nW= (w(i, j))n×n. We then customize the whole process\\nas a function parameterized by W∈Rn×nand the distri-\\nbution over Ygiven the input x,i.e.,p(Y|x)∈Rn×1:\\nSASC (p(Y|x),W) =Wp(Y|x) (9)\\nPrior-enhanced learning. Existing loss correction\\nmethods individually reweight each label, without taking', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='into consideration the correspondence among labels. Here,\\nwe propose to follow the self-supervised learning princi-\\nple [25, 31] and introduce a self-distillation learning strat-\\negy to benefit the MLR model from the structured semantic\\ncorrespondence among labels.\\nSpecifically, we derive two different versions for the\\ninput image xwith one weak transformation ω(·)and\\n3401\\none strong transformation Ω(·), respectively. Their corre-\\nsponding semantic distributions, denoted as p(y|ω(x))and\\np(y|Ω(x)), respectively, can be derived by Eq. (7). Then we\\nuse a consistency loss to encourage them to be consistent.\\nDifferent from [31], which simply regularizes the model\\nwith the most confident label, we construct a set of con-\\nfident labels O(x)with the top highest probability larger\\nthan a threshold Tinp(y|ω(x)),i.e.,O(x) ={c|c∈\\ntopK(p(y|ω(x)))∧p(c|ω(x))>T(c)}. A dynamic thresh-\\nold strategy is performed for each label, as [31]. The con-\\nsistency loss is then derived by:\\nLcst=−XY', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Lcst=−XY\\nc∈O(x)logp(c|Ω(x))\\n−XY\\nc/∈O(x)log(1−p(c|Ω(x)))(10)\\nWe calibrate the distribution of the weak-transformed\\nimage, i.e.,p(y|ω(x)), by using the SASC function (see\\nEq. (9)):\\np∗(y|ω(x)) = SASC (p(y|ω(x)),A∗) (11)\\nwhere A∗represents the structured semantic prior, derived\\nby Eq. (4). Considering that compared with the weak-\\ntransformed image, the strong-transformed image is usu-\\nally more difficult to learn. Therefore, we employ a self-\\ndistillation objective to optimize the distribution of the\\nstrong-transformed image Ω(x)with the guidance of the\\ncalibrated semantic distribution via the KL-divergence:\\nLdstl=−YX\\nc\\x12\\nqw\\nclogqs\\nc\\nqwc+ (1−qw\\nc) log1−qs\\nc\\n1−qwc\\x13\\n(12)\\nwhere qw\\nc=p∗(c|ω(x))andqs\\nc=p(c|Ω(x)).\\nOverall Objective. Finally, we formulate the prior-\\nenhanced self-supervised learning as a combination of the\\nconsistency objective and the self-distillation objective:\\nLpessl =λcstLcst+λdstlLdstl (13)\\n3.4. Network Optimization\\nDuring training, we adopt a multi-label classification ob-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='jective over the predicted likelihood, i.e.,p(yi|x)in Eq. (7),\\nto optimize our SCPNet, denoted as Lcls. We follow [32]\\nto design Lcls. The overall objective for the network opti-\\nmization is formulated as follows:\\nL=Lcls+Lpessl (14)\\n4. Experiment\\n4.1. Experiment Settings\\nDatasets. We conduct extensive experiments on several\\nstandard benchmarks for MLR with incomplete labels, in-\\ncluding the single positive label setting and the partial la-\\nbel setting. For the single positive label setting, following[16, 32], we use MS-COCO (COCO) [19], PASCAL VOC\\n(VOC) [11], NUSWIDE (NUS) [7], and CUB [28]. For the\\npartial label learning, we adopt MS-COCO (COCO) [19],\\nPASCAL VOC 2007 (VOC2007) [10] and Visual Genome\\n(VG-200) [18], as [4, 21]. We leave details of benchmark\\ndatasets in the supplementary due to the space limit.\\nImplementation details. We leverage published CLIP\\nweights1to initialize MLR models. To fairly compare the\\nproposed method with others, we adopt the ResNet50-based', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='CLIP and the Resnet101-based CLIP for the single positive\\nlabel and the partial label, respectively. During training, we\\ntune the image encoder and fix the text encoder of CLIP.\\nMore details are provided in the supplementary.\\nEvaluation. By default, we employ the mean average\\nprecision (mAP) as the evaluation metric, following pre-\\nvious works [5, 16, 32]. For the single positive label set-\\nting, we perform two different setups, i.e., the LargeLoss\\nsetup [16] and the SPLC setup [32], which are common in\\nthe community. We leave the details in the supplementary\\ndue to the space limit. For the partial label setting, follow-\\ning [21], we randomly maintain partial labels for the train-\\ning set with a ratio ranging from 10 %to 90%. Apart from\\nperformance on all ratios, we also report the average result.\\n4.2. Comparisons with State-of-the-Arts\\nMLR with single positive labels. We report the model\\nperformance on both the LargeLoss setup [16] and the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='SPLC setup [32]. To better reveal the effectiveness of the\\nproposed method, we also report the average performance\\nfor both setups. As shown in Tab. 1, for both setups, our\\nmethod can significantly outperform existing methods on\\nall benchmark datasets, achieving state-of-the-art perfor-\\nmance. Specifically, in the LargeLoss setup, the proposed\\nSCPNet can obtain a maximal performance improvement\\nof4.7%(NUS). As a whole, our method can accomplish\\nan overall performance improvement of 3.6%. In the SPLC\\nsetup, the maximal performance improvement achieved by\\nour method can reach 6.8%(NUS). As a result, our method\\ncan accomplish 4.7%improvement on average.\\nMLR with partial labels. As shown in Tab. 2, our\\nresults also consistently surpass existing state-of-the-art\\nmethods on all benchmark datasets, especially on the\\nCOCO and VG-200. Compared with DualCoOp [26] which\\nalso leverages CLIP to build MLR models, the proposed\\nmethod can obtain an improvement of 1.9%mAP on the MS', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='COCO. With a frozen image encoder during training as Du-\\nalCoOp, our method, denoted as SCPNet (ours)*, still en-\\njoys superior performance to DualCoOp. On the VOC2007,\\nour method obtains comparable performance with 0.3%im-\\nprovement. However, under small ratios, our method shows\\nits superiority to DualCoOp, e.g.,0.8%improvement with\\na ratio of 10%. On the VG-200, compared with SARB [21]\\n1https://github.com/openai/CLIP\\n3402\\nTable 1. Comparison with the state-of-the-art methods for MLR with the single positive label (%).\\nMethodLargeLoss setup [16] SPLC setup [32]\\nCOCO VOC NUS CUB Avg. COCO VOC NUS CUB Avg.\\nLSAN [8] 69.2 86.7 50.5 17.9 56.1 70.5 87.2 52.5 18.9 57.3\\nROLE [8] 69.0 88.2 51.0 16.8 56.3 70.9 89.0 50.6 20.4 57.7\\nLargeLoss [16] 71.6 89.3 49.6 21.8 58.1 - - - - -\\nHill [32] - - - - - 73.2 87.8 55.0 18.8 58.7\\nSPLC [32] 72.0 87.7 49.8 18.0 56.9 73.2 88.1 55.2 20.0 59.1\\nSCPNet (ours) 75.4 90.1 55.7 25.4 61.7 76.4 91.2 62.0 25.7 63.8', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Table 2. Comparison with the state-of-the-art methods for MLR with partial labels (%).\\nDatasets Method 10% 20% 30% 40% 50% 60% 70% 80% 90% Avg.\\nCOCOSSGRL [5] 62.5 70.5 73.2 74.5 76.3 76.5 77.1 77.9 78.4 74.1\\nGCN-ML [6] 63.8 70.9 72.8 74.0 76.7 77.1 77.3 78.3 78.6 74.4\\nSST [4] 68.1 73.5 75.9 77.3 78.1 78.9 79.2 79.6 79.9 76.7\\nSARB [21] 71.2 75.0 77.1 78.3 78.9 79.6 79.8 80.5 80.5 77.9\\nDualCoOp [26] 78.7 80.9 81.7 82.0 82.5 82.7 82.8 83.0 83.1 81.9\\nSCPNet (ours)* 80.3 82.2 82.8 83.4 83.8 83.9 84.0 84.1 84.2 83.2\\nSCPNet (ours) 79.1 82.1 82.8 83.9 84.5 84.9 85.4 85.7 85.9 83.8\\nVOC2007SSGRL [5] 77.7 87.6 89.9 90.7 91.4 91.8 91.9 92.2 92.2 89.5\\nGCN-ML [6] 74.5 87.4 89.7 90.7 91.0 91.3 91.5 91.8 92.0 88.9\\nSST [4] 81.5 89.0 90.3 91.0 91.6 92.0 92.5 92.6 92.7 90.4\\nSARB [21] 83.5 88.6 90.7 91.4 91.9 92.2 92.6 92.8 92.9 90.7\\nDualCoOp [26] 90.3 92.2 92.8 93.3 93.6 93.9 94.0 94.1 94.2 93.2\\nSCPNet (ours) 91.1 92.8 93.5 93.6 93.8 94.0 94.1 94.2 94.3 93.5', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='VG-200SSGRL [5] 34.6 37.3 39.2 40.1 40.4 41.0 41.3 41.6 42.1 39.7\\nGCN-ML [6] 32.0 37.8 38.8 39.1 39.6 40.0 41.9 42.3 42.5 39.3\\nSST [4] 38.8 39.4 41.1 41.8 42.7 42.9 43.0 43.2 43.5 41.8\\nSARB [21] 41.4 44.0 44.8 45.5 46.6 47.5 47.8 48.0 48.2 46.0\\nSCPNet (ours) 43.8 46.4 48.2 49.6 50.4 50.9 51.3 51.6 52.0 49.4\\nwhich enhances the MLR models with a structure-aware al-\\ngorithm, our SCPNet can significantly outperform it with\\nan average performance improvement of 3.4%.\\nThese experimental results show that our method can\\nconsistently obtain superior performance in different setups\\nfor MLR with incomplete labels, well demonstrating the ef-\\nfectiveness. To verify the generalization of the proposed\\nmethod, we also investigate the effectiveness in the few-\\nshot partial label setting and the real partial label scenario.\\nWe leave them in the supplementary due to the space limit.\\n4.3. Ablation Study\\nIn order to analyze the effectiveness of each component,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='we conduct the ablation study on both the single positive\\nlabel and the partial label settings. All results are shown\\nin Tab. 3. We also introduce a model that directly employs\\nLclsto optimize a ResNet-based MLR model, as the base-\\nline. As shown in Tab. 3, each component can obtain con-\\nsistent performance improvement in all datasets. Specifi-\\ncally, compared with the baseline model, our CMP can ob-\\ntain an average performance of 1.99% mAP, indicating thesuperiority of prompting a cross-modality vision-language\\nmodel. Augmented by SAM, our method can bring 0.69%\\nmAP improvement. Such improvements can be attributed\\nto the explicit semantic correspondence among labels cap-\\ntured by the proposed SAM component. Besides, the con-\\nsistency learning, i.e.,Lcst, and the self-distillation objec-\\ntive, i.e.,Ldstl, can lead to 1.25% and1.56% performance\\nimprovement, respectively. The overall improvement for\\nthe proposed PESSL can reach 2.09%, well demonstrating', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the strength of incorporating the structured semantic prior\\nduring model optimization. Finally, our proposed SCPNet\\ncan significantly outperform the baseline model with 4.77%\\nmAP improvement on average, well demonstrating the ef-\\nfectiveness and the superiority of the proposed method.\\n4.4. Model Analysis\\nHere, we perform comprehensive inspections for the pro-\\nposed method. All experiments are conducted in the single\\npositive label setting on the MS COCO dataset, by default.\\nDue to the space limit, we provide more analyses in the sup-\\nplementary material.\\n3403\\nTable 3. Effect of different modules in the proposed SCPNet method for both the single positive label setting and the partial label setting\\n(%). An average of all metrics is also reported.\\nModel CMP SAMPESSL Single Positive Label Partial LabelAvg.LcstLdstl COCO VOC NUS CUB COCO VOC2007 VG-200\\nBaseline 73.18 88.07 55.18 19.99 77.41 88.32 46.39 64.08\\nSCPNet✓ 74.36 88.46 60.66 21.42 80.90 89.16 47.55 66.07', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='✓ ✓ 75.12 89.09 61.08 21.66 82.12 90.16 48.11 66.76\\n✓ ✓ ✓ 75.70 90.92 61.75 23.67 82.85 92.50 48.70 68.01\\n✓ ✓ ✓ 75.84 90.92 61.56 24.51 83.35 93.21 48.83 68.32\\n✓ ✓ ✓ ✓ 76.42 91.16 62.04 25.71 83.76 93.49 49.36 68.85\\ncow\\nelephantzebragiraﬀebustraintruck boatcow\\nelephant\\nzebra\\ngiraﬀe\\nbus\\ntrain\\ntruck\\nboat\\n0.20.40.60.81.0\\nBaseline\\nperson\\nvehicleoutdoor\\nanimalaccessory\\nsportskitchen\\nfoodfurniture\\nelectronicappliance\\nindoorSCPNet\\nFigure 3. The structured semantic prior (left) and the learnt label representation (middle: in the baseline, right: in our SCPNet).\\nTable 4. Analysis on the correlation graph.\\nSAM PESSL mAP (%)\\nStaticStatic 76.42\\nDynamic 76.05\\nNo 75.83\\nDynamicStatic 76.08\\nDynamic 75.84\\nTable 5. Analysis on the prior extraction (%).\\nPrior Dynamic Image Glove BERT CLIP\\nmAP 75.84 75.67 76.15 76.16 76.42\\nCorrelation graph construction. We verify the positive\\neffect of the prior used in the correlation graph construction\\nfor both SAM and PESSL. To achieve this goal, we discuss', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='two kinds of correlation graph: 1) a static one derived from\\nthe pretrained CLIP model (see Eq. (4)), which captures the\\nstructured semantic prior, and 2) a dynamic one achieved by\\nthe learnable CMP, i.e., constructing the adjacency matrix\\nwith label features zicomputed by Eq. (5). We also report\\nPESSL without the prior, denoted as “No”. As illustrated\\nin Tab. 4, we can observe that our method can obtain the\\noptimal performance by using the static correlation graph\\nfor both SAM and PESSL. Besides, the static graph cansubstantially achieve better results than the dynamic one in\\nboth components, revealing the advantage of the structured\\nsemantic prior. We claim that in the MLR with incomplete\\nlabels, the challenge of insufficient label supervision makes\\nthe dynamic graph sub-optimal, thus inferior to the static\\none. By comparing PESSL with the prior (Row 2) and the\\none without the prior (Row 4), we can find that the latter\\nachieves inferior performance, which can demonstrate the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='benefit of the proposed prior, i.e.,A∗in Eq. (4).\\nPrior knowledge extraction. We further investigate the\\nadvantage of the proposed structured semantic prior ex-\\ntracted by CLIP with three other types of prior knowledge\\nas competitors. For a given label, 1) “Image” averages all\\nimage features corresponding to it; 2) “Glove” represents\\nits feature by pretrained Glove word embeddings [20]; and\\n3) “BERT” extracts the label feature by the prompt learning\\nas ours. We also report the result of dynamic label-to-label\\ncorrespondence as the baseline. As shown in Tab. 5, com-\\npared with Dynamic, except Image, Glove, BERT and CLIP\\nshow consistent advantage because of their superior abil-\\nity to capture the label-to-label correspondence. Besides,\\nour CLIP achieves the best performance, which reveals that\\nthe CLIP-based structured prior is more matched with the\\nCLIP-based MLR model due to their consistent knowledge.\\nGeneralization on the CNN-based architecture. To', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='show the generalization of the proposed prior-enhanced\\nmethod, we transfer our design principles to a vanilla\\n3404\\nTable 6. Prior for MLR models with the ImageNet-based ResNet.\\nImage Encoder Label Encoder mAP (%)\\nResNet sigmoid 73.18\\nResNet Ours 74.72\\nOurs Ours 76.42\\nTable 7. Analysis on the number of GCN Layer, i.e.,L(%).\\nL 2 3 4\\nmAP 75.88 76.42 76.22\\nTable 8. Analysis on λcstandλdstl(%).\\nλcst 0 1/16 1/8 1/4 1/8\\nλdstl 0 1/8 2/8 3/8\\nmAP 75.12 75.56 75.70 75.13 76.42 76.40 76.17\\nResNet-based MLR model with a sigmoid layer as the label\\nencoder. We analyze the impact of replacing the sigmoid\\nlayer with ours. As shown in Tab. 6, such modification can\\nresult in a performance gain of 1.54%, demonstrating the\\ngood generalization ability of the proposed method in the\\nCNN-based architecture.\\nAnalysis on hyper-parameters. As shown in Tab. 7 and\\nTab. 8, the best value of L,λcstandλdstlis atL= 3,\\nλcst= 1/8, and λdstl= 1/8, respectively. More analyses\\ncan be found in the supplementary material.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='4.5. More Insightful Analysis\\nTo provide more insights about the effectiveness of the\\nproposed method, we conduct visualization analyses on the\\nstructured semantic prior and the label representation in the\\nlatent feature space. First, we present the structured se-\\nmantic prior about the label-to-label correspondence by vi-\\nsualizing the adjacency matrix, i.e.,A∗in Eq. (4) on MS\\nCOCO. For ease of explanation, we select two categories,\\ni.e., animal and vehicle, and investigate the label correspon-\\ndence among labels associated with them. As shown in\\nFig. 3, the used structured semantic prior can successfully\\nconvey the similarity among labels although the CLIP is not\\nencouraged in the contrastive learning on the text. Second,\\nwe visualize the label features in the baseline (weights in\\nthe sigmoid layer) and our SCPNet (output of the SAM,\\ndenoted as zi∗in Eq. (7)). We can observe that labels be-\\nlonging to the same category are more well-aligned together', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='in our SCPNet, compared with those in the baseline model.\\nThis result indicates that our SCPNet can reasonably derive\\nmore discriminative label representations due to the appli-\\nance of the structured semantic prior.\\nTo verify the effect of the proposed structured semantic\\nprior on the issue of insufficient label supervision, we intro-\\n0 5 10 15 20\\nepoch0.00.20.40.60.8precision\\nSCPNet\\nCMP+Lcst\\nSASC(CMP+Lcst)\\n6 8 10 12 14 16 18 20\\nepoch626466687072747678mAP\\nSCPNet\\nCMP+LcstFigure 4. The precision on the training set (left) and the mAP on\\nthe test set (right).\\nduce a competitor model, i.e., CMP+ Lcst, which wipes out\\ncomponents involving the prior, i.e.,A∗in Eq. (4). We keep\\ntrack of the precision of model predictions on the training\\nset and the mAP result on the test set after each training\\nepoch. For CMP+ Lcst, we also visualize the precision over\\nits calibrated predictions by the SASC (·)function. As il-\\nlustrated in Fig. 4 (left), compared with CMP+ Lcst, both', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='SASC (CMP +Lcst)and our SCPNet can obtain consistent\\nimprovements in terms of the label prediction precision. It\\nindicates that the quality of label supervision can be pro-\\nmoted under the guidance of the proposed prior, thus bene-\\nfiting the performance on the test set (see Fig. 4 (right)).\\n5. Conclusion\\nIn this paper, we drive a structured semantic prior about\\nthe label-to-label correspondence from the vision-language\\nmodel, i.e., CLIP [22]. To mitigate the deficiency of label\\nsupervision for MLR with incomplete labels, we introduce a\\nsemantic correspondence prompt network, dubbed SCPNet,\\nwhich can explore such a structured semantic prior. It con-\\nstructs a cross-modality prompter to leverage the explicit\\nimage-to-label correspondence in the CLIP. A semantic as-\\nsociation module is equipped to associate related labels with\\nthe help of such a meaningful structured semantic prior.\\nFurthermore, we propose a prior-enhanced self-supervised', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='learning method for network optimization. Experimental\\nresults on a series of benchmark datasets for MLR with in-\\ncomplete labels show that our method can achieve state-of-\\nthe-art performance on both the partial label setting and the\\nsingle positive label setting, well demonstrating its effec-\\ntiveness and superiority. In the future, we will further study\\nhow to generalize our method to tackle other practical prob-\\nlems, e.g., the domain gap.\\nAcknowledgement. This work was supported by “Pi-\\noneer” and “Leading Goose” R&D Program of Zhejiang\\n(No. 2023C01038), National Natural Science Foundation\\nof China (Nos. 62271281, 61773301), Zhejiang Provin-\\ncial Natural Science Foundation of China under Grant (No.\\nLDT23F01013F01), China Postdoctoral Science Founda-\\ntion (No. BX2021161) and Shanxi Innovation Team Project\\n(No. 2018TD-012).\\n3405\\nReferences\\n[1] Emanuel Ben-Baruch, Tal Ridnik, Itamar Friedman, Avi\\nBen-Cohen, Nadav Zamir, Asaf Noy, and Lihi Zelnik-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Manor. Multi-label classification with partial annotations\\nusing class-aware selective loss. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 4764–4772, 2022. 2\\n[2] Dolly Carrillo, Vivian F L ´opez, and Mar ´ıa N Moreno. Multi-\\nlabel classification for recommender systems. Trends in\\nPractical Applications of Agents and Multiagent Systems ,\\npages 181–188, 2013. 1\\n[3] Tianshui Chen, Liang Lin, Xiaolu Hui, Riquan Chen, and\\nHefeng Wu. Knowledge-guided multi-label few-shot learn-\\ning for general image recognition. IEEE Transactions on\\nPattern Analysis and Machine Intelligence , 2020. 1\\n[4] Tianshui Chen, Tao Pu, Hefeng Wu, Yuan Xie, and Liang\\nLin. Structured semantic transfer for multi-label recognition\\nwith partial labels. In Proceedings of the AAAI conference\\non artificial intelligence , volume 36, pages 339–346, 2022.\\n2, 4, 5, 6\\n[5] Tianshui Chen, Muxin Xu, Xiaolu Hui, Hefeng Wu, and\\nLiang Lin. Learning semantic-specific graph representa-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tion for multi-label image recognition. In Proceedings of\\nthe IEEE/CVF international conference on computer vision ,\\npages 522–531, 2019. 1, 5, 6\\n[6] Zhao-Min Chen, Xiu-Shen Wei, Peng Wang, and Yanwen\\nGuo. Multi-label image recognition with graph convolu-\\ntional networks. In Proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition , pages 5177–\\n5186, 2019. 2, 3, 6\\n[7] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhip-\\ning Luo, and Yantao Zheng. Nus-wide: a real-world web im-\\nage database from national university of singapore. In Pro-\\nceedings of the ACM international conference on image and\\nvideo retrieval , pages 1–9, 2009. 2, 5\\n[8] Elijah Cole, Oisin Mac Aodha, Titouan Lorieul, Pietro Per-\\nona, Dan Morris, and Nebojsa Jojic. Multi-label learning\\nfrom single positive labels. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition ,\\npages 933–942, 2021. 1, 2, 3, 6\\n[9] Thibaut Durand, Nazanin Mehrasa, and Greg Mori. Learn-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ing a deep convnet for multi-label classification with partial\\nlabels. In Proceedings of the IEEE/CVF conference on com-\\nputer vision and pattern recognition , pages 647–657, 2019.\\n2\\n[10] Mark Everingham, Luc Van Gool, Christopher KI Williams,\\nJohn Winn, and Andrew Zisserman. The pascal visual object\\nclasses (voc) challenge. International journal of computer\\nvision , 88(2):303–338, 2010. 5\\n[11] Mark Everingham and John Winn. The pascal visual object\\nclasses challenge 2012 (voc2012) development kit. Pattern\\nAnal. Stat. Model. Comput. Learn., Tech. Rep , 2007:1–45,\\n2012. 2, 5\\n[12] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-\\ntrained language models better few-shot learners. arXiv\\npreprint arXiv:2012.15723 , 2020. 3[13] Dat Huynh and Ehsan Elhamifar. Interactive multi-label cnn\\nlearning with partial labels. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition ,\\npages 9423–9432, 2020.\\n[14] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neu-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='big. How can we know what language models know? Trans-\\nactions of the Association for Computational Linguistics ,\\n8:423–438, 2020. 3\\n[15] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina\\nToutanova. Bert: Pre-training of deep bidirectional trans-\\nformers for language understanding. In Proceedings of\\nNAACL-HLT , pages 4171–4186, 2019. 3\\n[16] Youngwook Kim, Jae Myung Kim, Zeynep Akata, and Jung-\\nwoo Lee. Large loss matters in weakly supervised multi-\\nlabel classification. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition , pages\\n14156–14165, 2022. 1, 2, 3, 4, 5, 6\\n[17] Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami\\nAbu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper Ui-\\njlings, Stefan Popov, Andreas Veit, et al. Openimages: A\\npublic dataset for large-scale multi-label and multi-class im-\\nage classification. Dataset available from https://github.\\ncom/openimages , 2(3):18, 2017. 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[18] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\\ntidis, Li-Jia Li, David A Shamma, et al. Visual genome:\\nConnecting language and vision using crowdsourced dense\\nimage annotations. International journal of computer vision ,\\n123(1):32–73, 2017. 5\\n[19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\\nPietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence\\nZitnick. Microsoft coco: Common objects in context. In\\nEuropean conference on computer vision , pages 740–755.\\nSpringer, 2014. 2, 5\\n[20] Jeffrey Pennington, Richard Socher, and Christopher Man-\\nning. Glove: Global vectors for word representation. In\\nEMNLP , pages 1532–1543, 2014. 3, 7\\n[21] Tao Pu, Tianshui Chen, Hefeng Wu, and Liang Lin.\\nSemantic-aware representation blending for multi-label im-\\nage recognition with partial labels. arXiv preprint\\narXiv:2203.02172 , 2022. 1, 2, 4, 5, 6\\n[22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\\ning transferable visual models from natural language super-\\nvision. In International Conference on Machine Learning ,\\npages 8748–8763. PMLR, 2021. 1, 2, 3, 4, 8\\n[23] Taylor Shin, Yasaman Razeghi, Robert L Logan IV , Eric\\nWallace, and Sameer Singh. Autoprompt: Eliciting knowl-\\nedge from language models with automatically generated\\nprompts. arXiv preprint arXiv:2010.15980 , 2020. 3\\n[24] Josef Sivic and Andrew Zisserman. Video google: Efficient\\nvisual search of videos. In Toward category-level object\\nrecognition , pages 127–144. Springer, 2006. 1\\n[25] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao\\nZhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk,\\nAlexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying\\n3406\\nsemi-supervised learning with consistency and confidence.\\nAdvances in neural information processing systems , 33:596–\\n608, 2020. 4', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='608, 2020. 4\\n[26] Ximeng Sun, Ping Hu, and Kate Saenko. Dualcoop: Fast\\nadaptation to multi-label recognition with limited annota-\\ntions. arXiv preprint arXiv:2206.09541 , 2022. 1, 2, 3, 4,\\n5, 6\\n[27] Ivona Tautkute, Tomasz Trzci ´nski, Aleksander P Skorupa,\\nŁukasz Brocki, and Krzysztof Marasek. Deepstyle: Multi-\\nmodal search engine for fashion and interior design. IEEE\\nAccess , 7:84613–84628, 2019. 1\\n[28] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-\\nona, and Serge Belongie. The caltech-ucsd birds-200-2011\\ndataset. 2011. 2, 5\\n[29] Ya Wang, Dongliang He, Fu Li, Xiang Long, Zhichao Zhou,\\nJinwen Ma, and Shilei Wen. Multi-label classification with\\nlabel graph superimposing. In Proceedings of the AAAI Con-\\nference on Artificial Intelligence , volume 34, pages 12265–\\n12272, 2020. 1, 2\\n[30] Vacit Oguz Yazici, Abel Gonzalez-Garcia, Arnau Ramisa,\\nBartlomiej Twardowski, and Joost van de Weijer. Orderless\\nrecurrent models for multi-label classification. In Proceed-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition , pages 13440–13449, 2020. 1, 2\\n[31] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jin-\\ndong Wang, Manabu Okumura, and Takahiro Shinozaki.\\nFlexmatch: Boosting semi-supervised learning with curricu-\\nlum pseudo labeling. Advances in Neural Information Pro-\\ncessing Systems , 34:18408–18419, 2021. 4, 5\\n[32] Youcai Zhang, Yuhao Cheng, Xinyu Huang, Fei Wen, Rui\\nFeng, Yaqian Li, and Yandong Guo. Simple and robust loss\\ndesign for multi-label learning with missing labels. arXiv\\npreprint arXiv:2112.07368 , 2021. 2, 3, 5, 6\\n[33] Yong Zheng, Bamshad Mobasher, and Robin Burke. Context\\nrecommendation using multi-label classification. In 2014\\nIEEE/WIC/ACM International Joint Conferences on Web In-\\ntelligence (WI) and Intelligent Agent Technologies (IAT) ,\\nvolume 2, pages 288–295. IEEE, 2014. 1\\n[34] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\\nLiu. Conditional prompt learning for vision-language mod-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='els. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition , pages 16816–16825,\\n2022. 2\\n[35] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\\nLiu. Learning to prompt for vision-language models. In-\\nternational Journal of Computer Vision , 130(9):2337–2348,\\n2022. 2, 4\\n3407', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='3D shape reconstruction of semi-transparent worms\\nThomas P. Ilett*Omer Yuval*Thomas Ranner*Netta Cohen*†David C. Hogg*†\\nUniversity of Leeds, Leeds, United Kingdom\\nFigure 1. Posture reconstruction pipeline and imaging setup.\\nAbstract\\n3D shape reconstruction typically requires identifying\\nobject features or textures in multiple images of a sub-\\nject. This approach is not viable when the subject is semi-\\ntransparent and moving in and out of focus. Here we over-\\ncome these challenges by rendering a candidate shape with\\nadaptive blurring and transparency for comparison with\\nthe images. We use the microscopic nematode Caenorhab-\\nditis elegans as a case study as it freely explores a 3D\\ncomplex fluid with constantly changing optical properties.\\nWe model the slender worm as a 3D curve using an in-\\ntrinsic parametrisation that naturally admits biologically-\\ninformed constraints and regularisation. To account for\\nthe changing optics we develop a novel differentiable ren-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='derer to construct images from 2D projections and compare\\n*{T.Ilett, O.Yuval, T.Ranner, N.Cohen,\\nD.C.Hogg }@leeds.ac.uk\\nFunding This work was supported by University of Leeds and EPSRC.\\nAuthor contributions Conceptualisation, Methodology, Formal analysis,\\nInvestigation, Software, Visualisation: TPI. Data curation, Validation:\\nTPI, OY . Writing: TPI (original), all (review and editing). Funding\\nacquisition, Supervision: NC, DCH, TR. †Equal contribution.\\nAcknowledgements Additional thanks to Matan Braunstein (for help with\\nFig. 1), Robert I. Holbrook (data), Felix Salfelder (discussions and data),\\nLukas Deutz (discussions) and Jen Kruger (proof reading).\\nData availability Supplementary movies are available here:\\nhttps://doi.org/10.6084/m9.figshare.22310650 .against raw images to generate a pixel-wise error to jointly\\nupdate the curve, camera and renderer parameters using\\ngradient descent. The method is robust to interference such', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='as bubbles and dirt trapped in the fluid, stays consistent\\nthrough complex sequences of postures, recovers reliable\\nestimates from blurry images and provides a significant im-\\nprovement on previous attempts to track C. elegans in 3D.\\nOur results demonstrate the potential of direct approaches\\nto shape estimation in complex physical environments in the\\nabsence of ground-truth data.\\n1. Introduction\\nMany creatures such as fish, birds and insects move in all\\ndirections to search and navigate volumetric environments.\\nAcquiring 3D data of their motion has informed models of\\nlocomotion, behaviour and neural and mechanical control\\n[3,22]. While technological advances have made the collec-\\ntion of large quantities of multi-viewpoint visual data more\\nattainable, methods for extracting and modelling 3D in-\\nformation remain largely domain-dependant as few species\\nshare common geometric models or exist within the same\\nspatial and temporal scales [4, 11, 14, 26, 37, 41, 50, 54, 65].', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Furthermore, while humans and some domesticated ani-\\nmals [30,60] may act naturally while wearing special mark-\\ners, marker-less observations of many species makes fea-\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n12565\\nture extraction more challenging and means pose estimation\\ngenerally lacks ground-truth data [48].\\nAs a case study in marker-less 3D shape reconstruction,\\nwe consider C. elegans , a hair-thick, ∼1 mm long animal\\nwith a simple tapered cylinder shape, which can be con-\\nstructed from a midline “skeleton”. In the wild, C. elegans\\ncan be found in a wide range of complex 3D environments,\\ne.g. decomposing organic matter, with continually changing\\nphysical properties [15, 17, 46]. However, to date, experi-\\nments have focused nearly exclusively on locomotion on a', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='plane, limiting insight to the constrained, planar behaviours.\\nWe obtained a large dataset (4 hours 53 minutes ≃\\n440,000 frames at 25Hz) of experimental recordings of in-\\ndividual worms moving freely inside a glass cube filled\\nwith a gelatin solution. The cube is positioned between\\nthree nearly-orthogonal static cameras fitted with telecentric\\nlenses. Initial pinhole camera model parameter estimates\\nare provided [45] but are imprecise and require continuous\\nadjustment across the course of a recording to account for\\nsmall vibrations and optical changes to the gel. We aim to\\nsimultaneously reconstruct a 3D shape and find corrected\\ncamera parameters to match these recordings in a process\\nakin to bundle adjustment [56].\\n3D reconstruction typically involves the identification\\nand triangulation of common features from multiple view-\\npoints or the synthesis of full images including texture and\\nshading information to match given scenes [16, 21, 47, 66].', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Imaging animals with length ∼1 mm requires sufficient\\nmagnification, but simultaneously capturing long-term tra-\\njectories up to 25 minutes requires a large volume of view\\n(10-20 worm lengths per axis). As the worm explores the\\ncube it frequently appears out of focus in one or more of\\nthe cameras. Air bubbles and dirt trapped in the gel along\\nwith old tracks are difficult to differentiate from the trans-\\nparent worm, particularly at the tapered ends. Self occlu-\\nsion invariably appears in a least one view, where hidden\\nparts darken the foreground while the ordering of fore/back-\\nparts is not discernible. As the semi-transparent and self-\\noccluding subject moves in the volume, photometric infor-\\nmation in one view bears little relevance to the appearance\\nin the others making feature identification and photometric\\nmatching particularly challenging. We found that standard\\napproaches may suffice for limited sub-clips, but lose parts', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='of the object or fail catastrophically for much of the data\\nand the solution requires a degree of adaptation.\\nWe present an integrated “project-render-score” algo-\\nrithm to obtain a midline curve for each image-triplet\\n(Fig. 1). Discrete curve vertices are projected through a\\ntriplet of pinhole camera models, rendered to produce an\\nimage-triplet for direct comparison against the recorded im-\\nages and scored according to their intersection with worm-\\nlike pixels in all three views. The differentiable renderer\\nstacks 2D super-Gaussian blobs at the projected locationsof each vertex to approximate the transparency along the\\nworm, accounting for the variable focus and providing soft\\nedges that direct the geometric model towards the midline.\\nThe scoring allows the detection of incongruities and keeps\\nthe curve aligned to the worm in all views. Regularisation\\nterms ensure smoothness along the body and in time. Curve,\\ncamera and rendering parameters are jointly optimised us-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ing gradient descent to convergence. Once the worm shape\\nhas been resolved, it is generally only lost during image\\ndegradation or significant self-occlusions that make the pos-\\nture unresolvable by eye.\\nIn summary, our main contributions are:\\n• A robust pipeline for 3D posture reconstruction of a\\nfreely deforming semi-transparent object from noisy\\nimages.\\n• A novel viewpoint renderer to capture optical distor-\\ntions and transparency.\\n• A feature-free bundle adjustment algorithm using di-\\nrect image comparison and gradient descent.\\n2. Related work\\nBundle adjustment (BA) is a procedure to jointly optimise\\n3D geometry and camera parameters [21, 56]. BA typically\\nidentifies common features of an object from multiple view-\\npoints in order to minimise a prediction error between pro-\\njections of the corresponding 3D points and their 2D ob-\\nservations. BA is frequently used in conjunction with other\\nmethods to find camera parameters using multiple images', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='of a 3D calibration object with known control points or for\\nfine-tuning results [13, 23, 36, 40, 57, 59].\\nFeature detection converts photometric information into\\nimage coordinates. In BA, coordinates of common features\\nare used to solve a geometric optimisation problem. Photo-\\nmetric bundle adjustment methods additionally require ob-\\njects to have the same appearance in all views [12, 18]. Our\\nmethod is entirely photometric, as such differing from BA.\\nAs our objects appear differently across views, all pixel in-\\nformation is used and the geometry is solved intrinsically.\\nPose estimation Deep network approaches have proved\\nwell-suited to 2D human-pose estimation as they are po-\\ntent feature extractors and large annotated training sets are\\navailable [1, 51, 55]. For 3D postures, ground truth multi-\\nview datasets are less common. Recent progress [35] re-\\nlies on end-to-end architectures [19, 27, 29, 32, 42, 61] or\\nsplitting the problem into 2D pose estimation and then con-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='structing the 3D pose [10, 38]. Despite similar approaches\\nused for non-human pose estimation, the huge variability\\nin scales and shapes among species introduces a variety of\\nchallenges [26]. Motion capture in controlled settings with\\nmarkers (providing ground truth skeleton and joint angle\\ndata for humans, horses and dogs [30,60]), are not available\\nfor most animals. Generalised mesh surfaces may be used,\\n12566\\nbut often require multiple views and thousands of parame-\\nters, and do not guarantee consistency through time. In con-\\ntrast, approximating an animal shape using a few-parameter\\nmorphable model can be both tractable and robust. Suc-\\ncessful examples include swimmers [9, 43], birds [27, 58],\\nmammals [2,6,28,39] and generic quadrupeds [7,67]. How-\\never, these methods expect opaque subjects with consistent\\ntextural appearances between views.\\nC. elegans has a simple geometric shape that can be well\\nreconstructed from a midline skeleton and parametrised by', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='curvature values along the body (see Sec. 3). This is the\\ndeformable template we look to fit to the data. Despite\\nthe apparent simplicity, each vertex of the discretised curve\\nhas two degrees of freedom (two curvature values) and as\\nwe use 128 vertices, our model is highly deformable and\\nrequires many parameters (although smoothness regulari-\\nsation simplifies the problem somewhat). In contrast to\\ndeep-learning approaches, our model includes only a small\\nnumber of explainable parameters and direct optimisation\\navoids lengthy training and dataset requirements.\\nC. elegans Numerous freely available software packages\\nare capable of simultaneous tracking and skeletonising sin-\\ngle or multiple worms in 2D using inexpensive microscopic\\nimaging [5, 25, 44, 52, 53, 62] (see [24] for a review). Most\\nof these skeletonisers combine image segmentation to sep-\\narate the animal from the background with thinning of the\\nmask to some midline pixels and fitting a spline.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='The 3D reconstruction problem has received relatively\\nlittle attention. Using at first two views [34] and then three,\\nKwon et al. [33] designed a motorised stage coupled with a\\nreal-time tracker to keep a worm in focus under high magni-\\nfication in a 3D environment while capturing trajectories of\\nup to 3 minutes. Thresholded images are lifted into 3D, in-\\ntersected in voxel space and thinned [20] to produce a final\\nskeleton. Kwon et al. omit camera modelling and assume\\nperfectly parallel projections – assumptions that result in\\nlarge errors for the data we use. Shaw et al. [49] employed\\nlight field microscopy to generate depth maps alongside im-\\nages from a single viewpoint. A midline skeleton is gener-\\nated by fitting a spline to the 3D coordinates of the central\\nvoxels. However, self-occlusions cannot be resolved and\\nonly relatively planar postures were investigated.\\nSalfelder et al. [45] and Yuval [63] both present 3D re-\\nconstruction algorithms using the three-camera set up and', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='calibration described in [45]. In Salfelder et al. [45], a neu-\\nral network is trained to identify 2D midlines from individ-\\nual camera images before lifting into 3D voxel space. To ac-\\ncount for changing camera parameters, a relative axial shift\\n(dx, dy, dz )is optimised for each frame-triplet to maximise\\nthe voxel intersection before thinning. Remaining voxel co-\\nordinates are used as control points to fit a curve using a\\nfinite-element formulation. This approach works well whenthe midline is well detected in each of the views, but can\\nfail on occluded postures or low-resolution, blurry images.\\nYuval [63] uses a neural network to track head and tail\\npoints in 3D lab coordinates and a curve is fit between these\\nfixed end points using a hill-climbing optimisation algo-\\nrithm. Scoring is based on curve smoothness and pixel in-\\ntensities at the projected curve points. This method works\\nwell when the head and tail are correctly identified but', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='struggles, or requires manual correction, otherwise.\\nIn our approach we find that incorporating the camera\\nmodel parameters into the optimisation results in more ro-\\nbust and accurate results. This extends the idea proposed\\nin Salfelder et al. [45] that adjusting the relative positions\\nof the cameras could result in large gains in accuracy. It\\nis likely that the relative shift adjustments, presented there,\\naccount for the changing optical properties.\\n3. Geometric model\\nNematode shapes can be well approximated by a tapered\\ncylinder and computed from a midline. We construct the\\nmidline curve in 3D using an object-centric parametrisa-\\ntion, separating shape from position and orientation to al-\\nlow us to easily constrain and regularise the shape to stay\\nwithin biologically-reasonable bounds. We discretise the\\ncurve into Nequidistant vertices and encode the posture in\\ncurvature K∈RN×2and length l∈Rthat fully define the\\nshape up to a rigid-body transformation.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='We express the 3D curve using the Bishop frame [8],\\ngiven by TM1M2where Tis the normalised tangent of the\\ncurve and M1, M2form an orthogonal basis along the mid-\\nline. At vertex n, the curvature is Kn= (m1\\nn, m2\\nn), where\\nm1\\nn, m2\\nn∈Rare the curvature components along M1, M2.\\n(The more familiar Frenet frame is less stable as it is unde-\\nfined at zero-curvature points.) Numerical integration of a\\nsystem of difference equations from starting point Pinitand\\ninitial orientation (Tinit, M1\\ninit, M2\\ninit)yields the curve path\\nP∈RN×3. See supplementary material (SM) for details.\\nDuring optimisation, errors accumulate near the starting\\npoint, Pinit, resulting in either parts of the curve moving\\nfaster than other or kinks developing (even with strong regu-\\nlarisation). To resolve this we sample an initial vertex index\\nn0from a Gaussian distribution (subject to rounding) cen-\\ntred at the middle index at every optimisation step. Setting\\nthe starting point Pinit=Pn0has the effect of continually', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='shifting the discontinuity so kinks are never given the op-\\nportunity to develop (Fig. 2). Summarising the integration\\nasF, the 3D curve is generated from the parameters:\\n(ˆP,ˆT,ˆM1) =F\\x00\\nPn0, Tn0, M1\\nn0, K, l, n 0\\x01\\n. (1)\\nEach gradient update adjusts all curvature values Kbut\\nthe position and orientation only at the randomly selected\\nn0vertex (Pn0, Tn0, M1\\nn0). Updating (P, T, M1)at only\\n12567\\nFigure 2. The 3D curve is traced out from initial point Pn0and orientation frame (Tn0, M1\\nn0, M2\\nn0). The index n0of the initial point is\\ndrawn from a normal distribution at each iteration to prevent kinks developing through repeated use of the same starting point. The final\\ncurve ˆPis computed in two parts by integrating the Bishop equations with curvature Ktowards the head and tail separately.\\nthis vertex produces a Pthat is inconsistent with the up-\\ndated K. Therefore, after applying gradient updates we\\nre-compute the full curve and orientation from n0and set', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='(P, T, M1)to the output (ˆP,ˆT,ˆM1).\\nSince the curve describes a biological creature, we con-\\nstrain the length lto(lmin, lmax)and limit the curvature by\\n|Kn|<2πkmax. The values of (lmin, lmax)we use vary de-\\npending on magnification but the bounds do not need to\\nbe tight and are in the range 0.5–2 mm . The curvature\\nconstraint kmaxis set by considering the number of circle\\nachieved by a constant curvature curve and is fixed at 3.\\n4. Project, Render, Score\\nThe core of the optimisation pipeline is separable into\\nthree main stages; project, render and score. The 3D curve\\nˆPgenerated in Eq. (1) is projected through the camera mod-\\nels into 2D points that are rendered into images and then\\nscored against the three views.\\n4.1. Project\\nThe cameras are modelled using a triplet of pinhole cam-\\nera models with tangential and radial distortion that project\\n3D points into image planes using perspective transforma-\\ntions. Each pinhole camera model offers a simple (15', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='parameters, {ηc}), tractable, approximation to the optical\\ntransformation. We also include relative shifts along the lo-\\ncal coordinate axes, ηs= (dx, dy, dz ), shared between the\\nthree models, as proposed by Salfelder et al. [45]. Initial\\ncamera coefficients for the triplet-model are provided along\\nwith the recordings and typically give root mean squared\\nreprojection errors up to 10 pixels ( ∼ O(worm radius )).\\nDue to the initial calibration errors and changes in optical\\nproperties as the gelatin sets and is disturbed by the worms\\nwe re-calibrate the cameras at every frame by including the\\ncamera parameters in the optimisation step. To avoid an\\nunder-determined problem, after we have found a config-\\nuration that supports good reconstructions for a recording\\nFigure 3. The rendering stage generates super-Gaussian blobs at\\neach vertex position on the image. The shape of the blobs depends\\non the optimisable parameters: the scale σ, the intensity ιand the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='exponent used in the Gaussian ρ.σandιare tapered down to\\nfixed minimum values at the head and tail. The effects of vary-\\ning these parameters from a converged solution (blue curves) are\\nshown above (green curves) and below (orange curves) each.\\nwe fix all but the ηsparameters. Interestingly, we still see\\nchanges (up to 30px∼0.15 mm ) inηsbut as this relates to\\nthe relative positioning it does not affect the posture recon-\\nstruction or long-term trajectories.\\nProjecting the 3D curve ˆPthrough the camera-triplet\\nmodel Γwith parameters η={η0, η1, η2, ηs}generates\\n2D image points per view, which we combine as Q=\\nΓ(ˆP, η)∈R3×N×2.\\n4.2. Render\\nIn order to evaluate the reconstruction directly against\\nthe raw data, we render the projected 2D midline points into\\n12568\\nimages using optimisable shape and rendering parameters.\\nSince worm bodies are well approximated by tapered cylin-\\nders, in theory we only require maximum and minimum ra-\\ndius values and a tapering function. However, C. elegans', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='are semi-transparent – increasingly so at the head and tail\\n– and their internal anatomy has varying optical properties\\nthat diffract and distort the light. These challenges are fur-\\nther exacerbated by the worms often being out of focus in at\\nleast one of the views, therefore even an anatomically accu-\\nrate model stands little chance of being correctly resolved.\\nWe render realistic images by combining 2D super-\\nGaussian functions centred on each projected vertex. Cru-\\ncially, we allow the rendering parameters to differ between\\ncameras since the animal seldom has the same photometric\\nqualities in different views. We optimise three parameters\\nfor each camera view c:σc∈Rcontrols the spread, ιc∈R\\nscales the intensity, and ρc∈Rsharpens or softens the\\nedges (Fig. 3). To capture the tapered shape we weight σc\\nandιcfrom their optimisable values along the middle 60%\\nto minimum values σminandιminat the ends and define the\\ntapered outputs ¯σc∈RNand¯ιc∈RN(SM). σminandιmin', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='are manually fixed for each recording to account for differ-\\nent magnification factors and worm size variability.\\nFor each camera index cand vertex index nwe define\\nthe rendered blob Bc,n∈Rw×w(image size w) for pixel\\n(i, j)as:\\nBc,n(i, j) = ¯ιc,nexp\\x14\\n−\\x12(i−Qc,n,0)2+ (j−Qc,n,1)2\\n2¯σ2c,n\\x13ρc\\x15\\n.\\n(2)\\nThe stacks of blobs are combined to generate the complete\\nrenderings R∈R3×w×wby taking the maximum pixel\\nvalue across all blobs: for pixel (i, j),\\nRc(i, j) = max {Bc,n(i, j)}n=0,...,N−1. (3)\\nThe orientation of the body directly affects the pixel in-\\ntensity of both raw and rendered images. When pointing\\ndirectly at a camera the peaks of the blobs cluster closely to-\\ngether and appear as a high-intensity (opaque) circle. Point-\\ning laterally causes the peaks to spread out on the image re-\\nvealing more of the lower-intensity tails. In both situations\\nour blob-rendering approach approximates transparency ef-\\nfects in the raw images without the need to model complex', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='intensity-orientation responses. Moreover, super-Gaussian\\nblobs allow sharp outlines to be produced in one view by\\nusing a large exponent and flat-top blobs, and blurry images\\nto be produced for another, using low intensity and high\\nvariance.\\n4.3. Score\\nIn order to evaluate how well the curve represents the\\nworm we require a way of distinguishing between worm-\\npixels and non-worm pixels such as dirt, bubbles, old tracks\\nFigure 4. The 3D curve points are scored individually according\\nto how well they match the three views. The triplet of blobs asso-\\nciated with vertex n(B.,n) are multiplied with the images Iand\\nsummed. We take the minimum of the three sums and then taper\\nthese values from the midpoint-out.\\nand even other worms. When the animal truly intersects\\nwith environmental interference it can be impossible to dif-\\nferentiate between the two, but in the majority of cases there\\nexists a gap between the worm and the noise that is visi-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ble in at least one of the views. By ensuring that the curve\\ncorresponds to a single contiguous pixel mass in allof the\\nimages we are able to safely ignore other artefacts (Fig. 4).\\nTo detect if the curve is bridging a gap, each vertex ˆPnis\\nscored by correlating its corresponding blobs B.,n(Sec. 4.2)\\nwith the images I. The raw score Sn∈Ris defined:\\nSn= min\\x1aP\\ni,jBc,n·Ic\\n¯σc,n¯ιc,n\\x1b\\nc=0,1,2(4)\\nwhere ·is element-wise multiplication and the sum is taken\\nover the image dimensions. By taking the minimum we\\nensure that vertices failing to match pixels in any one of the\\nviews will receive low scores regardless of how well they\\nmatch pixels in the other views.\\nIf the curve is bridging two disjoint groups of pixels that\\nare visible in all three views this will present as two peaks\\ninS. Since we are only interested in finding one object we\\nrestrict the scores to contain just one peak by tapering S\\nfrom the middle-out to form the intermediate S′. Finally', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='we normalise S′to get scores ˆSrelative to the peak:\\nS′\\nn=\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3min{Sn, S′\\nn+1}0≤n < N/ 2\\nSn n=N/2\\nmin{Sn, S′\\nn−1}N/2< n < N(5)\\nˆS=S′\\nmax n{S′}. (6)\\n12569\\nFigure 5. The noisy input images are cleaned by applying masks\\nthat force pixel-errors to be local to the current estimate. The blobs\\nBare scaled by the relative scores ˆS, combined using the maxi-\\nmum pixel value across blobs and thresholded to form the masks\\nM. The masks are applied to the raw input images Ito generate\\nthe targets: I⋆. Masking ensures only a single contiguous pixel\\nmass is detected. Without it, parts of the reconstruction can “stick”\\nto nearby bubbles and other artefacts as shown below.\\nThe final score profile ˆSprovides insight into how well\\nthe curve matches a contiguous pixel mass across all three\\nviews and how evenly that mass is distributed.\\nMasking From the score profile ˆSwe identify image ar-\\neas that are more likely to contain the pixel masses that cor-\\nrespond to the worm. Masks M∈R3×w×wapplied to the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='input, I⋆=M·I, focuses attention (and gradient) to only\\nthese areas of interest, consistently across all three views\\nand exclude interference outside the masks (Fig. 5, see SM).\\nPixel intensities outside the masks are significantly reduced,\\nbut not zeroed in order to avoid stagnation in case the recon-\\nstruction completely misses the worm.\\nCentre-shifting The scores ˆSalso indicate the relative\\npositioning of the curve over the target object. As the curve\\naligns with a pixel mass, vertices with high scores (appar-\\nently “converged”) tend to lock into place thus hindering\\nconvergence of the rest of the object. For each frame, we\\nuse the previous frame solution as the starting point, so the\\nmajority of points rapidly converge. However, errors intro-\\nFigure 6. As the animal moves along the path of its midline the\\ntail may be left behind (left column). This can be identified from\\nan unbalanced score profile ˆS. By periodically shifting the curve', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='along its length (adding new curvature values at one end and dis-\\ncarding from the other) the centroid index ( ¯n) of the scores can be\\ncentred. Gradient descent optimisation then updates the new cur-\\nvature values so the curve matches the target (right column).\\nduced at the tips remain as they are insufficient to generate\\nthe collective shift required. The effect can easily be identi-\\nfied from an unbalanced score profile (Fig. 6) and rectified\\nby periodically shifting the curve along its length between\\ngradient descent optimisation steps (see SM).\\n5. Optimisation\\nThe main pixel-loss to be minimised is defined as:\\nLpx=1\\n3w2X\\nc,i,j(Rc(i, j)−I⋆\\nc(i, j))2. (7)\\nTo improve head and tail detection we also minimise a\\nscores-loss,\\nLsc=max( S′)NP\\nnS′′n,where (8)\\nS′′\\nn=S′\\nn\\x122n−(N−1)\\nN−1\\x132\\n, (9)\\nthat is quadratically weighted towards the tips where the\\nscores are naturally lower due to the transparency.\\nIn addition we include a number of regularisation terms.\\nTo keep the curve smooth we define', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Lsm=N−1X\\nn=1|Kn−Kn−1|2, (10)\\nwhere | · |is the l2-norm. To ensure all parameters change\\n12570\\nsmoothly between frames we set\\nLt=X\\nx∈{l,K,ˆP,η,σ,ι,ρ }|xprev−x|2, (11)\\nwhere xprevrefers to the frozen value of the variable from\\nthe previous frame. And to avoid self-intersections, we use\\ndn,m=|ˆPn−ˆPm|, (12)\\nd′\\nn,m=1\\n3X\\nc¯σc,n+1\\n3X\\nc¯σc,m,and (13)\\nLi=N−N/k max−1X\\nn=0N−1X\\nm=n+N/k max(d′\\nn,m\\ndn,m,ifdn,m< d′\\nn,m\\n0, otherwise.\\n(14)\\nA loss is incurred, Li>0, when two points which are suf-\\nficiently far apart ( > N/k max) along the curve come within\\na distance defined by the sum of their mean rendering vari-\\nances (since these approximate the worm’s radius). Eq. (14)\\nforces the algorithm to find postures that are always feasible\\neven during self-occlusions and complex manoeuvres.\\nThe losses are combined in a weighted sum to yield the\\nfinal optimisation target:\\nL=ωpxLpx+ωscLsc+ωsmLsm+ωtLt+ωiLi.(15)\\nValues of ωused in our experiments are included in the SM.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='To achieve robust reconstructions it is important that the\\ncurve parameters learn fastest, then the rendering parame-\\nters and finally the camera parameters. Imposing this hierar-\\nchy of rates ensures camera model stability and prevents the\\nrenderer from over-blurring the edges (as it tries to “reach”\\nthe pixels). Thus, movement between frames is primar-\\nily captured through curve deformations. We use learning\\nratesλp= 1e−3for the curve parameters {P, T, M1, K, l},\\nλr= 1e−4for the rendering parameters {σ, ι, ρ}and\\nλη= 1e−5for the camera parameters η.\\nThe curve is initialised as a small ( ∼0.2 mm ), randomly\\noriented straight line centred in the field of view of all three\\ncameras. We slowly increase the length to lminover the first\\n200-500steps as the curve gets positioned and orientated.\\nThe pipeline is constructed using PyTorch [64] and the\\nloss minimised is using Adam [31] with periodic centre-\\nshifting of the curve vertices. Learning rates are decreased', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='by a factor of 0.8for every 5steps taken without improve-\\nment in Lto a minimum of 1e−6until convergence is de-\\ntected. Subsequent frames are instantiated with the solution\\nfrom the previous frame for efficiency and to maintain con-\\nsistency through complex sequences of self-occluding pos-\\ntures. Example videos showing the effects of varying some\\nof the options on the optimisation are described in SM.\\nFigure 7. Validation against 487 manual annotations. At the top\\nwe show an example of an annotated frame (left, orange) alongside\\na projection of our matching 3D midline (right, blue). Below we\\nplot the sample averages ±2std. We find our midlines are consis-\\ntently close to annotated points (blue curve), but annotations typi-\\ncally extend further into the head and tail regions (orange curve).\\n6. Results\\nUsing our method we generate high quality 3D midline\\nreconstructions for 43 of 44 recordings. One fails due to ex-\\ncessive coiling of the worm. Significant occlusions also oc-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='cur during successful reconstructions and when combined\\nwith loss of focus can cause the shape to be lost. Video\\nclips of good and poor reconstructions through challenging\\nenvironmental conditions are described in SM along with\\nablation results to show benefits of each component.\\nWe compare 2D reprojections of our midlines against\\n487 manual annotations that were produced from single im-\\nages in isolation and contain a varying number of unordered\\npoints. We calculate the minimum distance from each an-\\nnotated point to any reconstructed point and vice-versa and\\nfind that our midlines consistently come close ( ∼2px) to\\nhand-annotated points (Fig. 7). Annotated points at the ends\\nshow an increased distance ( ∼10px) to our midline points.\\nThis shows that our curves generally fall short of reaching\\nthe very tips of the worm by ∼ O(worm radius ).\\nOur method significantly outperforms previous methods\\ndeveloped using the same dataset [45, 63] when evaluated', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='against the manual annotations (SM), but these only cover a\\nselection of hand-picked examples. For a large-scale com-\\nparison we take 3D midlines and camera parameters found\\nby each method and, using our pipeline, render them to\\ngenerate comparable images (re-optimising the render pa-\\nrameters for their midlines, see SM). We skip the scoring\\nand masking and calculate Lpx. The results (Fig. 8) show\\nour method consistently produces shapes that more closely\\nmatch the raw images. The biggest advantage over previous\\napproaches is the improvement in robustness; we recover\\n12571\\nFigure 8. A comparison between our Midline Finder (MF), Yuval’s Worm-Tracker 3D (WT3D) [63] and Salfelder et al.’s ‘reconst’ [45]\\nmethods across a single trial ( ∼13 min ). In the majority of cases our method generates midlines that better match the data (lower pixel\\nlosses, Lpx). We show moving averages over 25 frames ( ∼1 s) with shaded areas indicating ±2std.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Figure 9. The rendering parameters change continually over the course of a recording to capture optical changes. Clear images ( e.g. early\\nframes in cameras 0 and 1, switching to late frames in camera 2) are consistent with small values of σand large values of ρ. Blurry images\\n(early camera 2, late camera 1) use high σand small ρ. We show moving averages over 25 frames ( ∼1 s) with shaded areas indicating\\n±2std. Example comparisons between the renders (red) and raw images (grey) are shown on either side.\\n4 h 37 min (ours) versus 1 h 32 min [45] and 45 min [63].\\nFig. 9 shows the rendering parameters during a trial as\\nthe worm moves in and out of focus in the different cam-\\neras. Clearer images result in smaller values of σand larger\\nvalues of ρ. The fluctuations in intensity ιare due in part to\\nthe posture of the worm in relation to the camera; when it is\\npointing directly towards the camera we see higher values\\nofιused to capture the darker image observed and when the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='shape is perpendicular to the camera we see lower values of\\nιto emulate the worm’s transparency. All three parameters\\nwork in tandem to produce the final effect.\\n7. Conclusion\\nWe present a robust and reliable framework for the 3D\\nreconstruction of a microscopic, semi-transparent subject\\nmoving through a fluid and evaluate against two other al-\\ngorithms and manually annotations. The key contribution\\nof our approach – constructing unique differentiable ren-\\nderings for each view – allows us to solve shape recon-struction and camera parameter optimisation by direct im-\\nage comparison. This avoids feature extraction and corre-\\nspondence matching, and hence offers a powerful alterna-\\ntive when those approaches are not well-suited, e.g. due to\\nthe variation in appearance between views.\\nMulti-view microscopic camera calibration, imaging\\nthrough fluids and parametric model fitting of semi-\\ntransparent subjects are challenges that have received little', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='attention in the literature. While we have focused here on\\nconstructing a curve to fit a microscopic worm from three\\nviews, our method could be applied to the 3D reconstruc-\\ntion of arbitrary shape models at any scale using any num-\\nber of viewpoints. Rendering points with adaptable super-\\nGaussian functions presents an effective solution to trans-\\nparency and focal issues, but more generally, our results in-\\ndicate that our direct optimisation approach may offer an\\neffective alternative to contemporary methods for 3D ap-\\nproximation of generic objects from a limited number of\\nsilhouette-like images.\\n12572\\nReferences\\n[1] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and\\nBernt Schiele. 2D human pose estimation: New bench-\\nmark and state of the art analysis. In 2014 IEEE Conference\\non Computer Vision and Pattern Recognition , pages 3686–\\n3693. IEEE, June 2014. 2\\n[2] Praneet C. Bala, Benjamin R. Eisenreich, Seng Bum Michael\\nYoo, Benjamin Y . Hayden, Hyun Soo Park, and Jan Zim-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='mermann. Automated markerless pose estimation in freely\\nmoving macaques with OpenMonkeyStudio. Nat Commun ,\\n11(1):4560, Sept. 2020. 3\\n[3] Jerrold L Belant, Joshua J Millspaugh, James A Martin, and\\nRobert A Gitzen. Multi-dimensional space use: The final\\nfrontier. Front. Ecol. Environ. , 10(1):11–12, Feb. 2012. 1\\n[4] Florian Berlinger, Melvin Gauci, and Radhika Nagpal. Im-\\nplicit coordination for 3D underwater collective behaviors in\\na fish-inspired robot swarm. Sci. Robot. , 6(50):eabd8668,\\nJan. 2021. 1\\n[5] Stefano Berri, Jordan H. Boyle, Manlio Tassieri, Ian A.\\nHope, and Netta Cohen. Forward locomotion of the nema-\\ntode C. elegans is achieved through modulation of a single\\ngait. Hfsp J. , 3(3):186–193, June 2009. 3\\n[6] Benjamin Biggs, Oliver Boyne, James Charles, Andrew\\nFitzgibbon, and Roberto Cipolla. Who left the dogs out?\\n3d animal reconstruction with expectation maximization in\\nthe loop. In Computer Vision–ECCV 2020: 16th European', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Conference, Glasgow, UK, August 23–28, 2020, Proceed-\\nings, Part XI 16 , pages 195–211. Springer, 2020. 3\\n[7] Benjamin Biggs, Thomas Roddick, Andrew Fitzgibbon, and\\nRoberto Cipolla. Creatures great and smal: Recovering the\\nshape and motion of animals from video. In Computer\\nVision–ACCV 2018: 14th Asian Conference on Computer\\nVision, Perth, Australia, December 2–6, 2018, Revised Se-\\nlected Papers, Part V 14 , pages 3–19. Springer, 2019. 3\\n[8] Richard L. Bishop. There is more than one way to frame a\\ncurve. Amer. Math. Monthly , 82(3):246–251, Mar. 1975. 3\\n[9] Thomas J. Cashman and Andrew W. Fitzgibbon. What shape\\nare dolphins? building 3D morphable models from 2D im-\\nages. IEEE Trans. Pattern Anal. Mach. Intell. , 35(1):232–\\n244, Jan. 2013. 3\\n[10] Ching-Hang Chen and Deva Ramanan. 3D human pose es-\\ntimation = 2D pose estimation + matching. In 2017 IEEE\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 7035–7043. IEEE, July 2017. 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[11] Nathan W. Cooper, Thomas W. Sherry, and Peter P. Marra.\\nModeling three-dimensional space use and overlap in birds.\\nAuk, 131(4):681–693, Oct. 2014. 1\\n[12] Amael Delaunoy and Marc Pollefeys. Photometric bundle\\nadjustment for dense multi-view 3D modeling. In 2014 IEEE\\nConference on Computer Vision and Pattern Recognition ,\\npages 1486–1493. IEEE, June 2014. 2\\n[13] Olivier Faugeras and Quang-Tuan Luong. The Geometry of\\nMultiple Images . The MIT Press, 2001. 2\\n[14] Alessandro Ferrarini, Giuseppe Giglio, Stefania Caterina\\nPellegrino, Anna Grazia Frassanito, and Marco Gustin. A\\nnew methodology for computing birds’ 3D home ranges.\\nAvian Res , 9(1):1–6, May 2018. 1[15] Lise Fr ´ezal and Marie-Anne F ´elix. The natural history of\\nmodel organisms: C. elegans outside the petri dish. eLife ,\\n4:e05849, Mar. 2015. 2\\n[16] Kui Fu, Jiansheng Peng, Qiwen He, and Hanxiao Zhang.\\nSingle image 3D object reconstruction based on deep learn-\\ning: A review. Multimed Tools Appl , 80(1):463–498, Sept.\\n2020. 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='2020. 2\\n[17] Marie-Anne F ´elix and Christian Braendle. The natural his-\\ntory of caenorhabditis elegans. Curr. Biol. , 20(22):R965–\\nR969, Nov. 2010. 2\\n[18] P. Georgel, S. Benhimane, and N. Navab. A unified approach\\ncombining photometric and geometric information for pose\\nestimation. In Procedings of the British Machine Vision Con-\\nference 2008 , pages 1–10. Citeseer, British Machine Vision\\nAssociation, 2008. 2\\n[19] Riza Alp Guler, Natalia Neverova, and Iasonas Kokkinos.\\nDensePose: Dense human pose estimation in the wild. In\\n2018 IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition , pages 7297–7306. IEEE, June 2018. 2\\n[20] Zicheng Guo and Richard W. Hall. Parallel thinning with\\ntwo-subiteration algorithms. Commun. ACM , 32(3):359–\\n373, Mar. 1989. 3\\n[21] Richard Hartley and Andrew Zisserman. Multiple View Ge-\\nometry in Computer Vision . Cambridge University Press,\\nMar. 2004. 2\\n[22] Robert I. Holbrook and Theresa Burt de Perera. Three-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='dimensional spatial cognition: Information in the vertical di-\\nmension overrides information from the horizontal. Anim\\nCogn , 14(4):613–619, Mar. 2011. 1\\n[23] C.T. Huang and O.R. Mitchell. Dynamic camera calibra-\\ntion. In Proceedings of International Symposium on Com-\\nputer Vision - ISCV , pages 169–174. IEEE, IEEE Comput.\\nSoc. Press, 1995. 2\\n[24] Steven J. Husson, Wagner S. Costa, Cornelia Schmitt, and\\nAlexander Gottschalk. Keeping track of worm trackers.\\nWormBook , pages 1–17, Sept. 2012. 3\\n[25] Avelino Javer, Michael Currie, Chee Wai Lee, Jim Hokan-\\nson, Kezhi Li, C ´eline N. Martineau, Eviatar Yemini, Laura J.\\nGrundy, Chris Li, QueeLim Ch’ng, William R. Schafer,\\nEllen A. A. Nollen, Rex Kerr, and Andr ´e E. X. Brown.\\nAn open-source platform for analyzing and sharing worm-\\nbehavior data. Nat Methods , 15(9):645–646, Aug. 2018. 3\\n[26] Le Jiang, Caleb Lee, Divyang Teotia, and Sarah Ostadabbas.\\nAnimal pose estimation: A closer look at the state-of-the-art,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='existing gaps and opportunities. Comput. Vis. Image Und. ,\\n222:103483, Sept. 2022. 1, 2\\n[27] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and\\nJitendra Malik. End-to-end recovery of human shape and\\npose. In 2018 IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition , pages 7122–7131. IEEE, June\\n2018. 2, 3\\n[28] Angjoo Kanazawa, Shahar Kovalsky, Ronen Basri, and\\nDavid Jacobs. Learning 3d deformation of animals from 2d\\nimages. In Computer Graphics Forum , volume 35, pages\\n365–374. Wiley Online Library, 2016. 3\\n[29] Isinsu Katircioglu, Bugra Tekin, Mathieu Salzmann, Vincent\\nLepetit, and Pascal Fua. Learning latent representations of\\n12573\\n3D human pose with deep neural networks. Int J Comput\\nVis, 126(12):1326–1341, Jan. 2018. 2\\n[30] Sinead Kearney, Wenbin Li, Martin Parsons, Kwang In Kim,\\nand Darren Cosker. RGBD-dog: Predicting canine pose from\\nRGBD sensors. In 2020 IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR) , pages 8336–8345.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='IEEE, June 2020. 1, 2\\n[31] Diederik P Kingma and Jimmy Ba. Adam: A method for\\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\\n2014. 7\\n[32] Nikos Kolotouros, Georgios Pavlakos, Michael Black, and\\nKostas Daniilidis. Learning to reconstruct 3D human pose\\nand shape via model-fitting in the loop. In 2019 IEEE/CVF\\nInternational Conference on Computer Vision (ICCV) , pages\\n2252–2261. IEEE, Oct. 2019. 2\\n[33] Namseop Kwon, Ara B. Hwang, Young-Jai You, Seung-Jae\\nV . Lee, and Jung Ho Je. Dissection of C. elegans behavioral\\ngenetics in 3-d environments. Sci Rep , 5(1):1–9, May 2015.\\n3\\n[34] Namseop Kwon, Jaeyeon Pyo, Seung-Jae Lee, and Jung Ho\\nJe. 3-d worm tracker for freely moving C. elegans .PLoS\\nONE , 8(2):e57484, Feb. 2013. 3\\n[35] Wu Liu, Qian Bao, Yu Sun, and Tao Mei. Recent advances\\nof monocular 2D and 3D human pose estimation: A deep\\nlearning perspective. ACM Comput. Surv. , 55(4):1–41, Nov.\\n2022. 2\\n[36] H. C. Longuet-Higgins. A computer algorithm for re-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='constructing a scene from two projections. Nature ,\\n293(5828):133–135, Sept. 1981. 2\\n[37] Simone Macr `ı, Daniele Neri, Tommaso Ruberto, Vio-\\nlet Mwaffo, Sachit Butail, and Maurizio Porfiri. Three-\\ndimensional scoring of zebrafish behavior unveils biological\\nphenomena hidden by two-dimensional analyses. Sci Rep ,\\n7(1):1–10, May 2017. 1\\n[38] Julieta Martinez, Rayat Hossain, Javier Romero, and\\nJames J. Little. A simple yet effective baseline for 3d hu-\\nman pose estimation. In 2017 IEEE International Confer-\\nence on Computer Vision (ICCV) , pages 2640–2649. IEEE,\\nOct. 2017. 2\\n[39] Valsamis Ntouskos, Marta Sanzari, Bruno Cafaro, Fed-\\nerico Nardi, Fabrizio Natola, Fiora Pirri, and Manuel Ruiz.\\nComponent-wise modeling of articulated objects. In 2015\\nIEEE International Conference on Computer Vision (ICCV) ,\\npages 2327–2335. IEEE, Dec. 2015. 3\\n[40] Onur ¨Ozyes ¸il, Vladislav V oroninski, Ronen Basri, and Amit\\nSinger. A survey of structure from motion. Acta Numer. ,\\n26:305–364, May 2017. 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[41] Brian L. Partridge, Tony Pitcher, J. Michael Cullen, and John\\nWilson. The three-dimensional structure of fish schools. Be-\\nhav Ecol Sociobiol , 6(4):277–288, Mar. 1980. 1\\n[42] Georgios Pavlakos, Xiaowei Zhou, Konstantinos G. Derpa-\\nnis, and Kostas Daniilidis. Coarse-to-fine volumetric predic-\\ntion for single-image 3D human pose. In 2017 IEEE Confer-\\nence on Computer Vision and Pattern Recognition (CVPR) ,\\npages 7025–7034. IEEE, July 2017. 2\\n[43] Mukta Prasad, Andrew Fitzgibbon, Andrew Zisserman, and\\nLuc Van Gool. Finding nemo: Deformable object class mod-elling using curve matching. In 2010 IEEE Computer Soci-\\nety Conference on Computer Vision and Pattern Recognition ,\\npages 1720–1727. IEEE, IEEE, June 2010. 3\\n[44] Daniel Ramot, Brandon E. Johnson, Tommie L. Berry, Lu-\\ncinda Carnell, and Miriam B. Goodman. The parallel worm\\ntracker: A platform for measuring average speed and drug-\\ninduced paralysis in nematodes. PLoS ONE , 3(5):e2208,\\nMay 2008. 3', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='May 2008. 3\\n[45] Felix Salfelder, Omer Yuval, Thomas P Ilett, David C\\nHogg, Thomas Ranner, and Netta Cohen. Markerless\\n3D spatio-temporal reconstruction of microscopic swimmers\\nfrom video. In Visual observation and analysis of Vertebrate\\nAnd Insect Behavior 2020 , 2021. 2, 3, 4, 7, 8\\n[46] Hinrich Schulenburg and Marie-Anne F ´elix. The natural\\nbiotic environment of Caenorhabditis elegans .Genetics ,\\n206(1):55–86, May 2017. 2\\n[47] S.M. Seitz, B. Curless, J. Diebel, D. Scharstein, and R.\\nSzeliski. A comparison and evaluation of multi-view stereo\\nreconstruction algorithms. In 2006 IEEE Computer Soci-\\nety Conference on Computer Vision and Pattern Recognition\\n- Volume 1 (CVPR’06) , volume 1, pages 519–528. IEEE,\\nIEEE, 2006. 2\\n[48] William Irvin Sellers and Eishi Hirasaki. Markerless 3D mo-\\ntion capture for animal locomotion studies. Biology Open ,\\n3(7):656–668, June 2014. 2\\n[49] Michael Shaw, Haoyun Zhan, Muna Elmi, Vijay Pawar,\\nClara Essmann, and Mandayam A. Srinivasan. Three-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='dimensional behavioural phenotyping of freely moving C. el-\\negans using quantitative light field microscopy. PLoS ONE ,\\n13(7):e0200108, July 2018. 3\\n[50] Colin A. Simpfendorfer, Esben M. Olsen, Michelle R. He-\\nupel, and Even Moland. Three-dimensional kernel utilization\\ndistributions improve estimates of space use in aquatic ani-\\nmals. Can. J. Fish. Aquat. Sci. , 69(3):565–572, Mar. 2012.\\n1\\n[51] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep\\nhigh-resolution representation learning for human pose esti-\\nmation. In 2019 IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition (CVPR) , pages 5693–5703. IEEE,\\nJune 2019. 2\\n[52] Nicholas A Swierczek, Andrew C Giles, Catharine H\\nRankin, and Rex A Kerr. High-throughput behavioral anal-\\nysis in C. elegans .Nat Methods , 8(7):592–598, June 2011.\\n3\\n[53] Raphael Sznitman, Manaswi Gupta, Gregory D. Hager,\\nPaulo E. Arratia, and Josu ´e Sznitman. Multi-environment\\nmodel estimation for motility analysis of caenorhabditis ele-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='gans. PLoS ONE , 5(7):e11631, July 2010. 3\\n[54] Diane Theriault, Zheng Wu, Nickolay I Hristov, Sharon M\\nSwartz, Kenneth S Breuer, Thomas H Kunz, and Margrit\\nBetke. Reconstruction and analysis of 3D trajectories of\\nBrazilian free-tailed bats in flight. In 20th Int. Conf. on Pat-\\ntern Recognition , pages 1–4, 2010. 1\\n[55] Alexander Toshev and Christian Szegedy. DeepPose: Hu-\\nman pose estimation via deep neural networks. In 2014\\nIEEE Conference on Computer Vision and Pattern Recog-\\nnition , pages 1653–1660. IEEE, June 2014. 2\\n12574\\n[56] Bill Triggs, Philip F. McLauchlan, Richard I. Hartley, and\\nAndrew W. Fitzgibbon. Bundle adjustment — a modern syn-\\nthesis. In Vision Algorithms: Theory and Practice , pages\\n298–372. Springer Berlin Heidelberg, 2000. 2\\n[57] R. Tsai. A versatile camera calibration technique for high-\\naccuracy 3D machine vision metrology using off-the-shelf\\nTV cameras and lenses. IEEE J. Robot. Automat. , 3(4):323–\\n344, Aug. 1987. 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='344, Aug. 1987. 2\\n[58] Sara Vicente and Lourdes Agapito. Balloon shapes: Recon-\\nstructing and deforming objects with volume from images.\\nIn2013 International Conference on 3D Vision , pages 223–\\n230. IEEE, IEEE, June 2013. 3\\n[59] J. Weng, P. Cohen, and M. Herniou. Camera calibration with\\ndistortion models and accuracy evaluation. IEEE Trans. Pat-\\ntern Anal. Machine Intell. , 14(10):965–980, 1992. 2\\n[60] Nils Wilhelm, Anna V ¨ogele, Rebeka Zsoldos, Theresia\\nLicka, Bj ¨orn Kr ¨uger, and J ¨urgen Bernard. FuryExplorer:\\nVisual-interactive exploration of horse motion capture data.\\nInSPIE Proceedings , volume 9397, pages 148–162. SPIE,\\nSPIE, Feb. 2015. 1, 2\\n[61] Shangzhe Wu, Christian Rupprecht, and Andrea Vedaldi.\\nUnsupervised learning of probably symmetric deformable\\n3D objects from images in the wild. In 2020 IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 1–10. IEEE, June 2020. 2\\n[62] Eviatar Yemini, Rex A. Kerr, and William R. Schafer. Track-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ing movement behavior of multiple worms on food. Cold\\nSpring Harb Protoc , 2011(12):pdb.prot067025, Dec. 2011.\\n3\\n[63] Omer Yuval. The neuromechanical control of Caenorhabdi-\\ntis elegans head motor behaviour in 3D environments . PhD\\nthesis, University of Leeds, 2022. 3, 7, 8\\n[64] Sergey Zagoruyko, Adam Lerer, Tsung-Yi Lin, PedroO. Pin-\\nheiro, Sam Gross, Soumith Chintala, and Piotr Dollar. A\\nMultiPath network for object detection. In Procedings of the\\nBritish Machine Vision Conference 2016 . British Machine\\nVision Association, 2016. 7\\n[65] Liqun Zhu and Wei Weng. Catadioptric stereo-vision system\\nfor the real-time monitoring of 3D behavior in aquatic an-\\nimals. Physiology & Behavior , 91(1):106–119, May 2007.\\n1\\n[66] Michael Zollh ¨ofer, Patrick Stotko, Andreas G ¨orlitz, Chris-\\ntian Theobalt, Matthias Nießner, Reinhard Klein, and An-\\ndreas Kolb. State of the art on 3D reconstruction with RGB-\\nd cameras. In Computer graphics forum , volume 37, pages\\n625–652. Wiley Online Library, 2018. 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[67] Silvia Zuffi, Angjoo Kanazawa, David W. Jacobs, and\\nMichael J. Black. 3D menagerie: Modeling the 3D shape\\nand pose of animals. In 2017 IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR) , pages 6365–6373.\\nIEEE, July 2017. 3\\n12575', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Leveraging Inter-rater Agreement for Classiﬁcation in the Presence of NoisyLabelsMaria Soﬁa Bucarelli2*Lucas Cassano1Federico Siciliano2*Amin Mantrach1Fabrizio Silvestri2, 31Amazon2Sapienza University of Rome3ISTI-CNR, Pisa, Italy{mariasofia.bucarelli, federico.siciliano}@uniroma1.it{lcecasl, mantrach}@amazon.lufsilvestri@diag.uniroma1.itAbstractIn practical settings, classiﬁcation datasets are obtainedthrough a labelling process that is usually done by humans.Labels can be noisy as they are obtained by aggregating thedifferent individual labels assigned to the same sample bymultiple, and possibly disagreeing, annotators. The inter-rater agreement on these datasets can be measured whilethe underlying noise distribution to which the labels aresubject is assumed to be unknown. In this work, we: (i)show how to leverage the inter-annotator statistics to esti-mate the noise distribution to which labels are subject; (ii)introduce methods that use the estimate of the noise distri-bution to', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='distri-bution to learn from the noisy dataset; and (iii) establishgeneralization bounds in the empirical risk minimizationframework that depend on the estimated quantities. We con-clude the paper by providing experiments that illustrate ourﬁndings.1. IntroductionSupervised learning has seen enormous progress in thelast decades, both theoretical and practical. Empirical riskminimization is used as a learning framework [23], whichrelies on the assumption that the model is trained with iid(independent and identically distributed) sampled data fromthe joint distribution between features and labels. As a con-sequence of generalization bounds, when this assumption issatisﬁed any desired performance can be achieved as longas enough training data is available. However in many real-world applications, due to ﬂaws during the data collectionand labeling process, the assumption that the training datais sampled from the true feature-label joint distribution doesnot hold. Training data is often', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='data is often annotated by human raterswho have some non-zero probability of making mistakes. It*This work was done during Maria Soﬁa Bucarelli’s and Federico Si-ciliano’s internship at Amazon.has been reported in [21] that the ratio of corrupted labelsin some real-world datasets is between8.0%and,38.5%.As a consequence of the presence of incorrect labels in thetraining dataset, the aforementioned assumption is violatedand hence performance guarantees based on generalizationbounds no longer hold.This gap between theory and practice raises the questionwhether it is possible to learn from datasets with noisy la-bels while still having performance guarantees. This ques-tion has received a lot of attention lately and has alreadybeen answered in the positive in some cases [15,16]. In-deed multiple works have introduced learning algorithmsthat can cope with datasets with incorrect labels while guar-anteeing desirable performance through provable general-ization bounds. However, these', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='However, these solutions do not solve theentirety of the problem due to the fact that they rely onprecise knowledge of the error rate to which the labelsare subject, which is often unknown in practice. Severalworks [16,26,27] attempt to address this issue by introduc-ing techniques to estimate such error rate.Some of thesemethods have the drawback of relying on assumptions thatdo not always hold in practice, such as the existence of an-chor samples [16]. Ideally, it would be desirable to designlearning algorithms that are both robust to noisy labels, andfor which performance guarantees can be provided.An approach, often used in industry to reduce the im-pact of errors made by human raters, is to label the samedataset multiple times by different annotators. Then the in-dividual labels are combined to reduce the probability oferroneous labels in the dataset, two popular approaches aremajority vote or soft labeling. In these cases inter-annotatoragreement (IAA) scores (like Cohen’s kappa', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='(like Cohen’s kappa [1] and Fleiss’kappa [5]) provide measurable metrics that are directly re-lated to the probability of error present in the labels.Since the IAA holds a direct relationship with the errorrate associated with the human raters, one could potentiallyestimate the error rate and leverage this estimate to modify', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n3439', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the learning algorithms with the objective of making themrobust to the resulting noise in the labels. This is the maindirection we explore in this work.Motivation and Contributions:This work is motivatedby two main points: i- to the best of our knowledge thereare no published results that indicate how to leverage theIAA statistics to estimate the label noise distribution; andii- the generalization bounds of existing noise tolerant train-ing methods often rely onunknownquantities (like the truenoise distribution) instead of on quantities that can be mea-sured (like the IAA statistics).Our contributions are the following: (i) we provide amethodology to estimate the label noise distribution basedon the IAA statistics; (ii) we show how to leverage thisestimate to learn from the noisy dataset; and (iii) we pro-vide generalization bounds for our methods that depend onknownquantities.2. Related worksOur work is related to literature on three main topics: (i)robust loss function design, (ii)', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='design, (ii) label aggregating and (iii)noise rate estimation.Robust loss functionsIn classiﬁcation tasks, the goal isto obtain the lowest probability of classiﬁcation error. The0\\x001loss counts how many errors a classiﬁer makes ona given dataset and is often used in the evaluation of theclassiﬁer. However, it is rarely used in optimization proce-dures because it is non-differentiable and non-continuous.To overcome this, many learning strategies use some con-vexsurrogatesof the0\\x001loss function (e.g. hinge loss,squared error loss, cross-entropy).It was proved ( [6], [7]) thatsymmetricloss functions,that are functions for which the sum of the risks over all cat-egories is equivalent to a constant for each arbitrary exam-ple, are robust to label noise. Examples of symmetric lossfunctions include the0\\x001loss, the Ramp Loss and (soft-max) Mean Absolute Error (MAE). In [29] authors showthat even if MAE is noise tolerant and cathegorical crossentropy (CCE) is not, MAE can perform poorly when', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='perform poorly when usedto train DNN in challenging domains. They also propose aloss function that can be seen as a generalization of MAEand CCE. Several other loss functions that do not strictlysatisfy the symmetry condition have also been proposed tobe robust against label noise when training deep neural net-works [4,13,24].[15] presents two methods to modify the surrogate lossin the presence of class-conditional random label noise. Theﬁrst method introduces a new loss that is an unbiased esti-mator for a given surrogate loss, and the second methodintroduces a label-dependent loss. The paper provides gen-eralization bounds for both methods, which depend on thenoise rate of the dataset and the complexity of the hypothe-sis space.Labels aggregationWhen constructing datasets for su-pervised learning, data is often not labeled by a single an-notator, rather multiple imperfect annotators are asked to as-sign labels to documents. Typically, separate labels are ag-gregated into one before', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='into one before learning models are applied [3,20].In our work, we propose to exploit a measure of the agree-ment between annotators to explicitly calculate the noiseof the dataset. Recently some works revisited the choiceof aggregating labels. In [19] authors explore how to trainLETOR models with relevance judgments distributions in-stead of single-valued relevance labels. They interpret theoutput of a LETOR model as a probability value or distri-bution and deﬁne different KL divergence-based loss func-tions to train a model. The loss they proposed can be used totrain any ranking model that relies on gradient-based learn-ing (in particular they focused on transformer-based neu-ral LETOR models and on the decision tree-based GBMmodel). However, the authors do not directly estimate thenoise rates in the annotations or study how learning fromthese noisy labels affects the generalization error of themodels trained with the methods they introduce. In [25]the authors analyze the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='authors analyze the performance of both label aggrega-tion and non-aggregation approaches in the context of em-pirical risk minimization for a number of popular loss func-tions, including those designed speciﬁcally for the noisy la-bel learning problem. They conclude that label separationis preferable to label aggregation when noise rates are highor the number of labelers/annotations is insufﬁcient. [17]and [22] exploit the availability of multiple human anno-tations to construct soft labels and concludes that this in-creases performance in terms of generalization to out-of-training-distribution test datasets, and robustness to adver-sarial attacks. [2] focus on efﬁciently eliciting soft labelsfrom individual annotators.Noise rate estimationA number of approaches have beenproposed for estimating the noise transition matrix (i.e. theprobabilities that correct labels are changed for incorrectones) [12,16,31]. Usually these methods use a small num-ber of anchor points (that are samples', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='(that are samples that belong to a spe-ciﬁc class with probability one) [8]. In particular, [16] pro-posed a noise estimation method based on anchor points,with the intent to provide an ‘end-to-end’ noise-estimation-and-learning method. Due to the lack of anchor points inreal data, some works focused on a way to detect anchorpoints in noisy data, [26,27]. In [27] the authors proposeto introduce an intermediate class to avoid directly estimat-ing the noisy class posterior. [28] also propose an iterativenoise estimation heuristic that aims to partly correct the er-ror and pointed out that the methods introduced by [16]', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='3440', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and [27] have an error in computing anchor points, and pro-vide conditions on the noise under which the methods workor fail. [26] provides a solution that can infer the transitionmatrix without anchor points. Indeed they use the instanceswith the highest class posterior probabilities for noisy dataas anchor points. Our work differs from the mentioned workthat use anchor points because we do not need to assumethe existence of anchor points or to have a validation set tolearn the noise rate and we only use noisy data to train ourmodel, moreover we neither aim to detect anchor points inthe noisy data. Also most of these works do not study thegeneralization properties of the proposed models,while wealso address this problem and ﬁnd bound that depend on theestimated noise transition matrix.Another approach is based on the clusterability condi-tion, that is an example belongs to the same true class of itsnearest-neighbors representations. [30] presented a methodthat relies on statistics of', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='on statistics of high-order consensuses among the2 nearest-neighbors noisy labels.3. Problem formulation3.1. NotationIn this paper we follow the following notation. Matricesand sets are denoted by upper-case and calligraphic letters,respectively. The space ofd-dimensional feature vectors isdenoted byX⇢Rd.We denote byCthe number of classes and byejthej-th standard canonical vector inRC, namely the vector thathas1in thej-th position and zero in all the other positions.Y={e1,...,eC}⇢{0,1}Cis the label set. Feature vec-tors and labels are denoted byxandy, respectively.Disthe joint distribution of the feature vectors and labels, i.e.(x, y)⇠D. The sampled dataset of sizenis denoted bybD={(xi,yi)}ni=1.f(x)denotes the output of the classiﬁerffor feature vectorxand is aCdimensional vector. Allvectors are column vectors.We denote by`(t, y)a generic loss function for the clas-siﬁcation task that takes as inputCdimensional vectorstandy. In practicetwill contain the prediction of the modelandywill', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the modelandywill be the ground-truth label as a one-hot encodedvector. Namely`:[ 0,1]C⇥Y!R.3.2. BackgroundWe consider the classiﬁcation problem within the super-vised learning framework, where the ultimate goal is to min-imizethe`-riskR`,D(f)=E(x,y)⇠D[`(f(x),y)], for someloss function`. We denote byDthe joint distribution of fea-ture vectorsxand labelsy. In practice, since the distributionis unknown instead of minimizingR`,D(f)we minimize anempirical risk over some sampled datasetbD:bR`,bD(f)=1nnXi=1`(f(xi),yi)=E(x,y)⇠bD[`(f(x),y)].(1)In this work we assume that the true labelsyiare un-known and consider two scenarios, both of which rely onHannotators.3.2.1 Scenario IIn this scenario we have access to theHlabels providedby the annotators for each sample, whereyi,arefers to thelabel provided by thea-th annotator for thei-th sample. Fora given feature vectorxithe distribution of labels providedby annotatorais given by its noise transition matrixTa,which is deﬁned as', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='is deﬁned as follows:(Ta)i,j:=P(ya=j|y=i)(2)Assumption 1.We assume that all annotators have thesame noise transition matrix (i.e.Ta=Tfor alla), thatTis symmetric and that its diagonal elements are largerthan0.5(i.e.P(ya=i|y=i)>0.5,8i2{1,...C}).Note that by deﬁnitionTis right stochastic and hencealso doubly stochastic. It is also strictly diagonally domi-nant and therefore non-singular.Proposition 3.0.1.Tis positive deﬁnite.Proof.SinceTis symmetric it follows that all eigenvaluesare real. Combining the fact that it is strictly diagonallydominant with Gershgorin’s theorem we conclude that alleigenvalues lie in the range(0,1]and henceTis positivedeﬁnite.Assumption 2.We assume that the annotators are condi-tionally independent on the true labely:P(ya,yb|y)=P(ya|y)P(yb|y).(3)We now deﬁne the IAA matrixMabbetween annotatorsaandbas follows:(Mab)i,j:=P(ya=i, yb=j)(4)Proposition 3.0.2.Leveraging Assumption2the agreementmatrixMa,bcan be written as follows:Ma,b=TaTDTb(5)D:=diag{⌫}(6)⌫:=[P(y=', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='1),···,P(y=C)]T.(7)Due to Proposition3.0.1and the fact thatDis positive def-inite it follows that all matricesMa,bare invertible.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='3441', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Assumption 3.We assume that the class probabilities (andhenceD) are known.Due to Assumption1all annotators share the same noisetransition matrixT. ThereforeMabis independent ofaandband from now on we remove this dependencyin the no-tation(i.e. we getM=TTDT). Furthermore, sinceTisinvertible andDdiagonal and positive deﬁnite it followsthatMis also positive deﬁnite.Note that since we have access to all the labels providedby theHannotators for all the samples we can obtain anestimate ofMwhich we denotecM.Assumption 4.We assume thatcMis a consistent estimator.For the case of two annotators, one possible consistentestimator[Ma,bthat exploits its symmetry condition is givenby:([Ma,b)i,j=nXk=11(ya,k=i, yb,k=j)+1(ya,k=j, yb,k=i)2n(8)If the annotators have the same transition matrix,Mwillbe the same for all pairs of annotators. So we can estimateM, in the case ofH\\x002by averaging the estimatorscMabobtain by Eq. (8) for all possible pairs of annotators. Theestimator in this case can be written', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='case can be written as(cM)i,j=1H(H\\x001)HXa=1HXb=1b6=anXh=11(ya,h=i, yb,h=j)n.(9)3.2.2 Scenario IIIn the second scenario, for eachi-th sample we are given aunique label˜yithat is produced by aggregating theHin-dividual labels according to some known aggregating pol-icy (like majority vote). In this case, since we do not haveaccess to the individual annotations we assume thatcMisprovided.The probability that labelyiis corrupted to some otherlabel˜yiis given by theaggregated noise transition matrix\\x002[0,1]C⇥C, where\\x00ij:=P(˜y=j|y=i)is the proba-bility of the true labelibeing ﬂipped into a corrupted labeljandCis the number of classes. Note that by deﬁnition\\x00is aright stochastic matrix that is determined byT, the amountof annotatorsHand the aggregating policy. We will studyboth the case where\\x00=T, and the case in which thereexists a generic Lipschitz function\\x00so that\\x00\\x001=\\x00(T).There are different policy choices to construct the datasetthat lead to\\x00=T. If we decide to use only one annotator,for', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='one annotator,for instancea, to build the ﬁnal dataset, namely for eachsample˜yi=yiawe have\\x00=Ta. Or if annotators arehomogeneous, i.e. they have the same noise transition ma-trixT, and to build the ﬁnal dataset we decide to randomlyselect the label of one of the annotators we have that\\x00=T.Even restricting ourselves to the case of homogeneousannotators, depending on the rule with which we build thedataset we can have a more complex relationship betweenthe matrixTand\\x00.We also obtain generalization bounds in the case werean estimate of the agreement matrixMis not available andwe only have access to a scalar representation of the inter-annotator agreement, in particular we consider the casewhere the Cohen’s\\uf8ffis given.3.2.3 ObjectiveThe objective in both scenarios is to: i) usecMto estimatethe noise transition matrices (Tand\\x00); ii) leverage theseestimates to be able to learn from the noisy dataset in a morerobust manner; and iii) obtain generalization bounds for theresulting learning', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='learning methods.4. Main resultsWe divide the main contributions in three sections. Inthe ﬁrst section we show how to estimate the noise matri-cesTNext we indicate how to leverage these estimates tolearn for the datasets with noisy labels. Finally we obtainbounds,depending on the Rademacher complexity of theclass of functions,on the generalization gap for aboundedand Lipschitzloss function4.1. Estimation of the noise transition matricesWe start stating the following Lemma that allows us towrite the unknown matrixT(and its inverse), as a functionofDandM.Lemma 4.1.IfD12commutes withTwe have that:T=U⇤12UT(10)T\\x001=U⇤\\x0012UT(11)D\\x0012MD\\x0012=U⇤UT(12)whereU⇤UTis the eigenvalue decomposition ofD\\x0012MD\\x0012(i.e.Uis some orthogonal matrix and⇤is a diagonal positive deﬁnite matrix).A detailed discussion of when the commutativity as-sumption is satisﬁed is included in AppendixB. The proofof the previous Lemma can be ﬁnd in AppendixC.1.Note that we could use Lemma4.1to estimateTas', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='estimateTas fol-lows:bT=bUb⇤12MbUT(13)', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='3442', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='wherebUb⇤MbUTis the eigenvalue decomposition ofD\\x0012cMD\\x0012. However such estimate can result in matri-ces that are not doubly stochastic, or diagonally dominantdue to estimation errors. A more accurate estimate ofTcould be obtained asbT=⇡(bUb⇤12MbUT)where⇡is a projec-tion operator to the set of doubly stochastic, positive deﬁnitematrices with diagonal elements greater than 0.5 and non-negative entries (which is a convex set). We can obtain suchprojection by solving the following optimization problem:bT=⇡(bUb⇤12MbUT) = argminB||B\\x00bUb⇤12MbUT||22(14)s.t.B=BTXjBi,j=18iBi,j\\x0008i, jBi,i\\x000.58iNote that this optimization problem is convex becausethe constraints are linear and for symmetric matrices it holdsthat||bT\\x00bUb⇤12MbUT||22=\\x00max(bT\\x00bUb⇤12MbUT), which is aconvex function ofbT.To summarize,Tcan be estimated as follows.First,obtain an estimate ofM. Then obtain the eigenvalue de-composition ofD\\x0012cMD\\x0012=bUb⇤bUT(note that this de-composition always exists becauseD\\x0012cMD\\x0012is sym-metric).', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='sym-metric). Finally obtain the estimate as:bT:=⇡(bUb⇤12bUT).Note that once the estimate ofbTis obtained,b\\x00can beobtained since we assumed the label aggregating policy tobe known.Lemma 4.2.LetMa,bbe the agreement matrix for anno-tatorsaandbdeﬁned in Eq. (4) and[Ma,bbe the esti-mated agreement matrix deﬁned in Eq. (8) and let||.||pbethe matrix norm induced by thepvector norm. For everyp2[1,1]and for every\\x00>0, with probability at least1\\x00\\x00||Ma,b\\x00[Ma,b||p\\uf8ffrC22nln2C2\\x00.(15)wherePndenotes the probability according to which thentraining samples are distributed, i.e. we are assuming thatthe samples are independently drawn according the proba-bilityP.Proof.The proof can be found in AppendixC.2.From Lemma4.2it follows that ifcMis estimated as inEq. (9), sincecMis an average ofdMabit also holds that foreveryp2[1,1]and for every\\x00>0, with probability atleast1\\x00\\x00||M\\x00cM||p\\uf8ffrC22nln2C2\\x00.(16)Theorem 4.3.LetTbe the noise transition matrix deﬁnedas in Eq.(2)andbTits estimate (deﬁned as in Eq.(14)).With', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='as in Eq.(14)).With probability at least1\\x00\\x00:||T\\x00bT||2\\uf8ffC(pC+ 1)\\x00max(D)\\x00min(ˆT)r12nln2C2\\x00(17a)||T\\x001\\x00bT\\x001||2\\uf8ff9C(pC+ 1)\\x00max(D)\\x00min(ˆT)2r12nln2C2\\x00(17b)forn>C2(pC+1)2(ln(2C2)22\\x00min(ˆT)2.Proof.The proof can be found in AppendixC.3.From the previous theoremwe can notice thatthe errorin estimationofTdecays as1pnas a function ofn.4.2. Learning from noisy labelsIn this section we show how to leverage the estimates ofthe error rates to train the models.4.2.1 Posterior distribution of true labels as soft-labelsIt is noteworthy that if we have access to the labels providedby all annotators, the posterior probabilities of the true la-bels can be calculated leveragingTand Bayes’ Theorem asfollows:P(yi=c|y1,i,...,yH,i)|{z}:=pc,i/⌫cHYh=1P(yh,i|yi=c)|{z}=Tc,yh,i(18)we recall that⌫c=P(yi=c)and that the conditional prob-abilities on the r.h.s. are given byT. In our case we canuse our noisy transition estimates to estimate the posteriorprobabilities of the true labels, and afterwards we can usethese', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='we can usethese posteriors to train the classiﬁer.Lemma 4.4.For inﬁnite annotators the posterior distribu-tion over every sample calculated using the trueTcon-verges to the dirac delta distribution centered on the truelabel almost surely (i.e.limH!1pc,ia.s.=1(yi=c)).Proof.See AppendixC.5.We can use the posterior distributions as soft-labelsdeﬁning the following loss for the i-th sample:`(f(xi),y1,i,...,yH,i)=`(f(xi),¯pi)(19)where¯pi=[p1,i,···,pC,i]T. Or we can use the posteriordistributions to weight the loss function at thei-th sampleevaluated at each of the possible labels:`(f(xi),y1,i,...,yH,i)=CXc=1pc,i`(f(xi),ec)(20)', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='3443', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='whereecis the vector inRCwith1in thec-th position.Notice that for categorical cross entropy loss the two func-tions deﬁned above correspond, but in general they deﬁnetwo different loss functions.Note that these soft-labels are different from the ones ob-tained by averaging the annotators labels as is done in [25].The method using the posteriors exploits theTmatrix andthus more information than the simple mean of the valuesof the losses among annotators. We therefore expect thisto yield better results than the aggregation using the meanproposed in [25]. These considerations are supported bythe empirical results we obtained on synthetic datasets (seeSec.6).4.2.2 Robust loss functionsAnother way to leverage the estimate ofTis to use robustloss functions, like the forward and backward loss functionspresented in [15,16]. Let`(t, y)be a generic loss func-tion for the classiﬁcation task, with a little abuse of nota-tion we deﬁne`(t)=[`(t, e1),...,`(t, eC)]T. The back-ward and forward loss', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and forward loss functions are deﬁned in Eq. (21a)and Eq. (21b), respectively.lb(t, y)=(b\\x00\\x001`(t))y(21a)lf(t, y)=(`(b\\x00Tt))y(21b)To explain the notation in Eq. (21a) we are ﬁrst doing thedot product between the matrix\\x00\\x001and the vector`(t)andthen the dot product of the resulting vector withy. Theselosses leverage aggregated labels and therefore different ag-gregating techniques can be used, like majority vote. An-other possible aggregating technique that leverages the pos-terior probabilities is to assume that the true label is the onethat corresponds to the class that has the highest posteriorprobability.4.3. Generalizations gap boundsIn this section we derive generalization gap bounds forthe backward loss that depend on the noise transition ma-trix estimated in Eq. (14). Since we are only addressingthe problem for the backward loss,from now onwe willdenotethe backward lossbyl.Remark 1.If`(t, y)is Lipschitz with constantL, theloss functionl(t, y)is Lipschitz with Lipschitz', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='with Lipschitz constant||\\x00\\x001||2L.We will prove the following theorem in the case of\\x00=T. We emphasize that all the results apply also when\\x00\\x001=\\x00(T\\x001)and that the function that associate\\x00\\x001andT\\x001,\\x00is Lipschitz with respect to the normp, i.e. there ex-ists a Lipschitz constantL\\x00,ps.t.||\\x00(T\\x001)\\x00\\x00(bT\\x001)||p\\uf8ffL\\x00,p||T\\x001\\x00bT\\x001||p. The only difference is that in the boundwe will have a factorL\\x00,p.It has been proved, ﬁrst in [15] (Lemma 1) for the bi-nary classiﬁcation task and then in general for the multi-class case in [16] (Theorem 1) thatl(t, y)is an unbiasedestimator for`, i.e.E˜y|y[l(t,˜y)] =`(t, y).Lemma 4.5.Let`be a bounded loss function, with`2[0,µ], s.t. there exists a Lipschitz function↵, with LipschitzconstantL, so that`(f(x),y)=↵|f(x)\\x00y|. LetbRl(f)bethe empirical risk for the lossland letRl,Dbe the risk forlosslunder the distributionD, withlunbiased estimatorfor the loss`. We denote byˆlthe backward loss', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='backward loss obtainedusingbT.supf2F|ˆRˆl(f)\\x00Rl,D(f)|\\uf8ff\"L\\x00min(bT2)+µ\\x00min(D)\\x00min(ˆT)2r1nln\\x004C\\x00\\x00#Rn(F)g(C).withg(C)=6C2(pC+ 1)Theorem 4.6.Letlbe an unbiased estimator for`deﬁnedas in Eq.(21a), Denotingˆf= argminf(bRˆl(f)). It holds thatR`,D(ˆf)\\x00minf2FR`,D(f)\\uf8ff\"2L\\x00min(bT2)+µ\\x00min(D)\\x00min(ˆT)2r1nln\\x004C\\x00\\x00#Rn(F)g(C)withg(C)=6C2(pC+ 1)The proofs of Lemma4.5and Theorem4.6can be ﬁndin AppendixC. We observe that in all the previous theo-rems, the bounds found are alwaysdecreasing as one overthe square root of the number of samples.The above the-orem gives us a performance bound for the classiﬁer foundminimizing the backward lossl, i.e. the unbiased estimatorof the loss`on the noisy dataset. The bounds found de-pend on, the Rademacher complexity of the function spaceand the Lipschitz constant of the loss function.The impor-tance of these bounds lies in the fact that they allow us toobtain performance bounds for a model trained with noisydata that depends on values that we can estimate from thenoisy', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='from thenoisy dataset.In particular, there is no dependence on thetrue noise transition matrix of the annotators, as in otherwork [15] which is instead a quantity that cannot be knowna priori having access only to the training data. More in de-tail the bound depends onthe estimatenoise transition ma-trix, the number of classes in the dataset,the Rademachercomplexity and the Lipschitz constant, which we can takeas known a prioriand onthe distribution of ground truth,which in many cases it makes sense to assume uniform.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='3444', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='5. Cohen’s\\uf8ffWe can also consider the case where an estimate of theIAA matrixMis not available and we only have access toa scalar representation of the inter-annotator agreement likeCohen’s\\uf8ff. In this case we can only estimate one parameterand hence the matrixThas to be parameterized by a singleparameter that can be estimated.One particular example is the case where the noise isuniform among classes. Under these hypotheses,Tis a ma-trix with all values1\\x00pon the diagonal andpC\\x001off thediagonal.Lemma 5.1(Relationship betweenpand\\uf8ff).In the caseof classiﬁcation with uniform noise for two homogeneousannotatorswith noise ratep, i.e ifais one annotator,P(ya=i|y=j)=pifi6=j. If the distribution of theground-truth labels is uniform, it holds that:p=( 1\\x00C\\x001)(1\\x00p\\uf8ff)(22)with\\uf8ffthe Cohen’s kappa coefﬁcient of the two annotators(see AppendixA).Proof.The proof can be found in AppendixC.6.IfTis assumed to be of the form described above (withall diagonal elements equal to1\\x00pand all off-diagonalentries equal),', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='equal), it has one eigenvalue equal to1and all therest are equal to1\\x00pC(C\\x001)\\x001(this follows from the factthat in this caseTcan be written as a weighted summationof the identity and a rank-one matrix). Hence using Eq. (22)we get that\\x00min(T)=p\\uf8ff. The bounds from Theorem4.6holds replacing\\x00min(T)withp\\uf8ff. This allows us to obtainbound for the generalization gap of a classiﬁer trained withbackward loss even in the case where a single statistic onagreement between annotators is provided.6. Experimental resultsWe performed experiments to validate the effectivenessof the method we propose for estimatingbTby studying theerror in the estimation as a function of the number of sam-ples. We also performed experiments to show how the esti-matedTcan be leveraged to train classiﬁers in the presenceof noise labels. In particular we performed experimentsfor a classiﬁcation task on a synthetic dataset and on theCIFAR10-N dataset, comparing the performance of a clas-siﬁer trained using labels obtained by', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='labels obtained by some baseline aggre-gation method with the performance of a classiﬁer trainedusing the distribution of posteriors obtained from the esti-mation of T (Eq. (18)) as soft-labels.Estimation ofTWith these experiments we aim to vali-date the theoretical results of Sec.4.1. We generate variousmatricesTthat are symmetric, stochastic and diagonallydominant, the exact details about the generation ofTcanbe found in AppendixD.1. For each annotator we producetheir prediction according to the matrixT. We run experi-ments for the number of annotatorsH= 10,7,3,2. We re-port here the results forH= 10, and4classes, all the otherplots are in AppendixD.1. In Fig.4(as well as the the plotsin the Appendix) we can be observed that the error in the es-timation decreases as1pnwithnnumber of samples, whichis in agreement with the bound provided in Theorem4.3.We also observed that, as expected, the estimation becomesmore accurate as the number of annotators increases.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Figure 1. Error in the estimation ofTfor4classes and10an-notators. The plots are obtained by averaging different admissiblematricesT(see AppendixB) and averaged over matrices that havethe same minimum eigenvalues rounded to the ﬁrst decimal.Classiﬁcation task with synthetic dataWe consider aclassiﬁcation task with a synthetic dataset. The features aregenerated uniformly in[0,1]2. The assignment of labels (y)is done by following the label distribution established foreach experiment, separating the space with lines parallelto the bisector of the ﬁrst and third quadrants More infor-mation on how the class distributions are generated can befound in AppendixD.2.For each dataset annotations are generated accordingto the noise transition matrixT. Various combinationsofTare tested that respect the assumptions of symmetry,stochasticity and diagonally dominance, as well as beingcommutative with D (more details can be found in Ap-pendixB).The number of annotators is variable in the set{3,5}. See', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the set{3,5}. See AppendixD.2for implementations details.LossesWe use categorical cross entropyas loss function.We use both hard labels and soft labels to trainthemodels.To train the models with hard labels an aggregationmethod is needed to obtain one ﬁnal label from the anno-tators. We consider random and majority vote. In randomaggregation the ﬁnal label is randomly picked from the la-bels of the annotators. In majority vote the ﬁnal label is the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='3445\\none with the most amount of votes (the mode), if the modeis not unique, we randomly choose one of the most votedclasses. As soft-labels we consider the relative frequencyamong annotators and the posterior distribution accordingto Eq. (18). In the case of frequency for each sample we av-erage the one-hot encoded annotations. Notice that random,majority vote and frequency soft labels do not leverage theestimate ofTwhile the posterior does. In Fig.2we reportthe results for4classes with distribution(0.4,0.1,0.4,0.1)and3annotators.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Figure 2. Comparison between performance of Cross EntropyLoss using majority vote, random aggregation method or theposteriors (posterior) and relative frequency (average) as soft la-bels.On the y-axis the accuracy on a clean dataset and on the x-axis the values of the minimum on the diagonal ofT. Small val-ues of the minimum diagonal value mean a noisy dataset, while theminimum is1in the noise-free case. The results are obtained for3annotators and4classes, by averaging on different ammissiblematricesT(see AppendixB) that have the same minimum diago-nal values rounded to the ﬁrst decimal. The error bands show themaximum and minimum performance for each method.We use accuracy with respect to a clean dataset as per-formance metric.Our results show that using the posteri-ors distribution ,as soft labels, allows for better performancethan using the average of the labels assigned by annotatorsand than using majority vote or random aggregation.Our method is shown to be more robust to the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='more robust to the noise andis also the one with less variance in the results.This con-ﬁrms our hypotheses that by leveraging the matrixbTbetterclassiﬁcation accuracy can be achieved.Experiments on CIFAR10-NThe CIFAR10-N dataset1contains CIFAR-10 train images with noisy labels annotatedby humans using Amazon Mechanical Turk. Each image islabelled by three independent annotators. Table1shows theaccuracy achieved using the different aggregation methods.For this experiment we used Resnet34 [10] with and with-out pre-training. In both cases, our approach of aggrega-1http://www.noisylabels.comtion achieves the best performance. Note that in this datasetthere are no guarantees that the assumptions we made onTare satisﬁed, however the method is still applicable withpositive results.Aggregation MethodPretrainedNot-Pretrainedrandom0.718±0.0350.579±0.023majority vote0.740±0.0170.590±0.006average0.762±0.0120.637±0.016posteriors (ours)0.794±0.0050.652±0.014Table 1. Test Accuracy on CIFAR10-N with', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='on CIFAR10-N with Resnet347. Concluding remarksWe have addressed the problem of learning from noisylabels in the case where the dataset is labeled by annota-tors that occasionally make mistakes. We have introduceda methodology to estimate the noise transition matrixTofthe annotators given the IAA. We further showed differenttechniques to leverage this estimate to learn from the noisydataset in a robust manner. We have shown theoreticallythat the methods we introduce are sound.We supported ourmethodology with some experiments that conﬁrms our es-timation of the noise transition matrix is valid and that thiscan be leveraged in the learning process to obtain better per-formance.LimitationsThe main limitation of our current approachto estimateTis that it only considers the case whereTissymmetric andDassumed to be knownand commute withT. Extending the results to the case whereTmight not besymmetric and differentamongannotators is one possiblefuture research direction.AcknowledgementsWe', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='acknowledge ﬁnancial support from NRRP MURproject PE0000013-FAIR. This research was partially sup-ported by MIUR under the grant “Dipartimenti di eccel-lenza 2018–2022” of the Department of Computer Scienceand the Department of Computer Engineering at SapienzaUniversity of Rome. It was also partially supported bythe ERC Advanced Grant 788893 “AMDROMA” , the ECH2020RIA project “SoBigData++” (871042), the MIURPRIN project “ALGADIMAR”, and the project SERICS(PE00000014) under the NRRP MUR program funded bythe EU-NGEU.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='3446', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='References[1]J. Cohen. A coefﬁcient of agreement for nominalscales.Educational and Psychological Measurement,20(1):37–46, 1960.1,11[2]K. M. Collins, U. Bhatt, and A. Weller. Eliciting andlearning with soft labels from every annotator, 2022.2[3]A. P. Dawid and A. M. Skene. Maximum likelihoodestimation of observer error-rates using the em algo-rithm.Journal of the Royal Statistical Society. SeriesC (Applied Statistics), 28(1):20–28, 1979.2[4]L. Feng, S. Shu, Z. Lin, F. Lv, L. Li, and B. An.Can cross entropy loss be robust to label noise?In C. Bessiere, editor,Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial In-telligence, IJCAI-20, pages 2206–2212. InternationalJoint Conferences on Artiﬁcial Intelligence Organiza-tion, 7 2020. Main track.2[5]J. Fleiss et al. Measuring nominal scale agree-ment among many raters.Psychological Bulletin,76(5):378–382, 1971.1[6]A. Ghosh, H. Kumar, and P. S. Sastry. Robust lossfunctions under label noise for deep neural', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='for deep neural networks.InProceedings of the Thirty-First AAAI Conferenceon Artiﬁcial Intelligence, AAAI’17, page 1919–1925.AAAI Press, 2017.2[7]A. Ghosh, N. Manwani, and P. Sastry. Making riskminimization tolerant to label noise.Neurocomputing,160:93–107, 2015.2[8]D. Hendrycks, M. Mazeika, D. Wilson, and K. Gim-pel. Using trusted data to train deep networks on la-bels corrupted by severe noise. In S. Bengio, H. Wal-lach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,and R. Garnett, editors,Advances in Neural Infor-mation Processing Systems, volume 31. Curran Asso-ciates, Inc., 2018.2[9]R. A. Horn and C. R. Johnson.Matrix Analysis. Cam-bridge University Press, USA, 2nd edition, 2012.14[10]A. Khetan, Z. C. Lipton, and A. Anandkumar. Learn-ing from noisy singly-labeled data, 2017.8[11]R. Meir and T. Zhang. Generalization error bounds forbayesian mixture algorithms.J. Mach. Learn. Res.,4:839–860, dec 2003.17[12]A. Menon, B. V . Rooyen, C. S. Ong, andB. Williamson. Learning from corrupted', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='from corrupted binary la-bels via class-probability estimation. In F. Bach andD. Blei, editors,Proceedings of the 32nd InternationalConference on Machine Learning, volume 37 ofPro-ceedings of Machine Learning Research, pages 125–134, Lille, France, 07–09 Jul 2015. PMLR.2[13]A. K. Menon, A. S. Rawat, S. J. Reddi, and S. Kumar.Can gradient clipping mitigate label noise? InInterna-tional Conference on Learning Representations, 2020.2[14]M. Mohri, A. Rostamizadeh, and A. Talwalkar.Foun-dations of Machine Learning. The MIT Press, 2012.17[15]N. Natarajan, I. S. Dhillon, P. K. Ravikumar, andA. Tewari. Learning with noisy labels. In C. Burges,L. Bottou, M. Welling, Z. Ghahramani, and K. Wein-berger, editors,Advances in Neural Information Pro-cessing Systems, volume 26. Curran Associates, Inc.,2013.1,2,6[16]G. Patrini, A. Rozza, A. Krishna Menon, R. Nock, andL. Qu. Making deep neural networks robust to labelnoise: A loss correction approach. InProceedings ofthe IEEE conference on computer', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='on computer vision and patternrecognition, pages 1944–1952, 2017.1,2,6[17]J. Peterson, R. Battleday, T. Grifﬁths, and O. Rus-sakovsky. Human uncertainty makes classiﬁcationmore robust. InProceedings - 2019 InternationalConference on Computer Vision, ICCV 2019, Pro-ceedings of the IEEE International Conference onComputer Vision, pages 9616–9625, United States,Oct. 2019. Institute of Electrical and Electronics En-gineers Inc.2[18]J. E. Potter. Matrix quadratic solutions.SIAM Journalon Applied Mathematics, 14(3):496–501, 1966.12[19]A. Purpura, G. Silvello, and G. A. Susto. Learning torank from relevance judgments distributions, 2022.2[20]V . C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez,C. Florin, L. Bogoni, and L. Moy. Learning fromcrowds.Journal of Machine Learning Research,11(43):1297–1322, 2010.2[21]H. Song, M. Kim, D. Park, Y . Shin, and J.-G. Lee.Learning from noisy labels with deep neural networks:A survey, 2020.1[22]A. Uma, T. Fornaciari, D. Hovy, S. Paun, B. Plank,and M. Poesio. A', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='M. Poesio. A case for soft loss functions.Pro-ceedings of the AAAI Conference on Human Compu-tation and Crowdsourcing, 8(1):173–177, Oct. 2020.2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='3447', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content=\"[23]V . N. Vapnik.Statistical Learning Theory. Wiley-Interscience, 1998.1[24]Y . Wang, X. Ma, Z. Chen, Y . Luo, J. Yi, and J. Bai-ley. Symmetric cross entropy for robust learning withnoisy labels. InIEEE International Conference onComputer Vision, 2019.2[25]J. Wei, Z. Zhu, T. Luo, E. Amid, A. Kumar, and Y . Liu.To aggregate or not? learning with separate noisy la-bels, 2022.2,6[26]X. Xia, T. Liu, N. Wang, B. Han, C. Gong, G. Niu, andM. Sugiyama. Are anchor points really indispensablein label-noise learning? In H. Wallach, H. Larochelle,A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar-nett, editors,Advances in Neural Information Pro-cessing Systems, volume 32. Curran Associates, Inc.,2019.1,2,3[27]Y . Yao, T. Liu, B. Han, M. Gong, J. Deng, G. Niu, andM. Sugiyama. Dual t: Reducing estimation error fortransition matrix in label-noise learning. InProceed-ings of the 34th International Conference on NeuralInformation Processing Systems, NIPS’20, Red Hook,NY , USA, 2020. Curran\", metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content=', USA, 2020. Curran Associates Inc.1,2,3[28]M. Zhang, J. Lee, and S. Agarwal. Learning fromnoisy labels with no change to the training process.In M. Meila and T. Zhang, editors,Proceedings of the38th International Conference on Machine Learning,volume 139 ofProceedings of Machine Learning Re-search, pages 12468–12478. PMLR, 18–24 Jul 2021.2[29]Z. Zhang and M. R. Sabuncu. Generalized crossentropy loss for training deep neural networks withnoisy labels. InProceedings of the 32nd Interna-tional Conference on Neural Information ProcessingSystems, NIPS’18, page 8792–8802, Red Hook, NY ,USA, 2018. Curran Associates Inc.2[30]Z. Zhu, Y . Song, and Y . Liu. Clusterability as an al-ternative to anchor points when learning with noisylabels, 2021.3[31]Z. Zhu, J. Wang, and Y . Liu. Beyond images: La-bel noise transition matrix estimation for tasks withlower-quality features. In K. Chaudhuri, S. Jegelka,L. Song, C. Szepesvari, G. Niu, and S. Sabato, ed-itors,Proceedings of the 39th International', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='39th International Confer-ence on Machine Learning, volume 162 ofProceed-ings of Machine Learning Research, pages 27633–27653. PMLR, 17–23 Jul 2022.2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='3448', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Histopathology Whole Slide Image Analysis with Heterogeneous Graph\\nRepresentation Learning\\nTsai Hor Chan1,∗, Fernando Julio Cendra1,2∗, Lan Ma2, Guosheng Yin1,3, Lequan Yu1\\n1Department of Statistics and Actuarial Science, The University of Hong Kong\\n2TCL Corporate Research Hong Kong\\n3Department of Mathematics, Imperial College London\\n{hchanth, fcendra }@connect.hku.hk, rubyma@tcl.com, guosheng.yin@imperial.ac.uk, lqyu@hku.hk\\nAbstract\\nGraph-based methods have been extensively applied to\\nwhole slide histopathology image (WSI) analysis due to the\\nadvantage of modeling the spatial relationships among dif-\\nferent entities. However, most of the existing methods fo-\\ncus on modeling WSIs with homogeneous graphs ( e.g., with\\nhomogeneous node type). Despite their successes, these\\nworks are incapable of mining the complex structural re-\\nlations between biological entities ( e.g., the diverse inter-\\naction among different cell types) in the WSI. We propose', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='a novel heterogeneous graph-based framework to lever-\\nage the inter-relationships among different types of nu-\\nclei for WSI analysis. Specifically, we formulate the WSI\\nas a heterogeneous graph with “nucleus-type” attribute to\\neach node and a semantic similarity attribute to each edge.\\nWe then present a new heterogeneous-graph edge attribute\\ntransformer (HEAT) to take advantage of the edge and\\nnode heterogeneity during massage aggregating. Further,\\nwe design a new pseudo-label-based semantic-consistent\\npooling mechanism to obtain graph-level features, which\\ncan mitigate the over-parameterization issue of conven-\\ntional cluster-based pooling. Additionally, observing the\\nlimitations of existing association-based localization meth-\\nods, we propose a causal-driven approach attributing the\\ncontribution of each node to improve the interpretability\\nof our framework. Extensive experiments on three pub-\\nlic TCGA benchmark datasets demonstrate that our frame-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='work outperforms the state-of-the-art methods with consid-\\nerable margins on various tasks. Our codes are available\\nat https://github.com/HKU-MedAI/WSI-HGNN.\\n1. Introduction\\nHistopathology slides provide rich information on diag-\\nnosis and treatment planning for many cancer diseases. The\\n*The first two authors contributed equally to this work.\\nFigure 1. Left: Input WSI. Middle: A WSI with selected patches\\nand associated node types. (Black - no label; cyan - neoplastic;\\nred - inflammatory; blue - connective; yellow - dead; green - non-\\nneoplastic epithelial). Right: Constructed heterogeneous graph\\nwith different types of nodes and edge attributes (Illustrative).\\nrecent technological advancements in tissue digital scanners\\nfacilitate the development of whole slide histopathology im-\\nage (WSI) analysis. However, traversing through the WSI\\nwith diverse magnifications is time-consuming and tedious\\nfor pathologists due to the large-scale nature of the WSI', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='(e.g., its typical size is 60,000 ×60,000 pixels). Hence deep\\nlearning techniques play an important role as they introduce\\naccurate and automated analysis of WSIs, which can signif-\\nicantly relieve the workload of pathologists.\\nSince it is difficult to fit the complete WSI into the mem-\\nory, most of the works adopt multiple instance learning\\n(MIL) to divide the WSI into instances and then aggre-\\ngate them for WSI analysis. However, these methods op-\\nerate on bags of instances that do not emphasize the inter-\\nrelationships between these instances. Recently, the emer-\\ngence of graph neural networks (GNNs) has made large\\nprogress in representing the spatial relationships between\\ninstances. As a result, there are many attempts to represent\\nthe WSIs as graphs of instances. Figure 1 presents an exam-\\nple of a graph constructed from WSI. Unlike convolutional\\nneural networks (CNNs) that aggregate features based on\\nlocality in the Euclidean space, GNNs focus on locality on', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='graph topology, which offers more flexibility in analyzing\\nthe deep connections between features in the image data be-\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n15661\\nyond the spatial locality [1]. For example, GNNs are able\\nto learn relational information and distinguish cells based\\non their apposition to tumor cells, or normal stroma (i.e.,\\ncells which are tumor-infiltrating lymphocytes or from an\\nadjacency inflammatory response), which are important for\\nprognosis [5, 27].\\nHowever, existing paradigms on graph-based WSI anal-\\nysis focus on representing the WSI with a homogeneous\\ngraph structure and then predicting the response via vanilla\\nGNNs with cluster-based pooling (i.e., based on similarities\\nof node embeddings). Despite their successes, these meth-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ods suffer from several drawbacks: (i) GNNs on homoge-\\nneous graphs focus on aggregating direct relational infor-\\nmation from neighboring nodes, where the complex rela-\\ntional information of the graphs is often neglected. (ii) For\\ndifferent graphs, the clusters defined by similarities between\\nnode embeddings have inconsistent meanings. This intro-\\nduces a large degree of freedom in parameters and leads to\\nover-parameterization issue [2]. Therefore, GNNs tend to\\neasily overfit due to a lack of identifiability [14].\\nIn view of these limitations, we propose a novel frame-\\nwork for WSI analysis, which leverages a heterogeneous\\ngraph to learn the inter-relationships among different types\\nof nodes and edges. The heterogeneous graph introduces a\\n“nucleus-type” attribute to each node, which can serve as\\nan effective data structure for modeling the structural inter-\\nactions among the nuclei in the WSI. To tackle the aggrega-\\ntion process in the heterogeneous graph, we propose a novel', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='heterogeneous-graph edge attribute transformer (HEAT) ar-\\nchitecture which can take advantage of the edge and node\\nheterogeneity. Thus, the diverse structural relations among\\ndifferent biological entities in the WSI can be incorporated\\nto guide the GNN for more accurate prediction. Further,\\nto obtain the graph-level representations for slide-level pre-\\ndiction, we propose a semantic-consistent pooling mecha-\\nnism — pseudo-label (PL) pooling, which pools node fea-\\ntures to graph level based on clusters with a fixed definition\\n(i.e., nucleus type). The proposed PL pooling can regularize\\nthe graph pooling process by distilling the context knowl-\\nedge (i.e., pathological knowledge) from a pretrained model\\nto alleviate the over-parameterization issue [2]. Addition-\\nally, we propose a Granger causality [13] based localization\\nmethod to identify the potential regions of interest with clin-\\nical relevance to provide more insights to pathologists and', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='promote the clinical usability of our approach.\\nWe extensively evaluate our method on three TCGA pub-\\nlic benchmark datasets, including colon adenocarcinoma\\ncancer (COAD) and breast invasive carcinoma (BRCA)\\ndatasets from the TCGA project [35] and the Camelyon\\n16 dataset [3], and compare to various latest state-of-the-art\\n(SOTA) methods. Our method outperforms the competitors\\non cancer staging, cancer classification, cancer typing, and\\nlocalization tasks.2. Related Works\\nMultiple Instance Learning on WSIs. Existing WSI anal-\\nysis approaches generally adopt MIL [5,7,12,26,30,33,41],\\nwhich first divide the WSI into fixed-size patches and\\nthen compress the information of these patches into low-\\ndimensional vectors. Conventional methods aggregate bags\\nof instances to learn WSI-level features for final predictions.\\nTellez et al. [30] compress the WSI-level image into em-\\nbedding vectors and use a standard CNN to perform patch-\\nlevel and WSI-level cancer classification. These CNN-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='based methods analyze local areas in the Euclidean space on\\nfixed connectivity (i.e., fixed-size kernels), limiting the per-\\nformance beyond the spatial locality. Graph-based methods\\n[5,15,41] have recently been proposed, which model the in-\\nteractions between instances via graphs. Their capability of\\nmodeling instances based on graph topology provides more\\nflexibility to analyze complex structures of WSIs. Chen\\net al. [5] propose patch-GCN, a method of modeling WSI\\nwith homogeneous graphs, and regress survival data with\\na graph convolutional neural network (GCN) [36]. Zheng\\net al. [41] propose a graph-based MIL method using graph\\ntransformer networks [40]. In spite of their power, most of\\nthese WSI methods use homogeneous graphs, which limits\\nthe information mined from WSIs. A recent method [15] is\\nproposed to model WSIs with heterogeneous graphs, where\\nthe heterogeneity in each patch is introduced by different\\nresolution levels. However, it only considers the resolution', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='level heterogeneity of patches, with insufficient ability to\\nmodel the complex contextual interaction between patches\\nin the same resolution level.\\nGraph Neural Networks. Although the SOTA GNNs\\nhave shown great successes in many problem domains\\n[16, 19, 20], they are mostly focused on homogeneous\\ngraphs [32, 36, 37, 40, 42]. These architectures extract the\\nlocality information on the graph topology and learn the\\ngraph representations by performing aggregation on neigh-\\nboring nodes. However, the potential heterogeneity in\\nnodes and edges is not incorporated by these homogeneous\\nGNN algorithms, and therefore their capability in mining\\nthe structural information is limited. Several works attempt\\nto address the heterogeneity in their architectural designs\\n[16, 28, 34] and assume that the relation type is finite and\\ndiscrete. However, when modeling images with graphs, the\\nheterogeneity in relations is typically continuous (e.g., the\\nsimilarity between nodes) or high-dimensional. Although', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='there are several attempts [5, 10] to extend SOTA GNNs\\n[32, 36] to incorporate edge attributes, their works are lim-\\nited to homogeneous graphs.\\nGraph Pooling. Graph pooling aims to aggregate node-\\nlevel features to obtain graph-level features. Conventional\\nmethods [36] directly take the average of node-level fea-\\ntures to extract graph-level features, which tends to over-\\n15662\\nFigure 2. The paradigm of our proposed heterogeneous graph-based WSI analysis framework, which includes heterogeneous graph con-\\nstruction, heterogeneous-graph edge attribute transformer (HEAT) for structural information aggregation, pseudo-label-based (PL) graph\\npooling for slide-level prediction and casual-driven localization.\\nsmooth the signals of the nodes and cannot generate rep-\\nresentative graph-level features. Recently, there is exten-\\nsive development of graph pooling algorithms based on the\\nclusters of the embeddings [6, 15, 25]. However, the clus-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ters constructed based on similarity are inconsistent across\\ngraphs. This leads to a large degree of freedom in parame-\\nters which easily causes overfitting. A semantic-consistent\\npooling method is therefore needed.\\nExplaining GNNs. Despite the success of graph neural net-\\nworks, their poor interpretability of the parameters makes\\nthem notoriously recognized as “blackboxes”. With the ad-\\nvances in network attribution methods [29], extensive at-\\ntempts have been made to open such “blackboxes” [24,39].\\nGenerating network explanation is an important qualitative\\nstep in the WSI analysis since it can highlight the abnor-\\nmal regions for further investigation. Conventional explain-\\ners try to find the associations between the parameters in\\ndeep neural networks (or the nodes in GNNs) and the pre-\\ndictions. GNNExplainer [39] is the SOTA method explain-\\ning the contributions of node features to the GNN pre-\\ndictions. It trains feature masks on each node and edge', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='feature to minimize the prediction loss of a trained GNN.\\nPGExplainer [24] shares the same objective as GNNEx-\\nplainer and trains a generative model to generate explana-\\ntions. Recently, there has been emerging attention in gener-\\nating causal explanations for GNNs [23,29], and most of the\\nmethods focus on the Granger causality as the explanationobjective. Gem [23] trains explanation generators from the\\ncausal perspective. Causal explainers attempt to provide ex-\\nplanations of features that are causal rather than associated\\nwith the neural network prediction.\\n3. Preliminaries\\nHeterogeneous Graph : A heterogeneous graph is defined\\nby a graph G=(V,E,A,R), where V,E,Arepresent the set\\nof entities (vertices or nodes), relations (edges), and entity\\ntypes, respectively. And Rrepresents the space of edge\\nattributes. For v∈ V,vis mapped to an entity type by a\\nfunction τ(v)∈ A . An edge e= (s, r, t)∈ E links the\\nsource node sand the target node t, and ris mapped to an', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='edge attribute by a function ϕ(e) =r∈ R . Every node v\\nhas a d-dimensional node feature x∈ X , where Xis the\\nembedding space of node features.\\nGranger Causality [13, 23]: Let Ibe all the available in-\\nformation and I−Xbe the information excluding variable\\nX. If we can make a better prediction of YusingIthan\\nusingI−X, we conclude that XGranger-causes Y.\\nWSI Classification : Given a WSI Xand a heterogeneous\\ngraphGconstructed from X, we wish to predict the label\\nywith a GNN model M. We also aim to assign an im-\\nportance score f(v)to each node v∈ V inGas the causal\\ncontribution of each patch to the prediction for localization.\\n15663\\nFigure 3. Examples of introduced meta-relations in a heteroge-\\nneous graph constructed from a WSI.\\n4. Methodology\\n4.1. Heterogeneous Graph Construction\\nWe introduce our methodology of modeling the WSI\\nwith a heterogeneous graph. Figure 2 presents the overall\\nworkflow of our proposed framework. We adopt the com-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='monly used OTSU thresholding algorithm [5] and sliding\\nwindow strategy to crop each WSI into non-overlapping\\npatches. Uninformative patches with backgrounds are re-\\nmoved. These patches define the nodes of the graph con-\\nstructed. To define the corresponding node type, we use\\nHoverNet [12] pretrained on the PanNuke dataset [8] to\\nclassify the patches into predefined types. HoverNet de-\\ntects nuclei in each patch and assigns types to these nuclei.\\nBy majority votes, we take the most frequently predicted\\nnucleus type to be the type of the patch. Figure 1 presents\\nan example of a WSI with patches selected from the OTSU\\nand node types generated by HoverNet [12]. We use a pre-\\ntrained feature encoder (i.e., KimiaNet [26]) to obtain the\\nembeddings of each patch, which serves as the features of\\neach node in the heterogeneous graph.\\nBased on the nodes and node features, we define the\\nedges and edge attributes between the patches. For each\\nnode v∈ V, we use the k-nearest neighbor algorithm to', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='findknodes that have the most similar features to that node,\\nand connect edges between node vand these neighboring\\nnodes. For each edge, we compute the Pearson R corre-\\nlation between the head and tail node features as the edge\\nattributes. The edge attributes introduce heterogeneity in\\nedges and highlight meta-relations in the WSI. We adopt\\ndata augmentations (e.g., randomly removing some edges)\\nduring training to alleviate the potential noises introduced\\nby the edge attributes. As a result, we obtain a hetero-\\ngeneous graph Gwith heterogeneity introduced by differ-\\nent node types and edge attributes. As shown in Figure 3,\\na heterogeneous graph outlines the meta-relations between\\nthe nuclei in a WSI. Mining these meta-relations can reveal\\nthe structural interactions between the cells, leading to im-\\nproved performances on different tasks.4.2. Heterogeneous Edge Attribute Transformer\\nThe conventional graph attention mechanism is inca-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='pable of tackling the heterogeneity of the graph. Inspired\\nby the transformer architecture [31] and its extension on\\ngraphs [16, 17, 40], we propose a new graph aggrega-\\ntion layer, named the Heterogeneous Edge Attribute Trans-\\nformer (HEAT) layer, to aggregate the structural relations\\nbetween biological entities in the built heterogeneous graph.\\nWe explicitly incorporate the node types and continuous\\nedge features into the aggregation process, which guides\\nthe learning of edge similarities. Our proposed architec-\\nture also generalizes the existing architecture to incorporate\\ncontinuous or high-dimensional edge attributes and simpli-\\nfies the use of linear layers to avoid overfitting led by model\\nover-parameterizations.\\nFor each edge e= (s, r, t)and each attention head i, we\\nproject the target node tinto a query vector with a linear\\nprojection layer Wi\\nτ(s), and the source node into a key vec-\\ntor with Wi\\nτ(t). We also compute the value vector hi\\nvalueof', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='valueof\\neach source node by the same projection layer Wi\\nτ(s)\\nhi\\nkey=Wi\\nτ(s)H(l−1)\\ns,hi\\nquery=Wi\\nτ(t)H(l−1)\\nt,\\nhi\\nvalue=Wi\\nτ(s)H(l−1)\\ns,\\nwhere H(l−1)\\nv is the input node feature for node v∈ Vfrom\\nthe(l−1)-th layer. These projection layers can project node\\nfeatures of various node types into a node-type-invariant\\nembedding space. The edge attributes from the (l−1)-th\\nlayer h(l−1)\\ne are also projected to h′\\ne=Wedgeh(l−1)\\ne by a\\nlinear projection layer Wedge. After projecting the node em-\\nbeddings, we compute the dot-product similarity between\\nthe query and key vectors and further multiply the linear\\ntransformed edge attribute to the similarity score to incorpo-\\nrate the edge attributes in G. We then concatenate the scores\\nfrom each head and take the softmax of the score (i.e., over-\\nweights of incoming edges for all neighboring nodes) to ob-\\ntain the final attention scores to the value vector hi\\nvalue,\\nAttention (e) =softmax\\n∀s∈N(t)\\x10\\n∥\\ni∈[1,h]ATT(e, i)\\x11\\n,\\nATT(e, i) =\\x10\\nhi\\nkeyh′\\nehi', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='hi\\nkeyh′\\nehi\\nquery\\x11\\n/√\\nd,\\nwhere N(t)is the set of all the source nodes pointing to\\ntarget node t,dis the dimension of node embeddings,\\nATT(e, i)represents the i-head attention score of edge e,\\n∥i∈[1,h]is the concatenation operator concatenating the at-\\ntention scores from all heads and Attention (e)represents\\nthe final attention score of the edges aggregating all the\\nheads. We multiply the attention score obtained by the value\\nvector to obtain the output features. By doing so, the out-\\nput features contain both the node-type and edge-attribute-\\nspecific information. Hence the HEAT layer can capture the\\n15664\\nAlgorithm 1 The HEAT algorithm.\\nInput:\\nHeterogeneous graph Gl−1with node features\\n{H(l−1)\\ni,∀i∈ V} and edge attribute {h(l−1)\\ne,∀e∈ E} ;\\nNode-type specific projection layers {Wi\\na,∀a∈ A}\\nEdge attribute transformation layer Wedge.\\nOutput: The updated graph Glwith node features\\n{H(l)\\ni,∀i∈ V} , and the edge features {h(l)\\ne,∀e∈ E}\\n1:Initialize projection layers for each node type', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='2:fore= (s, t)∈ Edo\\n3: hi\\nkey=Wi\\nτ(s)H(l−1)\\ns ▷Project the source node\\n4: hi\\nvalue=Wi\\nτ(s)H(l−1)\\ns ▷Compute value vector\\n5: hi\\nquery=Wi\\nτ(t)H(l−1)\\nt ▷Project the target node\\n6: h′\\ne←Wedge·h(l−1)\\ne ▷Project the edge attribute\\n7: ATT(e, i) =\\x10\\nhi\\nkeyh′\\nehi\\nquery\\x11\\n/√\\nd\\n8: Attention (e) =softmax\\n∀s∈N(t)(∥i∈[1,h]ATT(e, i))\\n9: h(l)\\ne←h′\\ne ▷Compute latent edge features\\n10:end for\\n11:fort∈ V do\\n12: H(l)\\nt=⊕∀s∈N(t)(∥i∈[1,h]hi\\nvalue·Attention (e))\\n13:end for\\n14:return Gl\\nstructural information in Gby transforming the node fea-\\ntures from different node types. It can also model different\\nsemantic relations since edge attributes are included in the\\naggregation.\\nFinally, we perform target-specific aggregation to update\\nthe feature of each target node by averaging its neighbor-\\ning node features. We concatenate all hattention heads to\\nobtain the attention vector for each pair of source and tar-\\nget nodes. For each target node t, we conduct a softmax', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='operation on all the attention vectors from its neighboring\\nnodes and then aggregate the information of all neighbor-\\ning source nodes of ttogether. The updated node features\\nH(l)\\ntforGlcan be represented as\\nH(l)\\nt=M\\n∀s∈N(t)\\x10\\n∥\\ni∈[1,h]hi\\nvalue·Attention (e)\\x11\\n,\\nwhere ⊕is an aggregation operator (e.g., mean aggrega-\\ntion). The updated graph Glis returned as the output of the\\nl-th HEAT layer. Algorithm 1 demonstrates the overall pro-\\ncess of our proposed HEAT layer.\\n4.3. Pseudo-label Graph Pooling\\nWe introduce a novel pooling method — pseudo-label\\n(PL) pooling, to aggregate information with respect to the\\npseudo-labels (i.e., node types) predicted from a pretrained\\nteacher network (e.g., HoverNet [12]). Unlike conventionalAlgorithm 2 The PL-Pool Algorithm\\nInput: Heterogeneous graph Gwith node features\\n{Hi,∀i∈ V} and node type set A.\\nOutput: The pooled graph-level feature S∈R|A|×d.\\n1:Initialize readout layers for each node type a∈ A.\\n2:Initialize aggregate feature matrix S.\\n3:fora∈ A do', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='3:fora∈ A do\\n4: Xa←feature matrix of nodes of type a\\n5: ha←readout a(Xa)▷Pool feature with readout\\nlayer\\n6: Sa=ha▷Assign pooled feature to the a-th row\\nofS\\n7:end for\\n8:return S\\nmethods of pooling features based on clusters, we define\\nclusters using a pretrained node classifier. Pooling from\\npseudo-labels ensures the semantic consistency in cluster\\ndefinitions and distills the context knowledge (e.g., nuclei\\nfeatures) from the teacher network. Specifically, for each\\nnode type a, we pool all node features belonging to type\\nainto a single vector hawith a readout layer. The pooled\\nfeatures from each node type are then aggregated into a fea-\\nture matrix S∈R|A|×d. The graph level feature is then\\ndetermined by another readout layer (e.g., mean readout).\\nAlgorithm 2 presents the workflow of the proposed PL\\nPooling. By pooling with the pseudo-labels, we are able to\\ncluster patch representation according to nuclei types, such\\nthat the graph-level features are enhanced with the prior', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='knowledge on nuclei type distributions. The detailed mech-\\nanism of the PL Pool is presented in the supplementary ma-\\nterials. We also perform an ablation study in Table 4 and\\nshow that PL Pooling outperforms existing pooling meth-\\nods in cancer classification tasks.\\n4.4. Prior Knowledge Regularization\\nHere we discuss the motivation for introducing prior\\nknowledge in our proposed HEAT and PL pooling algo-\\nrithms. In the context of WSI analysis when the data\\nare scarce, while data distributions are sparse and high-\\ndimensional. The curse of high dimensionality makes the\\nsampling distributions difficult to approximate the prop-\\nerties of true distributions of the WSIs. This leads to\\na significant gap between training and testing distribu-\\ntions. Hence regularization techniques are needed to reduce\\nthe model variance and mitigate performance deterioration\\nwhen transferring the model from training to testing envi-\\nronments. Since WSI data contain enriched prior knowl-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='edge (e.g., the interaction among different cell types), inte-\\ngrating such knowledge into the framework regularizes the\\nmodel, such that the testing performance improves. There-\\nfore, we design the above two designs by integrating prior\\n15665\\nCancer Staging (Four Stages) Cancer Classification\\nModel AUC Accuracy Macro-F1 AUC Accuracy Macro-F1\\nABMIL [18] 53.8 (3.7) 19.2 (7.8) 35.8 (4.4) 97.7 (2.3) 98.3 (0.9) 95.8 (2.2)\\nDSMIL [21] 59.3 (1.4) 35.7 (5.7) 37.9 (2.8) 99.7 (0.2) 98.6 (0.5) 96.9 (0.9)\\nReMix [38] 58.3 (1.5) 33.9 (7.8) 24.8 (7.5) 94.3 (3.4) 96.0 (4.6) 92.8 (5.9)\\nPatchGCN [5] 62.5 (4.9) 38.2 (3.1) 38.5 (5.7) 91.1 (5.3) 97.1 (2.0) 98.8 (1.0)\\nGTNMIL [41] 54.2 (2.6) 29.3 (1.4) 24.3 (3.9) 97.3 (2.6) 98.1 (1.3) 95.9 (2.4)TCGA–COADH2-MIL [15] 58.6 (2.7) 38.5 (5.4) 33.0 (5.0) 99.7 (0.4) 99.2 (0.5) 97.4 (1.7)\\nHEAT (Ours) 63.4 (2.5) 40.0 (2.1) 41.3 (2.7) 99.9 (0.2) 99.9 (0.3) 99.2 (0.4)\\nABMIL [18] 54.7 (4.6) 19.0 (10.0) 23.9 (3.2) 97.3 (1.7) 98.3 (1.1) 97.3 (1.6)', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='DSMIL [21] 51.4 (4.7) 18.3 (14.9) 23.2 (2.3) 98.7 (0.5) 95.6 (1.4) 93.3 (2.0)\\nReMix [38] 58.8 (2.2) 35.6 (16.2) 27.6 (5.8) 96.1 (0.7) 95.8 (2.6) 93.0 (3.4)\\nPatchGCN [5] 50.3 (0.2) 41.6 (0.5) 25.1 (0.3) 96.2 (1.7) 98.2 (0.8) 98.4 (0.8)\\nGTNMIL [41] 53.0 (3.7) 41.3 (4.4) 25.1 (2.3) 94.7 (1.0) 94.5 (0.2) 93.7 (1.7)TCGA–BRCAH2-MIL [15] 52.1 (7.2) 53.7 (2.6) 21.2 (2.5) 97.9 (2.7) 98.0 (1.5) 97.6 (2.2)\\nHEAT (ours) 61.9 (3.8) 55.8 (6.4) 27.7 (16.3) 98.8 (0.7) 98.3 (0.5) 99.5 (0.7)\\nTable 1. Cancer staging and classification results [%] of various methods on TCGA–COAD and TCGA–BRCA datasets.\\nknowledge into the feature aggregation procedure. Specifi-\\ncally, for the HEAT layer, we integrate the prior knowledge\\nof node type and node attributes when extracting node-level\\nfeatures. For PL Pooling, we pool node-level features us-\\ning prior definitions on node clusters. Moreover, we per-\\nform data augmentations (e.g., random pruning on edges', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and nodes) to regularize the learning from training distribu-\\ntions. Besides that, other regularization such as imposing a\\nGaussian prior on the model weights (i.e., using a Bayesian\\nneural network) would also achieve the goal.\\n4.5. Causal-driven Localization\\nWe make use of the Granger causality to outline causal\\nregions in the WSI with the causal graph explainer [23].\\nGiven a trained GNN model M, the causal contribution of\\neach node vis given by\\n∆δ,v=L(y,˜yG)− L(y,˜yG\\\\{v}), (1)\\nwhere yis the true label and ˜yG=M(G)and˜yG\\\\{v}=\\nM(G\\\\{v})are the predicted labels from Mwith input\\ngraphs GandG\\\\{v}, respectively. The causality heatmap\\nof the patches can then be visualized with the causal con-\\ntribution computed for each patch (i.e., node). Addressing\\ncausality in instance interpretation can adjust for observa-\\ntional and selection biases, which would improve the ex-\\nplanation accuracy. Moreover, the causal property of the\\nexplainer could facilitate pathologists to find out potential', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='biomarkers for diagnosis and prognosis by highlighting the\\npatches with clinical relevance in the WSI.\\n5. Experiments\\n5.1. Datasets\\nWe use WSIs from the public TCGA–COAD (cancer\\nstaging task: 1304 cases, classification task: 1434 cases),Model AUC Accuracy Macro-F1\\nABMIL [18] 79.5 (7.5) 80.3 (8.4) 81.3 (7.4)\\nDSMIL [21] 92.5 (1.7) 87.3 (2.0) 86.3 (2.0)\\nReMix [38] 92.5 (7.2) 90.0 (8.1) 90.3 (7.7)\\nPatchGCN [5] 88.6 (3.5) 92.1 (2.3) 92.3 (2.4)\\nGTNMIL [40] 89.7 (4.7) 81.2 (4.8) 89.2 (4.9)\\nH2-MIL [15] 92.1 (3.9) 88.2 (5.8) 88.0 (5.8)\\nHEAT (ours) 92.8 (2.5) 92.7 (2.2) 93.3 (1.9)\\nTable 2. Cancer typing results [%] of our method compared to\\nvarious methods on the TCGA–ESCA dataset.\\nTCGA–BRCA (cancer staging task: 1328 cases, classifica-\\ntion task: 1712 cases), and TCGA–ESCA (typing task: 213\\ncases) from the TCGA project [35] and Camelyon 16 [3]\\nas the benchmark datasets. On average, around 300 patches\\nare sampled from each WSI in the TCGA datasets (around', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='5,000 for Camelyon 16), where each patch corresponds to\\na node in the final heterogeneous graph. For the TCGA–\\nCOAD and the TCGA–BRCA datasets, we conduct two\\ntasks for the benchmark methods — cancer staging and can-\\ncer classification. For the cancer staging task, all the cases\\nare divided into the “Stage I”, “Stage II”, “Stage III”, and\\n“Stage IV” classes. For the cancer classification task, all the\\ncases are divided into the “Normal” and “Tumor” classes.\\nFor the cancer typing task, we use TCGA–ESCA dataset\\nwhere all the cases are divided into two classes i.e., “Type I:\\nadenocarcinoma” and “Type II: squamous cell carcinoma”.\\nWe also evaluate the localization ability of our framework\\non the Camelyon 16 dataset, as this dataset provides the tu-\\nmor mask annotations. A detailed summary of datasets is\\nprovided in supplementary materials.\\n5.2. Implementation Details\\nThe proposed framework is implemented in Python with\\nthePytorch library on a server equipped with four NVIDIA\\n15666', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='15666\\nGNN Architecture AUC Accuracy Macro-F1\\nGCN [36] 90.8 90.9 90.0\\nGAT [32] 85.8 86.4 88.9\\nGIN [37] 91.6 90.9 83.3\\nHetRGCN [28] 82.5 83.3 88.9\\nHGT [16] 87.8 87.5 83.3\\nHEAT (ours) 92.8 92.7 93.2\\nTable 3. Cancer typing results [%] of our method compared to\\nvarious GNN architectures on the TCGA–ESCA dataset.\\nPooling Method AUC Accuracy Macro-F1\\nSum pooling 95.5 99.3 99.2\\nMax pooling 95.1 98.6 99.2\\nMean pooling 97.7 95.8 99.8\\nGlobal attention pooling [22] 94.7 97.9 99.2\\nIH-Pool [15] 99.3 97.2 88.1\\nASAP [25] 99.2 98.6 95.1\\nPL-Pool (ours) 99.6 99.3 99.8\\nTable 4. Cancer classification results [%] on TCGA–COAD of\\nour pooling method to various comparable pooling methods using\\nGCN and KimiaNet feature encoder.\\nTESLA V100 GPUs. We use openslide [11] as the tool to\\nprocess the WSIs. The dropout ratio of each dropout layer\\nis selected as 0.2. All models are trained with 150 epochs\\nwith early stopping. The batch size is selected as 2. We', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='adopt the cross-entropy loss to train the network for classi-\\nfication tasks. We use the Adam optimizer to optimize the\\nmodel with a learning rate of 5×10−5and a weight decay\\nof1×10−5. We perform data augmentations on the train-\\ning graphs by randomly dropping the edges and nodes, and\\nadding Gaussian noises to the node and edge features.\\n5.3. Experiment Settings and Evaluation Metrics\\nWe compare our method with an array of SOTA meth-\\nods, including MIL or graph-based methods. We use five-\\nfold cross-validation to evaluate the overall performance of\\nour framework and other methods. We used the pretrained\\nKimiaNet as the feature extraction for all methods for a fair\\ncomparison. The details of compared methods are listed be-\\nlow.\\n• ABMIL [18]: a MIL framework aggregating bag-level\\ninstance information by the attention mechanism.\\n• DSMIL [21]: a dual-stream multiple instance learning\\nmethod using max pooling and attention to aggregate\\nthe signals from the individual patches.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='• ReMix [38]: a general and efficient MIL’s based\\nframework for WSI analysis that takes the advantage\\nof data augmentation and reduces method to produce\\nrich features.\\n• PatchGCN [5]: a hierarchical graph-based model on\\nsurvival data with patient-level and WSI-level aggre-\\ngations. We adapt this method as a GCN model with\\nglobal attention pooling [22].Balanced dataset AUC Accuracy Macro-F1\\nTCGA–COAD 99.1 (1.8) 99.1 (1.8) 99.2 (1.7)\\nTGCA–BRCA 98.7 (2.5) 98.7 (2.5) 98.7 (2.6)\\nTable 5. Cancer classification results [%] of our method on\\nTCGA–COAD and TCGA–BRCA balanced datasets.\\n• GTNMIL [41]: a graph-based MIL method based on\\nthe graph transformer network [40].\\n• H2-MIL [15]: a tree-graph-based multiple instance\\nlearning method that utilizes different magnification\\nlevels to represent hierarchical features.\\nFor the cancer staging, classification and typing tasks,\\nwe use AUC, classification accuracy, and macro F-1 score\\nas the evaluation metrics. Percentage [%] values are re-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ported for each of the metrics. Standard errors are reported\\nin brackets. For all metrics, a higher value indicates a better\\nperformance. Detailed definitions of the evaluation metrics\\ncan be found in the supplementary materials.\\n5.4. Comparison with Other Methods\\nQuantitative Results. Table 1 shows the cancer staging and\\nclassification results on the TCGA–COAD and the TCGA–\\nBRCA datasets, and Table 2 presents cancer typing results\\non the TCGA–ESCA dataset. Compared to graph-based\\nWSI analysis methods [5, 15, 41], our method demonstrates\\nimproved performance, which indicates our graph model-\\ning method potentially better represents the interaction of\\npatches in a WSI than existing graph-based methods. We\\nalso observe that aggregation on a graph of instances is\\nmore effective than aggregation on bags of instances in the\\nstaging tasks, which implies graph-based methods are more\\ncapable of capturing the global information of WSI for stag-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ing tasks than conventional MIL methods [18, 21, 38]. We\\nfurther compare HEAT on the BRCA subtyping task with a\\nrecent SOTA method on WSI — hierarchical image pyra-\\nmid transformer (HIPT) [4]. Our method achieves an AUC\\nof 89.69 (SD: 3.63), which outperforms the AUC of 87.4\\n(SD: 6.0) by HIPT.\\nAdditionally, we perform a t-test on the AUCs to demon-\\nstrate the statistical significance of our improvements over\\nthe SOTA methods, for which the results are presented in\\nTable 6. We observe that the improvements are statistically\\nsignificant over most of the baseline methods under the 0.05\\nsignificance level.\\nMethods COAD-S COAD-C BRCA-S BRCA-C\\nABMIL 1.91e-16 0 5.75e-5 6.08e-6\\nDSMIL 0.0005 0.0333 3.26e-9 0.3790\\nReMix 1.36e-5 0 0.0759 1.2e-16\\nPatchGCN 0.2899 0 5.14e-11 2.74e-15\\nGTNMIL 2.7e-15 0 5.58e-7 2.94e-35\\nH2-MIL 4.5e-5 0.0343 3.94e-8 0.0082\\nTable 6. P-values of two-sample t-tests on AUCs between our\\nmethod and baselines (S: cancer staging; C: cancer classification).\\n15667', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='15667\\nQualitative Results. We compute the causal contribution\\nof each patch using Equ. (1). We visualize the patch im-\\nage associated with that node to outline the causal regions\\nrelated to the predictions. We also compare our causal ex-\\nplanation method to numerous baseline graph interpretation\\nmethods based on associations [39]. Figures in the supple-\\nmentary materials present the explanation results with dif-\\nferent graph explainers on the Camelyon 16 dataset. It is\\nobserved that using an association-based explainer provides\\na smooth heatmap where many regions are highlighted as\\nimportant. A such heatmap is less accurate in localizing the\\ntumor regions and pathologists still need to traverse a large\\nnumber of abnormal regions suggested by the explainer to\\nidentify tumor regions. On the contrary, we observe that us-\\ning a causal explainer can outline the tumor regions in the\\nWSIs more accurately, with the heatmap more concentrated', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='on the ground-truth tumor regions compared to association-\\nbased explainers (e.g., GNNExplainer [39]).\\n5.5. Analysis of Our Framework\\nEffectiveness of Heterogeneous Graph Construction.\\nWe compare our method with other SOTA GNNs [16, 28,\\n32,36,37] to evaluate the effectiveness of our heterogeneous\\ngraph construction. For heterogeneous graph transformer\\n(HGT) [16] and HetRGCN [28], we define the discrete edge\\ntypes — each relation either has the “positive” type repre-\\nsenting positive correlations between the nodes of the edge,\\nor the “negative” type representing negative correlations.\\nTable 3 presents cancer typing results of our method com-\\npared to various SOTA GNN aggregation methods on the\\nTCGA–ESCA dataset. Not only our method outperforms\\nSOTA homogeneous GNN architectures [32, 36, 37], but it\\nis also superior to some recently heterogeneous GNN ar-\\nchitectures [16, 28]. This implies the advantage of our pro-\\nposed architecture for graph-based WSI analysis.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Analysis of Different Pooling Strategies. We compare\\nour proposed pooling strategy to a variety of comparable\\npooling methods, including basic pooling methods, such\\nas sum/max/mean poolings and advanced pooling strate-\\ngies [15, 25]. Table 4 presents the comparison results of\\ncancer classification on TCGA–COAD dataset. We fix the\\nmodel architecture to be GCN [36] and the feature encoder\\nas KimiaNet [26]. It is observed that our pooling strategy\\noutperforms the competitors, which validates the advantage\\nof using semantic-consistently defined clusters in pooling.\\nPerformance on Different Class Distributions. We ob-\\nserve the WSI datasets for cancer classification is imbal-\\nanced (i.e., approximately ten cancer WSIs to one nor-\\nmal WSI). We thus compose a balanced dataset (i.e., nor-\\nmal:cancer = 1:1) with the undersampling strategy to study\\nhow the difference in class distributions affect the perfor-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='mance of our model. Table 5 presents the comparison. Itis observed that our method achieves similar performance\\nwith the unbalanced setting (See Table 1).\\nGeneralizability. The pretrained features are a key compo-\\nnent of our proposed framework. As the pretrained embed-\\nding models are from a diverse WSI context, they can ex-\\ntract good features from most of the WSI datasets. Because\\nthe PanNuke dataset [9] (used to pretrain the HoverNet node\\ntype classifier) contains WSIs of most of the common can-\\ncer types, this leads to a broad generalization of HoverNet.\\nFurthermore, one may adopt contrastive learning to fine-\\ntune the pretrained models to improve their generalizability\\nto new datasets in potential deployment scenarios.\\nAccuracy of HoverNet. The performance of the Hover-\\nNet classifier would influence the sensitivity of our frame-\\nwork. Since the PanNuke dataset contains WSIs of most of\\nthe common cancer types and cohorts of the TCGA dataset', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='(e.g., COAD), there are domain overlaps between them.\\nHence the HoverNet trained on the PanNuke dataset can be\\ntransferred to the TCGA dataset for patch types classifica-\\ntion with good performance. Furthermore, we perform can-\\ncer classification on COAD using node types generated by\\nunsupervised K-means clustering. The performance (AUC:\\n98.5) is lower than that using HoverNet predicted node\\ntypes (AUC: 99.9). This demonstrates that incorporating\\nthe pretrained HoverNet outperforms unsupervised methods\\nand improves WSI analysis.\\n6. Conclusion\\nWe present a novel heterogeneous graph-based frame-\\nwork for WSI analysis. By modeling WSI as a heteroge-\\nneous graph with various node types and edge attributes,\\nour method not only leverages the locality information, but\\nalso mines the complex relational information of WSI. We\\nfurther design a novel heterogeneous edge attribute trans-\\nformer architecture to aggregate the structural information', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='in the graph and a semantic consistent pooling method\\nto address the potential over-parameterization problems in\\nconventional pooling. We provide a causal explanation\\nmechanism to highlight the causal contributions of the in-\\nstances to improve the clinical usability of our work. Exten-\\nsive experiments on public datasets validate the effective-\\nness of our proposed framework and our framework could\\nbe adapted to other graph-based computer vision tasks, such\\nas 3D point cloud analysis and anomaly detection.\\nAcknowledgement. We thank the anonymous reviewers\\nand the area chair for their insightful comments on our\\nmanuscript. This work was partially supported by the\\nResearch Grants Council of Hong Kong (17308321), the\\nTheme-based Research Scheme (T45-401/22-N), the Na-\\ntional Natural Science Fund (62201483), and the HKU-\\nTCL Joint Research Center for Artificial Intelligence spon-\\nsored by TCL Corporate Research (Hong Kong).\\n15668\\nReferences', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='15668\\nReferences\\n[1] David Ahmedt-Aristizabal, Mohammad Ali Armin, Simon\\nDenman, Clinton Fookes, and Lars Petersson. A survey on\\ngraph-based deep learning for computational histopathology.\\nComputerized Medical Imaging and Graphics , page 102027,\\n2021. 2\\n[2] Yogesh Balaji, Mohammadmahdi Sajedi, Neha Mukund\\nKalibhat, Mucong Ding, Dominik St ¨oger, Mahdi\\nSoltanolkotabi, and Soheil Feizi. Understanding over-\\nparameterization in generative adversarial networks. In\\nInternational Conference on Learning Representations ,\\n2020. 2\\n[3] Babak Ehteshami Bejnordi, Mitko Veta, Paul Johannes\\nVan Diest, Bram Van Ginneken, Nico Karssemeijer, Geert\\nLitjens, Jeroen AWM Van Der Laak, Meyke Hermsen,\\nQuirine F Manson, Maschenka Balkenhol, et al. Diagnos-\\ntic assessment of deep learning algorithms for detection of\\nlymph node metastases in women with breast cancer. Jama ,\\n318(22):2199–2210, 2017. 2, 6\\n[4] Richard J Chen, Chengkuan Chen, Yicong Li, Tiffany Y\\nChen, Andrew D Trister, Rahul G Krishnan, and Faisal', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Mahmood. Scaling vision transformers to gigapixel images\\nvia hierarchical self-supervised learning. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 16144–16155, 2022. 7\\n[5] Richard J Chen, Ming Y Lu, Muhammad Shaban,\\nChengkuan Chen, Tiffany Y Chen, Drew FK Williamson,\\nand Faisal Mahmood. Whole slide images are 2d point\\nclouds: Context-aware survival prediction using patch-based\\ngraph convolutional networks. In International Conference\\non Medical Image Computing and Computer-Assisted Inter-\\nvention , pages 339–349. Springer, 2021. 2, 4, 6, 7\\n[6] Moshe Eliasof and Eran Treister. Diffgcn: Graph con-\\nvolutional networks via differential operators and algebraic\\nmultigrid pooling. Advances in neural information process-\\ning systems , 33:18016–18027, 2020. 3\\n[7] Alton B Farris, Juan Vizcarra, Mohamed Amgad, Lee AD\\nCooper, David Gutman, and Julien Hogan. Artificial in-\\ntelligence and algorithmic computational pathology: an in-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='troduction with renal allograft examples. Histopathology ,\\n78(6):791–804, 2021. 2\\n[8] Jevgenij Gamper, Navid Alemi Koohbanani, Ksenija Benet,\\nAli Khuram, and Nasir Rajpoot. Pannuke: an open pan-\\ncancer histology dataset for nuclei instance segmentation and\\nclassification. In European congress on digital pathology ,\\npages 11–19. Springer, 2019. 4\\n[9] Jevgenij Gamper, Navid Alemi Koohbanani, Ksenija Benes,\\nSimon Graham, Mostafa Jahanifar, Syed Ali Khurram,\\nAyesha Azam, Katherine Hewitt, and Nasir Rajpoot. Pan-\\nnuke dataset extension, insights and baselines. arXiv preprint\\narXiv:2003.10778 , 2020. 8\\n[10] Liyu Gong and Qiang Cheng. Exploiting edge features for\\ngraph neural networks. In Proceedings of the IEEE/CVF\\nconference on computer vision and pattern recognition ,\\npages 9211–9219, 2019. 2\\n[11] Adam Goode, Benjamin Gilbert, Jan Harkes, Drazen Jukic,\\nand Mahadev Satyanarayanan. Openslide: A vendor-neutralsoftware foundation for digital pathology. Journal of pathol-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ogy informatics , 4, 2013. 7\\n[12] Simon Graham, Quoc Dang Vu, Shan E Ahmed Raza,\\nAyesha Azam, Yee Wah Tsang, Jin Tae Kwak, and Nasir\\nRajpoot. Hover-net: Simultaneous segmentation and classi-\\nfication of nuclei in multi-tissue histology images. Medical\\nImage Analysis , 58:101563, 2019. 2, 4, 5\\n[13] Clive WJ Granger. Investigating causal relations by econo-\\nmetric models and cross-spectral methods. Econometrica:\\njournal of the Econometric Society , pages 424–438, 1969. 2,\\n3\\n[14] Yuqi Gu and David B Dunson. Bayesian pyramids: Identi-\\nfiable multilayer discrete latent structure models for discrete\\ndata. Journal of the Royal Statistical Society Series B: Sta-\\ntistical Methodology , 2021. 2\\n[15] Wentai Hou, Lequan Yu, Chengxuan Lin, Helong Huang,\\nRongshan Yu, Jing Qin, and Liansheng Wang. H2-mil: Ex-\\nploring hierarchical representation with heterogeneous mul-\\ntiple instance learning for whole slide image analysis. 2022.\\n2, 3, 6, 7, 8\\n[16] Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Heterogeneous graph transformer. In Proceedings of The\\nWeb Conference 2020 , pages 2704–2710, 2020. 2, 4, 7, 8\\n[17] Tiancheng Huang, Ke Xu, and Donglin Wang. Da-hgt:\\nDomain adaptive heterogeneous graph transformer. arXiv\\npreprint arXiv:2012.05688 , 2020. 4\\n[18] Maximilian Ilse, Jakub Tomczak, and Max Welling.\\nAttention-based deep multiple instance learning. In Inter-\\nnational conference on machine learning , pages 2127–2136.\\nPMLR, 2018. 6, 7\\n[19] Ryosuke Kojima, Shoichi Ishida, Masateru Ohta, Hiroaki\\nIwata, Teruki Honma, and Yasushi Okuno. kgcn: a graph-\\nbased deep learning framework for chemical structures.\\nJournal of Cheminformatics , 12:1–10, 2020. 2\\n[20] Adam Lerer, Ledell Wu, Jiajun Shen, Timothee Lacroix,\\nLuca Wehrstedt, Abhijit Bose, and Alex Peysakhovich.\\nPytorch-biggraph: A large-scale graph embedding system.\\nProceedings of the 2nd SysML Conference , 2019. 2\\n[21] Bin Li, Yin Li, and Kevin W Eliceiri. Dual-stream multiple', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='instance learning network for whole slide image classifica-\\ntion with self-supervised contrastive learning. In Proceed-\\nings of the IEEE/CVF conference on computer vision and\\npattern recognition , pages 14318–14328, 2021. 6, 7\\n[22] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard\\nZemel. Gated graph sequence neural networks. Proceedings\\nof ICLR’16 , 2015. 7\\n[23] Wanyu Lin, Hao Lan, and Baochun Li. Generative causal\\nexplanations for graph neural networks. In International\\nConference on Machine Learning , pages 6666–6679. PMLR,\\n2021. 3, 6\\n[24] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu,\\nBo Zong, Haifeng Chen, and Xiang Zhang. Parameterized\\nexplainer for graph neural network. Advances in neural in-\\nformation processing systems , 33:19620–19631, 2020. 3\\n[25] Ekagra Ranjan, Soumya Sanyal, and Partha Talukdar. Asap:\\nAdaptive structure aware pooling for learning hierarchical\\n15669\\ngraph representations. In Proceedings of the AAAI Confer-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ence on Artificial Intelligence , volume 34, pages 5470–5477,\\n2020. 3, 7, 8\\n[26] Abtin Riasatian. Kimianet: Training a deep network for\\nhistopathology using high-cellularity. Master’s thesis, Uni-\\nversity of Waterloo, 2020. 2, 4, 8\\n[27] Joel Saltz, Rajarsi Gupta, Le Hou, Tahsin Kurc, Pankaj\\nSingh, Vu Nguyen, Dimitris Samaras, Kenneth R Shroyer,\\nTianhao Zhao, Rebecca Batiste, et al. Spatial organiza-\\ntion and molecular correlation of tumor-infiltrating lympho-\\ncytes using deep learning on pathology images. Cell reports ,\\n23(1):181–193, 2018. 2\\n[28] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne\\nvan den Berg, Ivan Titov, and Max Welling. Modeling rela-\\ntional data with graph convolutional networks. In European\\nsemantic web conference , pages 593–607. Springer, 2018. 2,\\n7, 8\\n[29] Patrick Schwab and Walter Karlen. Cxplain: Causal expla-\\nnations for model interpretation under uncertainty. Advances\\nin Neural Information Processing Systems , 32, 2019. 3', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[30] David Tellez, Geert Litjens, Jeroen van der Laak, and\\nFrancesco Ciompi. Neural image compression for gigapixel\\nhistopathology image analysis. IEEE transactions on pattern\\nanalysis and machine intelligence , 43(2):567–578, 2019. 2\\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. Advances in neural\\ninformation processing systems , 30, 2017. 4\\n[32] Petar Veli ˇckovi ´c, Guillem Cucurull, Arantxa Casanova,\\nAdriana Romero, Pietro Lio, and Yoshua Bengio. Graph\\nattention networks. International Conference on Learning\\nRepresentations , 2017. 2, 7, 8\\n[33] Shujun Wang, Yaxi Zhu, Lequan Yu, Hao Chen, Huangjing\\nLin, Xiangbo Wan, Xinjuan Fan, and Pheng-Ann Heng.\\nRmdl: Recalibrated multi-instance deep learning for whole\\nslide gastric image classification. Medical image analysis ,\\n58:101549, 2019. 2\\n[34] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Peng Cui, and Philip S Yu. Heterogeneous graph attention\\nnetwork. In The World Wide Web Conference , pages 2022–\\n2032, 2019. 2\\n[35] John N Weinstein, Eric A Collisson, Gordon B Mills,\\nKenna R Shaw, Brad A Ozenberger, Kyle Ellrott, Ilya\\nShmulevich, Chris Sander, and Joshua M Stuart. The cancer\\ngenome atlas pan-cancer analysis project. Nature genetics ,\\n45(10):1113–1120, 2013. 2, 6\\n[36] Max Welling and Thomas N Kipf. Semi-supervised clas-\\nsification with graph convolutional networks. In J. In-\\nternational Conference on Learning Representations (ICLR\\n2017) , 2016. 2, 7, 8\\n[37] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.\\nHow powerful are graph neural networks? In International\\nConference on Learning Representations , 2018. 2, 7, 8\\n[38] Jiawei Yang, Hanbo Chen, Yu Zhao, Fan Yang, Yao Zhang,\\nLei He, and Jianhua Yao. Remix: A general and efficient\\nframework for multiple instance learning based whole slide', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='image classification. Medical Image Computing and Com-puter Assisted Intervention–MICCAI 2022: 25th Interna-\\ntional Conference, Singapore, September 18–22, 2022, Pro-\\nceedings, Part II , 2022. 6, 7\\n[39] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik,\\nand Jure Leskovec. Gnnexplainer: Generating explanations\\nfor graph neural networks. Advances in neural information\\nprocessing systems , 32, 2019. 3, 8\\n[40] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang,\\nand Hyunwoo J Kim. Graph transformer networks. Advances\\nin neural information processing systems , 32, 2019. 2, 4, 6,\\n7\\n[41] Yi Zheng, Rushin H Gindra, Emily J Green, Eric J Burks,\\nMargrit Betke, Jennifer E Beane, and Vijaya B Kolacha-\\nlama. A graph-transformer for whole slide image classifica-\\ntion. IEEE transactions on medical imaging , 41(11):3003–\\n3015, 2022. 2, 6, 7\\n[42] Chenyi Zhuang and Qiang Ma. Dual graph convolutional\\nnetworks for graph-based semi-supervised classification. In', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Proceedings of the 2018 World Wide Web Conference , pages\\n499–508, 2018. 2\\n15670', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Two-view Geometry Scoring Without Correspondences\\nAxel Barroso-Laguna1Eric Brachmann1Victor Adrian Prisacariu1,2\\nGabriel Brostow1,3Daniyar Turmukhambetov1\\n1Niantic2University of Oxford3University College London\\nwww.github.com/nianticlabs/scoring-without-correspondences\\nAbstract\\nCamera pose estimation for two-view geometry tradi-\\ntionally relies on RANSAC. Normally, a multitude of image\\ncorrespondences leads to a pool of proposed hypotheses,\\nwhich are then scored to find a winning model. The inlier\\ncount is generally regarded as a reliable indicator of “con-\\nsensus”. We examine this scoring heuristic, and find that it\\nfavors disappointing models under certain circumstances.\\nAs a remedy, we propose the Fundamental Scoring Net-\\nwork (FSNet), which infers a score for a pair of overlap-\\nping images and any proposed fundamental matrix. It does\\nnot rely on sparse correspondences, but rather embodies\\na two-view geometry model through an epipolar attention', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='mechanism that predicts the pose error of the two images.\\nFSNet can be incorporated into traditional RANSAC loops.\\nWe evaluate FSNet on fundamental and essential matrix es-\\ntimation on indoor and outdoor datasets, and establish that\\nFSNet can successfully identify good poses for pairs of im-\\nages with few or unreliable correspondences. Besides, we\\nshow that naively combining FSNet with MAGSAC++ scor-\\ning approach achieves state of the art results.\\n1. Introduction\\nHow to determine the relative camera pose between two\\nimages is one of the cornerstone challenges in computer vi-\\nsion. Accurate camera poses underpin numerous pipelines\\nsuch as Structure-from-Motion, odometry, SLAM, and vi-\\nsual relocalization, among others [26, 37, 47, 48, 56]. Much\\nof the time, an accurate fundamental matrix can be es-\\ntimated by existing means, but the failures are prevalent\\nenough to hurt real-world tasks, and are hard to antici-\\npate [23]. Where are the mistakes coming from?', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Traditional approaches first detect then describe a set\\nof interest points in each image, and establish correspon-\\ndences between the two sets while possibly filtering them,\\ne.g., checking for mutual nearest neighbors or applying\\nLowe’s ratio test [36]. Then, random subsets of correspon-\\ndences are sampled and a 5-point or 7-point algorithm is\\nReference frame \\nDestination frame \\nF-Mat Hypothesis \\nMAGSAC++ \\nFSNet \\nFigure 1. Example where SuperPoint-SuperGlue [19, 46] corre-\\nspondences are highly populated by outliers, but there are still\\nenough inliers to produce a valid fundamental matrix hypothe-\\nsis. In such scenarios with unreliable correspondences, current\\ntop scoring methods fail (MAGSAC++ [5]), while our proposed\\nFSNet model, a correspondence-free scoring approach, is able to\\npick out the best fundamental matrix.\\nused to estimate many essential or fundamental matrix hy-\\npotheses, respectively, ( i.e., two-view geometry models). A', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='RANSAC [25] loop iterates over the generated hypotheses,\\nand ranks them. Conventionally, the ranking is scored by\\ncounting inliers, i.e. the number of correspondences within\\na threshold of that two-view geometry hypothesis. Finally,\\nthe top-ranked hypothesis is further refined by using all in-\\nlier correspondences.\\nAs the research in robust model estimation advances [2,\\n5, 13, 15, 41, 58], the different stages of the pipeline are be-\\ning revisited, e.g., local feature detection and description is\\nlearned with neural networks, outlier correspondences are\\nfiltered with learned models, hypotheses are sampled more\\nefficiently, or the inlier threshold is optimized. Although the\\nlatest matching pipelines produce very accurate and robust\\ncorrespondences [19, 46, 50], correspondence-based scor-\\ning methods are still sensitive to the ratio of inliers, num-\\nber of correspondences, or the accuracy of the keypoints', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n8979\\n[2, 5, 10, 23]. Incorrect two-view geometry estimation can\\nlead to invalid merges in 3D reconstruction models [48],\\nbad localization services [1], or more expensive steps when\\nfinding outliers in pose graphs [14].\\nA second family of approaches emerged in recent years,\\nwhere a neural network is trained to directly regress two-\\nview geometry from the input images [1, 40, 42, 68].\\nThus, such approaches replace all the components of the\\nRANSAC pipeline. This can be a viable approach when\\ntwo views are extremely difficult, even when they do not\\noverlap [11]. However, challenging scenarios, e.g., wide-\\nbaseline, or large illumination changes, can lead to incor-\\nrect predictions [32]. Typically, poses directly regressed', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='this way have fewer catastrophic relative pose predictions,\\nbut they have difficulty in estimating precise geometry [1].\\nOn the other hand, correspondence-based hypotheses can\\nbe very precise, if estimated correspondences are of suffi-\\nciently high quality. Our approach uses correspondences\\nto generate model hypotheses, but does not use correspon-\\ndences to score them during the RANSAC loop.\\nWe propose a fundamental matrix scoring network that\\nleverages epipolar geometry to compare features of the im-\\nages in a dense manner. We refer to our method as the Fun-\\ndamental Scoring Network, or FSNet for short. Inspired\\nby the success of Vision Transformers [61], and detector-\\nfree matchers [31, 50], we define an architecture that incor-\\nporates the epipolar geometry into an attention layer, and\\nhence the quality of the fundamental matrix hypothesis con-\\nditions the coherence of the computed features. Figure 1\\nshows an example where correspondences are highly popu-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='lated by outliers. However, there are still enough inliers to\\ngenerate a good fundamental matrix, and FSNet was able to\\nselect it from the hypothesis pool.\\nOur contributions are 1) an analysis of the causes of scor-\\ning failures, as well as more insights into the traditional\\nRANSAC approach of relative pose estimation; 2) FSNet, a\\nnetwork that predicts angular translation and rotation errors\\nfor a given image pair and a fundamental matrix hypothesis;\\n3) an image order-invariant design of FSNet that outputs the\\nsame values for (Image A, Image B, F) and (Image B, Im-\\nage A, FT) inputs; 4) a solution that can be combined with\\nstate-of-the-art methods to cope with current failure cases.\\n2. Related Work\\nEstablishing correspondences. Previous correspondence\\nestimation built around SIFT [36] has largely been su-\\nperseded by learned methods. Keypoint detectors [6, 62],\\npatch-based descriptors [53, 54], joint detector-descriptors\\n[8, 19, 22, 43], or shape estimators [7, 38, 65] are some of', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the steps that have benefited from data-driven techniques.\\nTo find correspondences, the mutual nearest neighbors and\\nLowe’s ratio test [36] approach has also been revisited, and\\nlearned matchers have pushed forward the matching capa-\\nFigure 2. Failure cases analysis . We plot for ScanNet and\\nMegaDepth datasets the percentage of image pairs for which\\na good fundamental matrix (fundamental matrix with pose er-\\nror below 10°) was not selected due to scoring failure, degen-\\nerate case, or pre-scoring failure, i.e., there was not a valid\\nfundamental matrix in the hypothesis pool. We see that the fail-\\nures center on the scoring function, it is especially difficult to score\\nhypotheses in the indoor scenario (ScanNet), where local features\\nsuffer more [50].\\nbility of feature extractors [46]. Complementary to match-\\ners, additional filtering methods learn to detect and reject\\noutlier correspondences [12,51,64,66,67]. Once correspon-\\ndences are established, the RANSAC loop finds the best', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='hypothesis among the pool. Multiple works aim at sam-\\npling an all-inlier correspondence minimal-set sooner than\\nRANSAC [10, 15, 55, 57, 58]. Combining that with early\\ntermination techniques and detection of degenerate config-\\nurations [13, 17, 30] can significantly improve the results\\nand run-time. More recently, alternative methods have been\\nproposed to improve upon the classical detect-then-describe\\napproach, e.g., detector-free matchers [31, 50, 60], or direct\\nrelative-pose regressor networks [1, 11, 42, 44, 68].\\nModel quality. Model quality research focuses mainly\\non improving the heuristics for classical inlier counting\\n[49, 58, 59]. LO-RANSAC [16] applies a local optimiza-\\ntion step to promising models generated from the RANSAC\\nsampling. GC-RANSAC [3] extended previous local opti-\\nmization and uses graph techniques to infer spatial struc-\\ntures and mask out outliers. MAGSAC++ [4, 5] proposes\\nan iterative inlier counting score over a range of inlier', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='thresholds, which reduces the sensitivity to the inlier-outlier\\nthreshold parameter. MQ-Net [2] combines the inlier count-\\ning score with a neural network that predicts the quality of\\na hypothesis from correspondence residuals.\\nTransformers in vision. Since its introduction, the trans-\\nformer architecture [61] has become the standard in natural\\nlanguage processing due to its performance and simple de-\\nsign. Transformers are getting attention in the vision com-\\nmunity, and have been applied successfully to image match-\\ning [31, 50], multi-view stereo [20, 29, 63], or depth esti-\\nmation [27, 34], among others [21, 44]. Moreover, differ-\\nent works have been proposed to guide the cross-attention\\nmechanism with epipolar supervision [20,27,29,63], where\\nthe most popular strategies use the epipolar attention to limit\\n8980\\n0 20 40 60 80 100 120 140 160 +180\\nNumber of correspondences0.00.20.40.60.8MAA at 10°MAA vs Number of Correspondences\\nMegaDepth - t\\nMegaDepth - R\\nScanNet - t', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ScanNet - t\\nScanNet - RFigure 3. MAA@10° vs number of correspondences . We show\\nthat the number of correspondences has a high impact on the pre-\\ncision of the fundamental matrix. The latest matchers already filter\\nout the correspondence outliers [46], and naturally, the number of\\ncorrespondences correlates with the difficulty of correctly match-\\ning two images. Thus, we see that the number of correspondences\\nis a good indicator to decide when to trust the correspondences.\\nthe matching search space [29], or to check the consistency\\nof a depth map [27].\\n3. Analysis\\nIn this section we explore shortcomings of the tradi-\\ntional RANSAC-based pose estimation approach. First,\\nwe show that correspondence-based scoring of fundamen-\\ntal matrix hypotheses is the major reason for RANSAC\\nreturning incorrect relative poses. Next, we show that\\nthe low number of correspondences often leads to scor-\\ning function failures motivating correspondence-free ap-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='proach. To predict fundamental matrix quality without\\ncorrespondence-based heuristics, we need a good training\\nsignal. We consider alternatives to correspondence-based\\nscoring functions that can be used to quantify the quality of\\nfundamental matrix hypotheses. For our analyses, we use\\nSuperPoint-SuperGlue (SP-SG) [19, 46] correspondences\\nwith MAGSAC++ [5], a top-performing publicly available\\nfeature extraction, feature matching and robust estimation\\nmodel. We opt for a top performing combination instead\\nof a classical baseline, i.e., SIFT with RANSAC, to anal-\\nyse a more realistic use case. We then mine image pairs\\nwith low overlapping views from validation splits of Scan-\\nNet [18] and MegaDepth [35] datasets as in [46, 50] and\\nstudy MAGSAC++’s behavior for 500 iterations, i.e., 500\\nfundamental matrix hypotheses were generated and scored.\\nSee Section 4.6 for more details on the validation set gener-\\nation.\\nWhere do wrong solutions come from? In Figure 2,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='we show the number of times a good fundamental ma-\\ntrix (fundamental matrix with pose error <10°) was se-\\nlected. We also count failures: (i) pre-scoring, i.e., no\\ngood fundamental matrix is among hypotheses, (ii) de-\\ngeneracy cases, e.g., inlier correspondences can be ex-\\nplained with a homography, (iii) scoring failures, where bad\\nfundamental matrix was chosen by MAGSAC++ heuristicmAA at 10°\\nScanNet MegaDepth\\nR t R t\\nGT Pose error 0.75 0.72 0.94 0.89\\nSED 0.57 0.48 0.86 0.78\\nRE1 0.74 0.41 0.91 0.78\\nEpi. Distance 0.57 0.40 0.89 0.76\\nMAGSAC++ 0.51 0.22 0.80 0.55\\nTable 1. Error criteria evaluation. Fundamental matrices are\\ngenerated with SP-SG [19, 46] and MAGSAC++ [5], and evalu-\\nated under densely projected correspondences with ground-truth\\ncamera poses and depth maps on our validation set. GT Pose error\\nuses directly the error associated to the fundamental matrix to rank\\nthem, being the upper-bound of the scoring function.\\nwhile a good fundamental matrix was among the 500 hy-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='potheses but had lower score. As seen in the figure, 42%\\n(ScanNet) and 23% (MegaDepth) of the image pairs gener-\\nate a valid fundamental matrix but the scoring method is not\\nable to select it.\\nWhy are wrong solutions selected? Inlier heuristics find\\nfewer inliers for a good fundamental matrix than a bad\\nfundamental matrix. So, what leads to a low number of\\ninliers for a good fundamental matrix? Intuitively, feature\\nmatching methods return fewer correspondences when the\\nimage pair is difficult to match, due to repetitive patterns,\\nlack of textures, or small visual overlap, among the possi-\\nble reasons. In such scenarios, correspondences can also\\nbe highly contaminated by outliers, making it even harder\\nto select the correct fundamental matrix. Figure 3 shows\\nthe MAA@10° as in [32] w.r.t the number of correspon-\\ndences generated by SP-SG and MAGSAC++. We see the\\nstrong correlation between the number of correspondences\\nand the accuracy of the selected model. Furthermore, the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='SuperGlue correspondences in this experiment are already\\nof high-quality, as bad correspondences were filtered out.\\nIn supplementary materials, we show that loosening the\\ncorrespondence filtering criteria, and hence, increasing the\\nnumber of correspondences, does not lead to improved re-\\nsults. This observation motivates our correspondence-free\\nfundamental matrix hypothesis scoring approach, which\\nleads to improvements in relative pose estimation task when\\ncorrespondences are not reliable.\\nHow should we select good solutions? If we want to train a\\nmodel that predicts fundamental matrix quality what should\\nthis model predict? As we are interested in ranking hy-\\npotheses, or potentially discard all of them if they are all\\nwrong, the predicted value should correlate with the error\\nin the relative pose or the mismatch of epipolar constraint .\\nThe quality of relative pose is measured for rotation and\\ntranslation separately. The angle of rotation between the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='estimated and the ground truth rotation matrices provides\\nthe rotation error. As fundamental and essential matrices\\n8981\\nTransformer \\nxPose Error Regressor \\n…Hypothesis pool Epipolar line \\nEpipolar candidates Epipolar Cross-attention \\nQuery point \\nResNet \\nblocks 2D avg \\nPooling \\nResNet \\nblocks 2D avg \\nPooling \\nMax + MLP Self and Cross-attention \\nFigure 4. FSNet architecture has four components. 1. A CNN feature extractor computes the feature maps fAandfBfrom input images\\nAandBat 1/4 of input resolution (Section 4.1). 2. The extracted features are then processed by the transformer block, which contains\\nNtself and cross-attention layers. The transformer outputs†fAand†fB, which are stored and reused for every Fihypothesis (Section\\n4.2). 3. The epipolar cross-attention layer applies cross-attention along the epipolar lines, and embeds Fiinto the feature maps fA\\niand\\nfB', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='iand\\nfB\\ni. Epipolar attention is done every two positions, reducing the final feature maps to an 1/8 of the input resolution (Section 4.3). 4. The\\npose regressor applies a ResNet and a 2D average pooling block, and outputs vA→B\\ni andvB→A\\ni . The vectors are combined through a max\\npooling operator, and the final MLP layer predicts the eR\\niandet\\nierrors associated to Fi(Section 4.4).\\nare scale-invariant, the scale-invariant metric of translation\\nerror is the angular distance between the estimated and the\\nground truth translation vectors. Thus, relative pose error\\nrequires estimation of two values.\\nThere are multiple error criteria [24] for fitting\\nfundamental matrices. Reprojection error (RE) measures\\nthe minimum distance correction needed to align the corre-\\nspondences with the epipolar geometry. Symmetric epipo-\\nlar (SED) measures geometric distance of each point to its\\nepipolar line. Finally, Sampson (RE1) distance is a first-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='order approximation of RE. The benefit of these metrics is\\nthat the quality of fundamental matrices is measured with\\none scalar value.\\nSo, how do we choose the best error criteria as train-\\ning signal for FSNet? Should it be relative pose error,\\nor one of the fundamental matrix error criteria? Let us\\nassume that we have access to multiple oracle models,\\none oracle for each of different error criteria in Table 1.\\nSo, one oracle model that predicts the relative pose er-\\nror perfectly, one oracle model that predicts SED error\\nperfectly, etc. Which of these oracle models provides\\nthe best fundamental matrix scoring approach? To simu-\\nlate evaluation of these oracle models, we use SP-SG and\\nMAGSAC++ to mine fundamental matrix hypotheses from\\nvalidation datasets, and use the ground truth depth maps and\\ncamera poses to generate dense correspondences between\\nimage pairs to exactly compute all error criteria. In Ta-\\nble 1, we show the MAA@10° of different scoring criteria', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='when evaluating mined fundamental matrices using oracle\\nmodels in ScanNet and MegaDepth image pairs. As canbe seen in the table, all the fundamental matrix error crite-\\nria under-perform compared to relative pose error metrics,\\nhence FSNet should be supervised by the relative rotation\\nand translation errors.\\n4. Method\\nFSNet estimates the quality of a fundamental matrix hy-\\npothesis, Fi, for the two input images, AandB, without\\nrelying on correspondences and processing the images di-\\nrectly. Besides fundamental matrix quality estimation, in\\na calibrated setup, FSNet could compute the score of the\\nessential matrix, Ei, by first obtaining Fibased on their\\nrelationship: Ei=KT\\nBFiKA. Figure 4 shows the four ma-\\njor components of FSNet architecture: the feature extractor,\\nthe transformer, the epipolar cross-attention, and the pose\\nerror regressor. The supplementary materials provide more\\ndetailed description of our network.\\n4.1. Feature Extractor', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='The feature extractor is a standard convolutional archi-\\ntecture as in [50], it follows the Unet-style network [45] de-\\nsign with skip and residual connections [28] and computes\\nfeature maps at 1/4of the input resolution. Before feature\\nextraction, we center-crop and resize the input images A\\nandBto a resolution of (H, W )(in our experiments we use\\n(256,256) ). Images are then processed by the convolutional\\nfeature extractor to produce the C-dimensional feature maps\\nfAandfB.\\n8982\\n4.2. Transformer\\nWe use an Lmulti-head attention transformer architec-\\nture following [50], and alternate between self and cross-\\nattention blocks to exploit the similarities within and across\\nthe feature maps. We denote the transformed features as\\n†fAand†fB. Following the transformer nomenclature,\\nsome features are used to compute the query (Q), and po-\\ntentially different features are used to compute the key (K)\\nand the value (V). Q retrieves information from V based on', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the attention weight computed from the product of Q and\\nK. In the self-attention layer, the same feature map builds\\nQ, K, and V , meanwhile, in the cross-attention layer, Q is\\ncomputed from a different feature map than K and V . We\\ninterleave the self and cross-attention block Nttimes.\\nSelf-attention and Cross-attention layers. To limit the\\ncomputational complexity of our transformer block, we use\\na Linear Transformer [33] as in [50]. Linear Transformer\\nreduces the computational complexity of the original Trans-\\nformer from O(N2)toO(N)by making use of the associa-\\ntivity property of matrix products and replacing the expo-\\nnential similarity kernel with a linear dot-product kernel.\\nSpecifically, in a self-attention layer, the input feature map\\nf′is used to compute Q, K and V . We concatenate the re-\\nsult of the attention layer with the input f′feature map and\\npass it through a two-layer MLP. The output of the MLP is\\nthen added to f′and passed to the next block. In the cross-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='attention layer, we repeat the previous process but compute\\nQ from one feature map and K and V from the second.\\nWe found that positional encoding for attention layers\\ndid not improve our results, similarly to findings in [34],\\nand hence, we do not use positional encodings in FSNet.\\n4.3. Epipolar Cross-attention\\nUp to this point, attended feature maps,†fAand†fB,\\nare cached and reused. Given that FSNet computes the\\nscore for every Fi, this design assures a more practi-\\ncal scenario where the overhead of computing additional\\nfundamental matrix scores is small.\\nFor every fundamental matrix hypothesis Fi, our epipo-\\nlar cross-attention mechanism embeds Fitogether with fea-\\nture maps†fAand†fB. Every position pA= [u, v]in\\nfeature map†fAhas a corresponding epipolar line in†fB\\ndefined as lA→B\\nuv =F′\\ni¯pA, where ¯pArefers to the homoge-\\nneous coordinates of pAandF′\\niis a scaled Fiby a factor\\nof1/4. As we consider potentially hundreds of hypotheses,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the resolution of feature maps impacts run-time speed. So,\\nwe opt to define query points, pA= [u, v], with a step sam-\\npling of two. This reduces even further the final feature map\\nto a resolution of 1/8of the input image.\\nSo, for every feature†fA\\nuv∈†fAwe sample†fBatD\\nequidistant locations along the epipolar line lA→B\\nuv . We\\nstart sampling where the epipolar line meets the feature map\\n(from left to right) and use bilinear interpolation to pro-duceDfeatures†fB\\nluv. If sampling positions fall outside the\\nimage plane, or the epipolar line never crosses the image,\\nwe zero pad the features. Thus, we build feature volume\\n†fB\\ni∈[C, D, W/ 8, H/8]from feature map†fBandFi.\\nWe use†fAto compute Q, and†fB\\nivolume to obtain the K\\nand V , and perform attention along the epipolar candidate\\npoints. Finally, we use Q, K, and V to obtain epipolar trans-\\nformed features fA\\ni. For order-invariance, we also compute\\nfB\\niby repeating these operations for (†fB,†fA)pair of fea-\\nture maps and FT\\ni.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ture maps and FT\\ni.\\n4.4. Pose Error Regressor\\nAs seen in Figure 4, the pose error regressor uses a\\nResNet block to extracts features from fA\\niandfB\\ni. Follow-\\ning [1,11], we apply a 2D average pooling that results in two\\n1D vectors, vA→B\\ni andvB→A\\ni , with size C′. Both 1D vec-\\ntors are then merged by a max pooling operator, such that\\ndifferent order of the input images always produce the same\\nfeature vector vi. An MLP layer then regresses the angular\\ntranslation and rotation errors, et\\niandeR\\ni, associated to Fi.\\n4.5. Loss Function\\nContrary to previous binary [2] or multi-class [11] for-\\nmulation of the pose error, we experimentally found that\\nFSNet is more accurate when regressing independently the\\ntranslation and rotation errors (see Section 5.3). The su-\\npervision we use is angular errors, hence, the predicted\\nerror, ei, and the ground truth error, ¯ei, are bounded be-\\ntween [0◦,180◦]. Directly using a simple L1 loss ( l=\\n|¯ei−ei|) would treat all error ranges equally. However,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='we are interested in accurate error estimation of all good\\nfundamental matrices, while not requiring being as pre-\\ncise when the fundamental matrices have high pose errors.\\nHence, we propose to use a soft clamping of the ground\\ntruth error as well as the network prediction, such as:\\nL=|g(¯et\\ni)−g(et\\ni)|+|g(¯eR\\ni)−g(eR\\ni)|, (1)\\nwhere g(x)refers to the tanh( x/ts)function, and tsis the\\nscaling factor that adjusts the (soft) threshold after which\\nthe accuracy of the angular errors is not important.\\n4.6. Implementation Details\\nWe train indoor and outdoor FSNet models on ScanNet\\n[18] and MegaDepth [35] datasets. To generate training and\\nvalidation sets, we first extract SP-SG [19, 46] correspon-\\ndences using appropriate indoor and outdoor pre-trained\\nmodels. We then draw minimal subsets of correspondences\\nrandomly and extract 500 two-view hypotheses for every\\nimage pair. For each hypothesis, we compute the angular\\ntranslation ( et) and rotation ( eR) errors using the ground', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='truth extrinsic and intrinsic parameters. During training, we\\nensure that the ground truth hypothesis is among the 500\\n8983\\n0-100 100-Inf All\\nMAA@10° ↑ Median (°) ↓ MAA@10° ↑ Median (°) ↓ MAA@10° ↑ Median (°) ↓\\nR / t / max(R, t) eR/et R / t / max(R, t) eR/et R / t / max(R, t) eR/et\\nFundamental\\nRANSAC [25] 0.15 / 0.07 / 0.04 17.97 / 30.28 0.24 / 0.06 / 0.05 11.91 / 37.03 0.17 / 0.07 / 0.04 15.54 / 32.18\\nMAGSAC++ [5] 0.28 / 0.12 / 0.08 9.19 / 24.38 0.63 / 0.29 / 0.27 2.80 / 8.66 0.38 / 0.17 / 0.14 6.35 / 17.89\\nFSNet (F) 0.29 /0.19 / 0.12 7.98 /13.30 0.52 / 0.25 / 0.21 4.16 / 9.62 0.36 / 0.21 / 0.15 6.52 / 12.05\\nFSNet (F + E) 0.29 / 0.18 / 0.11 8.01 / 14.05 0.50 / 0.24 / 0.20 4.35 / 10.26 0.35 / 0.20 / 0.14 6.64 / 12.78\\nw/ Corresp. filter 0.29 /0.19 / 0.12 7.98 /13.30 0.63 / 0.29 / 0.27 2.80 / 8.66 0.39 / 0.22 / 0.16 6.09 / 11.59\\nw/ Candidate filter 0.33 / 0.18 /0.13 7.48 / 14.91 0.66 /0.36 /0.32 2.69 /6.50 0.43 /0.23 /0.19 5.38 /11.39\\nEssential', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Essential\\nEssNet [68] - - - - 0.01 / 0.02 / 0.01 48.64 / 52.95\\nMap-free [1] - - - - 0.39 / 0.13 / 0.09 5.75 / 14.20\\nRANSAC [25] 0.27 / 0.16 / 0.13 9.78 / 16.64 0.63 / 0.38 / 0.33 3.03 / 6.15 0.37 / 0.24 / 0.19 6.55 / 11.68\\nMAGSAC++ [5] 0.29 / 0.19 / 0.14 8.90 / 16.00 0.65 / 0.41 / 0.38 2.57 / 5.65 0.40 / 0.26 / 0.21 5.95 / 10.96\\nFSNet (E) 0.36 /0.25 /0.18 6.34 /10.51 0.61 / 0.35 / 0.31 3.22 / 6.78 0.44 / 0.28 / 0.22 5.13 / 8.95\\nFSNet (F + E) 0.35 / 0.24 / 0.17 6.70 / 10.65 0.60 / 0.33 / 0.30 3.41 / 7.15 0.43 / 0.27 / 0.21 5.26 / 9.21\\nw/ Corresp. filter 0.36 /0.25 /0.18 6.34 /10.51 0.65 / 0.41 / 0.38 2.57 / 5.65 0.44 /0.29 /0.23 5.07 /8.75\\nw/ Candidate filter 0.33 / 0.22 / 0.16 7.71 / 13.41 0.69 /0.44 /0.40 2.40 /5.26 0.44 / 0.28 /0.23 5.14 / 9.48\\nTable 2. Fundamental and essential matrix estimation on ScanNet . We compute the MAA@10° and Median error (°) metrics in the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='test split of ScanNet, and divide the pairs of images based on the number of SP-SG correspondences to highlight the benefits of FSNet,\\nwhich results in 3,522 (0-100) and 1,478 (100-Inf) image pairs. In fundamental and essential estimation, we see that when number of\\ncorrespondences is small (0-100), FSNet provides a more robust solution than competitors. In the overall split (All), FSNet obtains more\\nprecise rotation errors for the fundamental estimation tasks, while outperforming in all metrics in essential estimation. Moreover, we also\\nshow how FSNet and MAGSAC++ can be easily combined to obtain more reliable approaches.\\nhypotheses. In batch generation, we cluster the hypotheses\\ninto bins based on the pose error and randomly select a bin,\\nfrom which one hypothesis is uniformly sampled.\\nFor the indoor model, we use the training splits proposed\\nin [50]. For our outdoor model, we use the MegaDepth\\ntraining and validation scenes proposed in [50], except for', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='scenes in the test set of the CVPR IMW 2019 PhotoTourism\\ndataset [32, 52] as in [46]. For both datasets, we generate a\\ntotal of 90,000 and 30,000 image pairs with 500 fundamen-\\ntal and essential matrix hypotheses for each image pair. To\\nhighlight the benefit of FSNet over hard image pairs, we\\nsample image pairs in both datasets that have a visual over-\\nlapping score between 10% and 40% of the image.\\nWe indicate the training source of FSNet by (F) and\\n(E) for fundamental and essential matrices, respectively.\\nWe also found that the distribution of essential matrices\\nis very different from distribution of fundamental matrices\\n(see supplementary), so, we combine fundamental and es-\\nsential matrix hypotheses into (F + E) training dataset.\\nFSNet is trained end-to-end with randomly initialized\\nweights, a learning rate of 1×10−4and a batch size of 56\\nimage pairs. We train on four V100 GPUs and the network\\nconverges after 72 hours. For the feature extractor, we use a', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ResNet-18 [28] as in [50]. The transformer block uses three\\nattention layers ( Nt= 3), and we sample D= 45 match-\\ning candidates along the epipolar line. The loss formulation\\nusests= 25 , such that the pose errors above 40°are (soft)\\nclamped. At test time, FSNet selects the model hypothesisthat returns the minimum pose error, where the pose error\\nforFiis computed as ei= max( eR\\ni, et\\ni).\\n5. Experiments\\nThis section presents results for different fundamental\\nand essential hypothesis scoring methods. We first compute\\ncorrespondences with SP-SG [19,46], and then use the uni-\\nversal framework USAC [41] to generate hypotheses with\\na uniformly random minimal sampling. Besides the inlier\\ncounting of RANSAC [25], we also use the state-of-the-art\\nMAGSAC++ scoring function [5], and compare them both\\nto FSNet scoring method. To control for randomness, we\\nuse the same set of hypotheses generated for each image\\npair for the evaluation of all methods. We refine all the se-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='lected hypotheses by applying LSQ fitting followed by the\\nLevenberg-Marquardt [39] optimization to the correspon-\\ndences that agree with the best hypothesis model.\\nTo evaluate the scoring methods, we decompose the hy-\\npothesis (fundamental or essential matrix) into rotation ma-\\ntrix and translation vectors, and compute the angular errors\\n(eRandet) w.r.t ground truth rotation and translation. We\\nreport the mean Average Accuracy (mAA), which thresh-\\nolds, accumulates, and integrates the errors up to a max-\\nimum threshold (in our experiments 10°) [32]. The inte-\\ngration over the accumulated values gives more weight to\\naccurate poses and discards any camera pose with a relative\\nangular error above the maximum threshold. Besides the\\n8984\\n0 10 20 30 40 50 60\\nCorrect F-Mats selection (%)5°10°20°\\n 18.92 16.04 13.54 13.98 41.40 38.02 35.94 31.22 63.40 63.02 62.50 50.04Combining FSNet and MAGSAC++\\nMAGSAC++\\nFSNet\\nCorresp. filter\\nCandidate filterFigure 5. Combining FSNet and MAGSAC++. Complement-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ing both scoring methods retrieves the highest number of correct\\nfundamental matrices under all error thresholds (5°, 10° and 20°).\\nmAA, we report the median errors of the selected models.\\n5.1. Indoor Pose Estimation\\nEvaluations on the Scannet test set are shown in Table 2.\\nMotivated by the analysis in Section 3, and to emphasize the\\nbenefits of FSNet, we also report results on two types of im-\\nage pairs: image pairs with up to 100 input correspondences\\nand image pairs with more than 100 input correspondences.\\nThe threshold is chosen based on our validation, where 64%\\nof the image pairs do not have more than 100 correspon-\\ndences, and hence, top-performing methods can struggle.\\nWe see that in the few-correspondences split (0-100),\\nall FSNet models outperform RANSAC and MAGSAC++,\\nbeing especially effective on the angular translation error\\nmetrics. Overall, FSNet returns the best mAA score and\\nmedian error for translation, while having comparable re-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='sults to MAGSAC++ on rotation, both for fundamental and\\nessential matrix evaluations. We also observe that FSNet\\n(F+E) does not score as well as models trained on F or E\\nhypotheses only, further indicating differences in the distri-\\nbution of fundamental and essential matrices.\\nFor essential matrix evaluations, we also report scores of\\nrelative pose regression methods (RPR), i.e., EssNet [68]\\nand Map-free [1], which also do not rely on correspon-\\ndences. FSNet (E) obtains the best mAA scores and median\\nerrors in the full test set (All). RPR methods do not compute\\nas accurate poses as RANSAC-based methods or FSNet.\\nAs MAGSAC++ is very effective when SP-SG is able\\nto extract sufficient number of correspondences (100-Inf),\\nwe propose two approaches to combine both methods. In\\nCorresp. filter , we use the number of correspondences such\\nthat if the number is below 100, we use FSNet to score,\\notherwise, we use MAGSAC++. In Candidate filter , we', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='first select top hypotheses using MAGSAC++, and then we\\nuse FSNet to choose the best hypothesis out of the selected\\nhypotheses. We use FSNet to score the top 10 hypothe-\\nses for fundamental, and top 20 hypotheses for essential\\nbased on a hyperparameter search on the validation set.\\nMoreover, in Figure 5, we show the percentage of correct\\nfundamental matrices, i.e., hypotheses with a pose error un-MAA@10° ↑ Median (°) ↓\\n0-100 All All\\nR / t R / t eR/et\\nFundamental\\nRANSAC [25] 0.12 / 0.11 0.55 / 0.21 3.72 / 17.75\\nMAGSAC++ [5] 0.24 / 0.17 0.80 / 0.44 1.39 / 5.42\\nFSNet (F) 0.21 / 0.19 0.71 / 0.35 2.08 / 7.22\\nw/ Corresp. filter 0.21 / 0.19 0.80 / 0.44 1.39 / 5.42\\nw/ Candidate filter 0.27 /0.21 0.82 /0.47 1.32 /4.78\\nEssential\\nRANSAC [25] 0.25 / 0.31 0.81 / 0.59 1.13 / 3.06\\nMAGSAC++ [5] 0.24 / 0.30 0.85 / 0.65 0.92 / 2.18\\nFSNet (E) 0.30 / 0.31 0.81 / 0.55 1.39 / 3.42\\nw/ Corresp. filter 0.30 / 0.31 0.85 / 0.65 0.92 / 2.18\\nw/ Candidate filter 0.29 /0.35 0.87 /0.67 0.88 /2.08', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Table 3. Fundamental and essential matrix estimation on Pho-\\ntoTourism dataset . We show the mAA (10°) and median error\\nwhen using SP-SG correspondences. Contrary to indoor datasets\\n[18], in the outdoor scenario, current feature extractors provide\\nvery accurate and robust features, and in the test split only 1.5% of\\nthe image pairs returned fewer than 100 correspondences. Never-\\ntheless, combinations of FSNet and MAGSAC++ provide the most\\nreliable methods for fundamental and essential matrix estimation.\\nder given thresholds (5°, 10°, and 20°), and see how com-\\nbining both methods always maximizes the number of cor-\\nrect fundamental matrices. Moreover, we observe that in\\nthe more flexible regime (20°) the FSNet performance is\\nalmost identical to the combinations of both methods, in-\\ndicating that although FSNet estimations are not as accu-\\nrate as MAGSAC++, there are fewer catastrophic predic-\\ntions. We link this behaviour with the nature of our method,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='which uses low resolution feature maps to predict the pose\\nerrors, and hence, lacks precision. On the other hand,\\nMAGSAC++ relies on potentially ill-conditioned sets of\\ncorrespondences, resulting in occasional catastrophic poses.\\n5.2. Outdoor Pose Estimation\\nIn Table 3, we show the mAA and median errors of\\nthe fundamental and essential matrix estimation tasks on\\nthe PhotoTourism test set. Analogous to the indoor eval-\\nuation, we split the image pairs based on the number of\\nSP-SG correspondences. Contrary to the indoor scenario,\\nthere are only 1.5% of the image pairs in the test set that\\nhave fewer than 100 SP-SG correspondences. This is in line\\nwith the analysis of Figure 2, where scoring failures are not\\nas common in outdoor dataset as in indoor dataset. There\\nare multiple explanations of these phenomena: (i) walls and\\nfloors are mostly untextured surfaces which lead to fewer\\nreliable correspondences for indoor images, (ii) indoor sur-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='faces are closer to the camera, so small camera motion can\\nresult in large parts of the scene not being visible from both\\n8985\\nMAA@10° ↑ Median (°) ↓\\n0-100 All All\\nR / t R / t eR/et\\nSupervision\\nRE1 0.23 / 0.12 0.28 / 0.14 8.53 / 19.42\\nCE as in [2] 0.27 / 0.18 0.34 / 0.20 6.94 / 13.21\\nL1 0.25 / 0.16 0.32 / 0.18 7.52 / 14.07\\nFSNet (Soft- L1)0.29 /0.19 0.36 /0.21 6.52 /12.05\\nMAGSAC++ [5] 0.28 / 0.12 0.38 / 0.17 6.35 / 17.89\\nTable 4. Loss function and generalization ablation results on\\nthe fundamental matrix estimation task in the test set of ScanNet.\\nSupervising the training of FSNet directly with the camera pose\\nerror provides the best results, while the CE approach outperforms\\nL1 norm and Sampson distance (RE1) prediction models.\\ncameras, which could result in fewer correspondences, (iii)\\noutdoor datasets are generated with SfM methods that rely\\non 2D-2D correspondences estimated with RANSAC, while\\nindoor datasets rely on RGBD SLAM with dense geometry', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='alignment, so inherent algorithmic bias could result in fewer\\ndifficult image pairs in outdoor datasets [9].\\nIn the fundamental estimation task, FSNet scoring ap-\\nproach obtains the most accurate poses in the low number\\nof correspondences split (0-100), while still outperforming\\nRANSAC inlier counting approach in the full test set. In\\nthe essential estimation task, even though we observe that\\ncorrespondence-based scoring approaches, i.e., RANSAC\\nor MAGSAC++, outperform FSNet, they still benefit from\\ncombining their predictions with FSNet.\\n5.3. Understanding FSNet\\nLoss supervision ablation. In Table 4, we experimen-\\ntally verify our analysis of supervision signals in Section 3.\\nWe show that FSNet trained with Eq. 1 outperforms FSNet\\ntrained using RE1 supervision (best epipolar constraint er-\\nror criteria among oracle models in Table 1) and FSNet\\ntrained using L1norm without soft clamping. We also train\\nFSNet using binary cross-entropy (CE), where fundamen-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tal matrix hypothesis Fiis deemed correct if ei<10◦and\\nincorrect otherwise. We follow [2] and incorporate the net-\\nwork uncertainty in the cross-entropy loss; please see the\\nsupplementary materials for details.\\nFailures . Figure 6 shows failure cases for FSNet hypothesis\\nselection. As FSNet needs to run on hundreds of hypothe-\\nses in a reasonable time, the network is designed to work\\non low resolution images. Unfortunately, at low resolution,\\nmatching features can seem degenerate. Looking into the\\nfailures, we observe that the fundamental matrices selected\\nby FSNet have a valid epipolar geometry, i.e., epipolar lines\\nin image Bcross the surrounding regions of corresponding\\npoints from image A, and hence, the low resolution features\\n((W/8, H/8) = (32 ,32)) given to the pose error regressor\\nlack the precision to produce a good estimate.\\nBest hypothesis FSNet selection \\nFigure 6. Failure examples. We plot the co-visible points and', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='corresponding epipolar lines of the fundamental matrix selected\\nby FSNet and the best hypothesis in the pool. Besides, we also\\nmark the intersection of the best and FSNet epipolar lines. We\\nsee that the lines intersect at positions close to neighbor regions of\\nthe corresponding points, and hence, the low-resolution design of\\nFSNet limits its capability to select the optimal hypothesis.\\nTime analysis. Given a pair of images and 500 hypotheses,\\nFSNet scores and ranks them in 0.97s on a machine with\\none V100 GPU. Running the feature extraction and trans-\\nformer subnetworks takes 138.28ms. This is precomputed\\nonce and reused for every hypothesis. The hypotheses are\\nqueried in batches, so the epipolar cross-attention and pose\\nerror regressor subnetworks run in 167.13ms for a batch of\\n100 fundamental matrices. Besides, strategies such as Can-\\ndidate filter can reduce FSNet time by only scoring promis-\\ning hypotheses, e.g., for fundamental estimation it only adds', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='150ms on top of MAGSAC++ runtime (71ms).\\n6. Conclusions\\nWe introduce FSNet, a correspondence-free two-view\\ngeometry scoring method. Our experiments show that our\\nlearned model achieves SOTA results, performing excep-\\ntionally well in challenging scenarios with few or unreliable\\ncorrespondences. These results suggest exciting avenues for\\nfuture research. We mention some of them here.\\nNaive combinations of FSNet with traditional inlier\\ncounting lead to even higher reliability. This suggests\\nthat more sophisticated combinations of scoring approaches\\ncould lead to further improvements. Alternatively, a bet-\\nter network design may improve prediction accuracy, which\\ncould supersede inlier counting in all scenarios.\\nThe first stages of our pipeline (feature extraction, self\\nand cross-attention blocks) form a subnetwork that precom-\\nputes feature maps for each image pair. This subnetwork\\narchitecture is similar to the latest feature extraction meth-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ods,e.g. [50]. This means that feature map precomputation,\\nlocal feature extraction, feature matching, and hypothesis\\ngeneration can all be performed by this subnetwork. Train-\\ning the whole network on two tasks: estimating good cor-\\nrespondences and two-view geometry scoring could lead to\\nsynergistic improvements on both tasks.\\n8986\\nReferences\\n[1] Eduardo Arnold, Jamie Wynn, Sara Vicente, Guillermo\\nGarcia-Hernando, ´Aron Monszpart, Victor Adrian\\nPrisacariu, Daniyar Turmukhambetov, and Eric Brach-\\nmann. Map-free visual relocalization: Metric pose relative\\nto a single image. Proceedings of the European Conference\\non Computer Vision (ECCV) , 2022. 2, 5, 6, 7\\n[2] Daniel Barath, Luca Cavalli, and Marc Pollefeys. Learning\\nto find good models in RANSAC. In CVPR , pages 15744–\\n15753, 2022. 1, 2, 5, 8\\n[3] Daniel Barath and Ji ˇr´ı Matas. Graph-cut RANSAC. In Pro-\\nceedings of the IEEE conference on computer vision and pat-\\ntern recognition , pages 6733–6741, 2018. 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[4] Daniel Barath, Jiri Matas, and Jana Noskova. MAGSAC:\\nmarginalizing sample consensus. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 10197–10205, 2019. 2\\n[5] Daniel Barath, Jana Noskova, Maksym Ivashechkin, and Jiri\\nMatas. MAGSAC++, a fast, reliable and accurate robust\\nestimator. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition , pages 1304–1312,\\n2020. 1, 2, 3, 6, 7, 8\\n[6] Axel Barroso-Laguna, Edgar Riba, Daniel Ponsa, and Krys-\\ntian Mikolajczyk. Key.Net: Keypoint detection by hand-\\ncrafted and learned CNN filters. In Proceedings of the\\nIEEE/CVF international conference on computer vision ,\\npages 5836–5844, 2019. 2\\n[7] Axel Barroso-Laguna, Yurun Tian, and Krystian Mikola-\\njczyk. ScaleNet: A shallow architecture for scale estimation.\\nInProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition , pages 12808–12818, 2022.\\n2\\n[8] Axel Barroso-Laguna, Yannick Verdie, Benjamin Busam,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and Krystian Mikolajczyk. HDD-Net: Hybrid detector de-\\nscriptor with mutual interactive learning. In Proceedings of\\nthe Asian Conference on Computer Vision , 2020. 2\\n[9] Eric Brachmann, Martin Humenberger, Carsten Rother, and\\nTorsten Sattler. On the limits of pseudo ground truth in vi-\\nsual camera re-localisation. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision , pages 6218–\\n6228, 2021. 8\\n[10] Eric Brachmann and Carsten Rother. Neural-guided\\nRANSAC: Learning where to sample model hypotheses. In\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision , pages 4322–4331, 2019. 2\\n[11] Ruojin Cai, Bharath Hariharan, Noah Snavely, and Hadar\\nAverbuch-Elor. Extreme rotation estimation using dense cor-\\nrelation volumes. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition , pages\\n14566–14575, 2021. 2, 5\\n[12] Luca Cavalli, Viktor Larsson, Martin Ralf Oswald, Torsten\\nSattler, and Marc Pollefeys. AdaLAM: Revisiting hand-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='crafted outlier detection. arXiv preprint arXiv:2006.04250 ,\\n2020. 2\\n[13] Luca Cavalli, Marc Pollefeys, and Daniel Barath. NeFSAC:\\nNeurally filtered minimal samples. In ECCV , 2022. 1, 2[14] Hongkai Chen, Zixin Luo, Jiahui Zhang, Lei Zhou, Xuyang\\nBai, Zeyu Hu, Chiew-Lan Tai, and Long Quan. Learning\\nto match features with seeded graph matching network. In\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision , pages 6301–6310, 2021. 2\\n[15] Ondrej Chum and Jiri Matas. Matching with PROSAC-\\nprogressive sample consensus. In 2005 IEEE computer so-\\nciety conference on computer vision and pattern recognition\\n(CVPR’05) , volume 1, pages 220–226. IEEE, 2005. 1, 2\\n[16] Ond ˇrej Chum, Ji ˇr´ı Matas, and Josef Kittler. Locally opti-\\nmized RANSAC. In Joint Pattern Recognition Symposium ,\\npages 236–243. Springer, 2003. 2\\n[17] Ondrej Chum, Tomas Werner, and Jiri Matas. Two-view ge-\\nometry estimation unaffected by a dominant plane. In 2005', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='IEEE Computer Society Conference on Computer Vision and\\nPattern Recognition (CVPR’05) , volume 1, pages 772–779.\\nIEEE, 2005. 2\\n[18] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\\nber, Thomas Funkhouser, and Matthias Nießner. ScanNet:\\nRichly-annotated 3d reconstructions of indoor scenes. In\\nProceedings of the IEEE conference on computer vision and\\npattern recognition , pages 5828–5839, 2017. 3, 5, 7\\n[19] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-\\nnovich. SuperPoint: Self-supervised interest point detection\\nand description. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition workshops , pages\\n224–236, 2018. 1, 2, 3, 5, 6\\n[20] Yikang Ding, Wentao Yuan, Qingtian Zhu, Haotian Zhang,\\nXiangyue Liu, Yuanjiang Wang, and Xiao Liu. TransMVS-\\nNet: Global context-aware multi-view stereo network with\\ntransformers. In CVPR , pages 8585–8594, 2022. 2\\n[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale. arXiv preprint\\narXiv:2010.11929 , 2020. 2\\n[22] Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle-\\nfeys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-Net:\\nA trainable CNN for joint detection and description of local\\nfeatures. arXiv preprint arXiv:1905.03561 , 2019. 2\\n[23] Hongyi Fan, Joe Kileel, and Benjamin Kimia. On the in-\\nstability of relative pose estimation and RANSAC’s role. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition , pages 8935–8943, 2022. 1,\\n2\\n[24] Mohammed E Fathy, Ashraf S Hussein, and Mohammed F\\nTolba. Fundamental matrix estimation: A study of error cri-\\nteria. Pattern Recognition Letters , 32(2):383–391, 2011. 4\\n[25] Martin A Fischler and Robert C Bolles. Random sample', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='consensus: a paradigm for model fitting with applications to\\nimage analysis and automated cartography. Communications\\nof the ACM , 24(6):381–395, 1981. 1, 6, 7\\n[26] Debabrata Ghosh and Naima Kaabouch. A survey on image\\nmosaicing techniques. Journal of Visual Communication and\\nImage Representation , 34:1–11, 2016. 1\\n[27] Vitor Guizilini, Rares ,Ambrus ,, Dian Chen, Sergey Zakharov,\\nand Adrien Gaidon. Multi-frame self-supervised depth with\\n8987\\ntransformers. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pages 160–\\n170, 2022. 2, 3\\n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In Proceed-\\nings of the IEEE conference on computer vision and pattern\\nrecognition , pages 770–778, 2016. 4, 6\\n[29] Yihui He, Rui Yan, Katerina Fragkiadaki, and Shoou-I Yu.\\nEpipolar transformers. In CVPR , pages 7779–7788, 2020. 2,\\n3\\n[30] Maksym Ivashechkin, Daniel Barath, and Ji ˇr´ı Matas. VSAC:', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Efficient and accurate estimator for h and f. In Proceedings\\nof the IEEE/CVF International Conference on Computer Vi-\\nsion, pages 15243–15252, 2021. 2\\n[31] Wei Jiang, Eduard Trulls, Jan Hosang, Andrea Tagliasacchi,\\nand Kwang Moo Yi. COTR: Correspondence transformer for\\nmatching across images. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision , pages 6207–\\n6217, 2021. 2\\n[32] Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas,\\nPascal Fua, Kwang Moo Yi, and Eduard Trulls. Image\\nmatching across wide baselines: From paper to practice.\\nInternational Journal of Computer Vision , 129(2):517–547,\\n2021. 2, 3, 6\\n[33] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and\\nFranc ¸ois Fleuret. Transformers are RNNs: Fast autoregres-\\nsive transformers with linear attention. In International Con-\\nference on Machine Learning , pages 5156–5165. PMLR,\\n2020. 5\\n[34] Zhaoshuo Li, Xingtong Liu, Nathan Drenkow, Andy Ding,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Francis X Creighton, Russell H Taylor, and Mathias Un-\\nberath. Revisiting stereo depth estimation from a sequence-\\nto-sequence perspective with transformers. In ICCV , pages\\n6197–6206, 2021. 2, 5\\n[35] Zhengqi Li and Noah Snavely. MegaDepth: Learning single-\\nview depth prediction from internet photos. In Proceed-\\nings of the IEEE conference on computer vision and pattern\\nrecognition , pages 2041–2050, 2018. 3, 5\\n[36] David G Lowe. Object recognition from local scale-invariant\\nfeatures. In Proceedings of the seventh IEEE international\\nconference on computer vision , volume 2, pages 1150–1157.\\nIeee, 1999. 1, 2\\n[37] Jiri Matas, Ondrej Chum, Martin Urban, and Tom ´as Pa-\\njdla. Robust wide-baseline stereo from maximally stable ex-\\ntremal regions. Image and vision computing , 22(10):761–\\n767, 2004. 1\\n[38] Dmytro Mishkin, Filip Radenovic, and Jiri Matas. Repeata-\\nbility is not enough: Learning affine regions via discrim-\\ninability. In Proceedings of the European Conference on', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Computer Vision (ECCV) , pages 284–300, 2018. 2\\n[39] Jorge J Mor ´e. The levenberg-marquardt algorithm: imple-\\nmentation and theory. In Numerical analysis , pages 105–\\n116. Springer, 1978. 6\\n[40] Omid Poursaeed, Guandao Yang, Aditya Prakash, Qiuren\\nFang, Hanqing Jiang, Bharath Hariharan, and Serge Be-\\nlongie. Deep fundamental matrix estimation without corre-\\nspondences. In Proceedings of the European Conference on\\nComputer Vision (ECCV) Workshops , pages 0–0, 2018. 2[41] Rahul Raguram, Ondrej Chum, Marc Pollefeys, Jiri Matas,\\nand Jan-Michael Frahm. USAC: A universal framework for\\nrandom sample consensus. IEEE transactions on pattern\\nanalysis and machine intelligence , 35(8):2022–2038, 2012.\\n1, 6\\n[42] Ren ´e Ranftl and Vladlen Koltun. Deep fundamental matrix\\nestimation. In Proceedings of the European conference on\\ncomputer vision (ECCV) , pages 284–299, 2018. 2\\n[43] Jerome Revaud, Philippe Weinzaepfel, C ´esar De Souza, Noe\\nPion, Gabriela Csurka, Yohann Cabon, and Martin Humen-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='berger. R2D2: repeatable and reliable detector and descrip-\\ntor.arXiv preprint arXiv:1906.06195 , 2019. 2\\n[44] Chris Rockwell, Justin Johnson, and David F. Fouhey. The\\n8-point algorithm as an inductive bias for relative pose pre-\\ndiction by vits. In T3DV , 2022. 2\\n[45] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.\\nU-Net: Convolutional networks for biomedical image seg-\\nmentation. In International Conference on Medical image\\ncomputing and computer-assisted intervention , pages 234–\\n241. Springer, 2015. 4\\n[46] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,\\nand Andrew Rabinovich. SuperGlue: Learning feature\\nmatching with graph neural networks. In Proceedings of\\nthe IEEE/CVF conference on computer vision and pattern\\nrecognition , pages 4938–4947, 2020. 1, 2, 3, 5, 6\\n[47] Torsten Sattler, Bastian Leibe, and Leif Kobbelt. Efficient\\n& effective prioritized matching for large-scale image-based\\nlocalization. IEEE transactions on pattern analysis and ma-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='chine intelligence , 39(9):1744–1756, 2016. 1\\n[48] Johannes L Schonberger and Jan-Michael Frahm. Structure-\\nfrom-motion revisited. In Proceedings of the IEEE con-\\nference on computer vision and pattern recognition , pages\\n4104–4113, 2016. 1, 2\\n[49] Charles V . Stewart. MINPRAN: A new robust estimator for\\ncomputer vision. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence , 17(10):925–938, 1995. 2\\n[50] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and\\nXiaowei Zhou. LoFTR: Detector-free local feature matching\\nwith transformers. In Proceedings of the IEEE/CVF con-\\nference on computer vision and pattern recognition , pages\\n8922–8931, 2021. 1, 2, 3, 4, 5, 6, 8\\n[51] Weiwei Sun, Wei Jiang, Eduard Trulls, Andrea Tagliasacchi,\\nand Kwang Moo Yi. ACNe: Attentive context normalization\\nfor robust permutation-equivariant learning. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition , pages 11286–11295, 2020. 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[52] Bart Thomee, David A Shamma, Gerald Friedland, Ben-\\njamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and\\nLi-Jia Li. YFCC100M: The new data in multimedia research.\\nCommunications of the ACM , 59(2):64–73, 2016. 6\\n[53] Yurun Tian, Axel Barroso Laguna, Tony Ng, Vassileios Bal-\\nntas, and Krystian Mikolajczyk. HyNet: Learning local de-\\nscriptor with hybrid similarity measure and triplet loss. Ad-\\nvances in Neural Information Processing Systems , 33:7401–\\n7412, 2020. 2\\n[54] Yurun Tian, Xin Yu, Bin Fan, Fuchao Wu, Huub Heijnen,\\nand Vassileios Balntas. SOSNet: Second order similarity\\n8988\\nregularization for local descriptor learning. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 11016–11025, 2019. 2\\n[55] Wei Tong, Jiri Matas, and Daniel Barath. Deep\\nMAGSAC++. arXiv preprint arXiv:2111.14093 , 2021. 2\\n[56] Philip HS Torr and David W Murray. Outlier detection and\\nmotion segmentation. In Sensor Fusion VI , volume 2059,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='pages 432–443. SPIE, 1993. 1\\n[57] Philip Hilaire Torr, Slawomir J Nasuto, and John Mark\\nBishop. NAPSAC: High noise, high dimensional robust\\nestimation-it’s in the bag. In British Machine Vision Con-\\nference (BMVC) , volume 2, page 3, 2002. 2\\n[58] Philip HS Torr and Andrew Zisserman. MLESAC: A new\\nrobust estimator with application to estimating image geom-\\netry. Computer vision and image understanding , 78(1):138–\\n156, 2000. 1, 2\\n[59] Philip H. S. Torr. Bayesian model estimation and selection\\nfor epipolar geometry and generic manifold fitting. Interna-\\ntional Journal of Computer Vision , 50(1):35–61, 2002. 2\\n[60] Prune Truong, Martin Danelljan, Luc Van Gool, and Radu\\nTimofte. Learning accurate dense correspondences and when\\nto trust them. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pages 5714–\\n5724, 2021. 2\\n[61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Polosukhin. Attention is all you need. Advances in neural\\ninformation processing systems , 30, 2017. 2\\n[62] Yannick Verdie, Kwang Yi, Pascal Fua, and Vincent Lepetit.\\nTILDE: A temporally invariant learned detector. In Proceed-\\nings of the IEEE conference on computer vision and pattern\\nrecognition , pages 5279–5288, 2015. 2\\n[63] Xiaofeng Wang, Zheng Zhu, Fangbo Qin, Yun Ye, Guan\\nHuang, Xu Chi, Yijia He, and Xingang Wang. MVSTER:\\nEpipolar transformer for efficient multi-view stereo. arXiv\\npreprint arXiv:2204.07346 , 2022. 2\\n[64] Kwang Moo Yi, Eduard Trulls, Yuki Ono, Vincent Lepetit,\\nMathieu Salzmann, and Pascal Fua. Learning to find good\\ncorrespondences. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition , pages 2666–2674,\\n2018. 2\\n[65] Kwang Moo Yi, Yannick Verdie, Pascal Fua, and Vincent\\nLepetit. Learning to assign orientations to feature points. In\\nProceedings of the IEEE conference on computer vision and\\npattern recognition , pages 107–116, 2016. 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[66] Jiahui Zhang, Dawei Sun, Zixin Luo, Anbang Yao, Lei\\nZhou, Tianwei Shen, Yurong Chen, Long Quan, and Hongen\\nLiao. Learning two-view correspondences and geometry us-\\ning order-aware network. In Proceedings of the IEEE/CVF\\ninternational conference on computer vision , pages 5845–\\n5854, 2019. 2\\n[67] Chen Zhao, Zhiguo Cao, Chi Li, Xin Li, and Jiaqi Yang.\\nNM-Net: Mining reliable neighbors for robust feature corre-\\nspondences. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition , pages 215–224,\\n2019. 2\\n[68] Qunjie Zhou, Torsten Sattler, Marc Pollefeys, and Laura\\nLeal-Taixe. To learn or not to learn: Visual localizationfrom essential matrices. In 2020 IEEE International Confer-\\nence on Robotics and Automation (ICRA) , pages 3319–3326.\\nIEEE, 2020. 2, 6, 7\\n8989', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Meta-tuning Loss Functions and Data Augmentation for Few-shot Object\\nDetection\\nBerkan Demirel1,2Orhun Bu ˘gra Baran1Ramazan Gokberk Cinbis1\\n1Middle East Technical University2HA VELSAN Inc.\\nbdemirel@havelsan.com.tr bugra@ceng.metu.edu.tr gcinbis@ceng.metu.edu.tr\\nAbstract\\nFew-shot object detection, the problem of modelling novel\\nobject detection categories with few training instances, is\\nan emerging topic in the area of few-shot learning and ob-\\nject detection. Contemporary techniques can be divided\\ninto two groups: fine-tuning based and meta-learning based\\napproaches. While meta-learning approaches aim to learn\\ndedicated meta-models for mapping samples to novel class\\nmodels, fine-tuning approaches tackle few-shot detection\\nin a simpler manner, by adapting the detection model to\\nnovel classes through gradient based optimization. Despite\\ntheir simplicity, fine-tuning based approaches typically yield\\ncompetitive detection results. Based on this observation, we', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='focus on the role of loss functions and augmentations as the\\nforce driving the fine-tuning process, and propose to tune\\ntheir dynamics through meta-learning principles. The pro-\\nposed training scheme, therefore, allows learning inductive\\nbiases that can boost few-shot detection, while keeping the\\nadvantages of fine-tuning based approaches. In addition, the\\nproposed approach yields interpretable loss functions, as\\nopposed to highly parametric and complex few-shot meta-\\nmodels. The experimental results highlight the merits of\\nthe proposed scheme, with significant improvements over\\nthe strong fine-tuning based few-shot detection baselines on\\nbenchmark Pascal VOC and MS-COCO datasets, in terms of\\nboth standard and generalized few-shot performance metrics.\\n1. Introduction\\nObject detection is one of the computer vision problems\\nthat has greatly benefited from the advances in supervised\\ndeep learning approaches. However, similar to the case in', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='many other problems, state-of-the-art in object detection re-\\nlies on the availability of large-scale fully-annotated datasets,\\nwhich is particularly problematic due to the difficulty of\\ncollecting accurate bounding box annotations [18, 46]. This\\npractical burden has lead to a great interest in the approaches\\nAugmentations \\nFSOD model \\nLoss function \\nLoss Query set \\nmAP\\nREINFORCE over the augmentation & loss functions Gradient descent Few-shot support \\nsetRecall \\nPrecision \\nRecall Proxy task \\nFigure 1. The overall architecture of the meta-tuning approach.\\nthat can potentially reduce the annotation cost, such as\\nweakly-supervised learning [29, 57], learning from point\\nannotations [7], and mixed supervised learning [45]. A more\\nrecently emerging paradigm in this direction is few-shot ob-\\nject detection (FSOD). In the FSOD problem, the goal is to\\nbuild detection models for the novel classes with few labeled\\ntraining images by transferring knowledge from the base', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='classes with a large set of training images. In the closely\\nrelated Generalized-FSOD (G-FSOD) problem, the goal is\\nto build few-shot detection models that perform well on both\\nbase and novel classes.\\nFSOD methods can be categorized into meta-learning\\nand fine-tuning approaches. Although meta-learning based\\nmethods are predominantly used in the literature in FSOD\\nresearch [8,22,31,36,52,75,76,79,81,83], several fine-tuning\\nbased works have recently reported competitive results [6,\\n15, 32, 53, 61, 65, 72, 84]. The main premise of meta-learning\\napproaches is to design and train dedicated meta-models\\nthat map given few train samples to novel class detection\\nmodels, e.g. [73] or learn easy-to-adapt models [30] in a\\nMAML [16] fashion. In contrast, however, fine-tuning based\\nmethods tackle the problem as a typical transfer learning\\nproblem and apply the general purpose supervised training\\ntechniques, i.e. regularized loss minimization via gradient-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='based optimization, to adapt a pre-trained model to few-shot\\nclasses. It is also worth noting that the recent results on fine-\\ntuning based FSOD are aligned with related observations on\\nfew-shot classification [9, 12, 63] and segmentation [4].\\nWhile some of the FSOD meta-learning approaches are at-\\ntractive for being able to learn dedicated parametric training\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n7339\\nmechanisms, they also come with two important shortcom-\\nings: (i) the risk of overfitting to the base classes used for\\ntraining the meta-model due to model complexity, and (ii)\\nthe difficulty of interpreting what is actually learned; both of\\nwhich can be crucially important for real-world, in-the-wild\\nutilization of a meta-learned model. From this point of view,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the simplicity and generality of a fine-tuning based FSOD ap-\\nproach can be seen as major advantages. In fact, one can find\\na large machine learning literature on the components (opti-\\nmization techniques, loss functions, data augmentation, and\\narchitectures) of an FT approach, as opposed to the unique\\nand typically unknown nature of a meta-learned inference\\nmodel, especially when the model aims to replace standard\\ntraining procedures for modeling the novel few-shot classes.\\nWhile MAML [16] like meta-learning for quick adaptation\\nis closer in nature to fine-tuning based approaches, the van-\\nishing gradient problems and the overall complexity of the\\nmeta-learning task practically limits the approach to target\\nonly one or few model update steps, whereas an FT approach\\nhas no such computational difficulty.\\nPerhaps the biggest advantage of a fine-tuning based\\nFSOD approach, however, can also be its biggest disad-\\nvantage: its generality may lack the inductive biases needed', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='for effective learning with few novel class samples while\\npreserving the knowledge of base classes. To this end, such\\napproaches focus on the design of fine-tuning details, e.g.\\nwhether to freeze the representation parameters [65], use\\ncontrastive fine-tuning losses [61], increase the novel class\\nvariances [84], introduce the using additional detection heads\\nand branches [15, 72]. However, optimizing such details\\nspecifically for few-shot classes in a hand-crafted manner is\\nclearly difficult, and likely to be sub-optimal.\\nTo address this problem, we focus on applying meta-\\nlearning principles to tune the loss functions and augmen-\\ntations to be used in the fine-tuning stage for FSOD, which\\nwe call meta-tuning (Figure 1). More specifically, much like\\nthe meta-learning of a meta-model, we define an episodic\\ntraining procedure that aims to progressively discover the\\noptimal loss function and augmentation details for FSOD\\npurposes in a data-driven manner. Using reinforcement learn-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ing (RL) techniques, we aim to tune the loss function and\\naugmentation details such that they maximize the expected\\ndetection quality of an FSOD model obtained by fine-tuning\\nto a set of novel classes. By defining meta-tuning over well-\\ndesigned loss terms and an augmentation list, we restrict the\\nsearch process to effective function families, reducing the\\ncomputational costs compared to AutoML methods that aim\\nto discover loss terms from scratch for fully-supervised learn-\\ning [20, 42]. The resulting meta-tuned loss functions and\\naugmentations, therefore, inject the learned FSOD-specific\\ninductive biases into a fine-tuning based approach.\\nTo explore the potential of the meta-tuning scheme for\\nFSOD, we focus on the details of classification loss func-tions, based on the observations that FSOD prediction mis-\\ntakes tend to be in classification rather than localization\\ndetails [61]. In particular, we first focus on the softmax\\ntemperature parameter, for which we define two versions:', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='(i) a simple constant temperature, and (ii) time (fine-tuning\\niteration index) varying dynamic temperature, parameterized\\nas an exponentiated polynomial. In all cases, the parameters\\nlearned via meta-tuning yield an interpretable loss function\\nthat has a negligible risk of over-fitting to the base classes, in\\ncontrast to a complex meta-model. We also model augmen-\\ntation magnitudes during meta-tuning for improving the data\\nloading pipeline for few-shot learning purposes. Addition-\\nally, we incorporate a score scaling coefficient for learning\\nto balance base versus novel class scores.\\nWe provide an experimental analysis on the Pascal\\nVOC [13] and MS-COCO [40] benchmarks for FSOD, using\\nthe state-of-the-art fine-tuning based baselines MPSR [72]\\nand DeFRCN [53]. Our experimental results show that the\\nproposed meta-tuning approach provides significant perfor-\\nmance gains in both FSOD and Generalized FSOD settings,\\nsuggesting that meta-tuning loss functions and data augmen-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tation can be a promising direction in FSOD research.\\n2. Related Work\\nThis section provides an overview of recent developments\\non few-shot image classification, few-shot object detection,\\nautomated loss function and data augmentation discovery.\\nFew-shot classification. Most of the meta-learning ap-\\nproaches for few-shot learning (FSL) of classification mod-\\nels can be grouped as adaptation-based andmapping-based\\napproaches. Adaptation-based (also called gradient-based )\\napproaches aim to learn model parameters that can easily be\\nadapted to new unseen few-shot tasks within a few model up-\\ndate steps, e.g. [17, 38, 47, 48, 51, 54, 58]. Mapping-based ap-\\nproaches (also called metric-based ) aim to bypass a gradient-\\ndescent based adaptation step, and instead learn a data-to-\\nclassifier mapping, e.g. [5, 44, 49, 59, 60, 62, 64, 77, 78, 80].\\nSome of the other notable approaches include learning\\nto generate synthetic data for novel classes [23, 33, 68], us-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ing better feature representations [1, 2, 19, 28, 41, 63, 67] or\\nutilizing differentiable convex solvers [3, 34]. Importantly,\\nseveral works highlight that a carefully trained representa-\\ntion combined with simple fine-tuning or even just shallow\\nclassifiers can yield competitive or better performance than\\nmeta-learning based approaches, e.g. [9, 12, 63].\\nFew-shot object detection. The FSOD approaches can be\\nsummarized as meta-learning and fine-tuning (also called\\ntransfer-learning ) based ones. Most meta-learning based\\nFSOD approaches embrace formulations similar to those\\nused in mapping-based meta-learning approaches for FSL,\\ne.g. [8, 22, 31, 36, 52, 75, 76, 79, 81, 83]. Support feature\\naggregation is one of the main aspects where meta-learning-\\n7340\\nbased methods differ from each other. Xiao and Marlet [75]\\nuse both the differences and the channel-wise multiplication\\nof the features in addition to the combination of the features', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='directly for support-query aggregation. Fan et al. [14] use\\nattention blocks to make support and query features more\\ndistinguishable for base and novel object classes. Zhang et\\nal.[81] use inter-class correlations to highlight important\\nsupport features. Li et al. [36] propose to use specialized\\nsupport and query features for classification and localization.\\nRecent efforts towards improving meta-learning based\\nFSOD include complimentary techniques, mainly to im-\\nprove loss functions, feature matching, and novel class sam-\\nple usage efficiency. [36] uses class margin loss, [26] uses\\nmargin-based ranking loss, [82] uses hybrid loss which con-\\nsist of focal loss, adaptive margin loss and contrastive loss.\\nHuet al. [27] perform feature matching between query and\\nsupport images to use the information from the support im-\\nages more effectively. Similarly, Han et al. [21] construct\\na matching network between query and support instances\\nusing heterogeneous graph convolutional networks. Li and', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Li [35] augment novel class samples via adding Gaussian\\nnoise. Yin et al. [79] decouple classification task from local-\\nization by using the proposed class-conditional architecture.\\nFine-tuning-based methods typically freeze parts of\\na pre-trained detection network, add auxiliary detection\\nheads, increase the novel class variances and then apply\\ngradient descent based model update steps, unlike meta-\\nlearning-based methods that use complex episodic learn-\\ning [15, 32, 53, 61, 65, 71, 72]. Wang et al. [65] propose a\\nFaster-RCNN [56] based approach, where the class-agnostic\\nregion proposal network (RPN) component is kept frozen\\nduring fine-tuning. Sun et al. [61] use a similar approach and\\ndifferently include FPN and RPN layers to the learnable pa-\\nrameter set in the same architecture. These learnable layers\\nallow using contrastive proposal encodings that facilitate the\\nmore accurate classification of novel objects. Wu et al. [72]', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='show that the scale distribution of support set tends to be\\nimbalanced, and proposes a multi-scale positive sample re-\\nfinement (MPSR) branch as an addition to the main model.\\nFanet al. [15] propose Retentive R-CNN architecture to\\nprevent forgetting during fine-tuning for base classes. The\\nobtained object proposals are fed into two ROI detectors\\nresponsible for base class and novel class instances. Qiao\\net al. [53] focus on decoupling network modules, and intro-\\nduce a gradient decoupling layer and prototypical calibration\\nblock. Kaul et al. [32] extend the novel class annotations in\\nthe training set. In this context, the proposed method obtains\\nobject candidates from the base detector, and applies the box\\nrefinement step.\\nWhile our approach is based on fine-tuning based FSOD,\\nwe embrace meta-learn principles to optimize the loss func-\\ntion and augmentations to improve the fine-tuning process', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='for FSOD, without learning a complex and over-fitting-pronemeta-model. The resulting loss function and data augmenta-\\ntions are then utilized within the fine-tuning steps.\\nAutomated loss function discovery. Loss function discov-\\nery is an emerging AutoML topic towards improving the\\nlearning systems in a data-driven manner. Existing methods\\nare mainly based on either (i) constructing the loss func-\\ntion directly from the basic operators [20, 42, 55] or (ii)\\noptimizing parameterized loss functions [37, 66]. For loss\\nconstruction, [42] proposes a genetic algorithm that consists\\nof loss function verification and quality filtering modules. In\\nthis approach, the predefined proxy task eliminates divergent\\nand poor candidate loss functions and survives the promising\\nloss functions for other steps. [20] uses a genetic algorithm\\nto select candidate loss functions from a tree of simple math-\\nematical operations, and the successful loss functions pass', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='to other stages to mutate. [55] suggests a method to learn not\\nonly the loss function but also the whole machine learning al-\\ngorithm from scratch. For loss optimization, [37] re-analyzes\\nthe existing loss functions and presents them in a combined\\nformula. [66] observes that the search space used in [37] can\\nbe too complex, and propose to simplify the search space via\\nheuristics. In contrast to these works targeting supervised\\ntraining scenarios, we aim to adapt loss function learning\\nprinciples to the FSOD problem.\\nAutoML for data augmentation. A variety of auto-\\nmated data augmentation techniques have recently been\\nproposed [10,11,25,39]. Cubuk et al. [10] generate augmen-\\ntation policies using reinforcement learning and a controller\\nRNN. Ho et al. [25] propose a method that reduces the\\ncomputational costs compared to [10] by using a population-\\nbased framework. Similarly, Lim et al. [39] propose a direct\\nBayesian method to reduce costs. Cubuk et al. [11] show', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='that the optimal augmentation magnitudes tend to be similar\\nacross transformations, and the search process can greatly be\\nsimplified by using a shared value. We follow this suggestion\\nand use a shared magnitude across the transforms in our for-\\nmulation. In contrast to these works on supervised learning,\\nhowever, we focus on learning detectors with few-samples.\\nIn summary, while loss function and augmentation dis-\\ncovery topics increasingly attract attention towards improv-\\ning supervised training pipelines, ours is the first work on\\nlearning few-sample specific inductive biases for fine-tuning\\nbased few-shot object detection based on meta-learning and\\nAutoML principles, to the best of our knowledge.\\n3. Method\\nThis section provides a brief summary of the FSOD prob-\\nlem definition and the baseline model we utilize. We then\\npresent our definition and instantiation of meta-tuning.\\nProblem definition. We follow the FSOD setup of [31],', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='where a relatively large set of training images for the set\\nCbofbase classes is made available. Each training im-\\n7341\\nage corresponds to a tuple (x, y)consisting of image x\\nand annotations y={y0, ..., y M}. Each object annotation\\nyi={ci, bi}contains a category label ( ci) and a bounding\\nbox(bi={xi, yi, wi, hi}). Once the FSOD model training\\nis complete, the evaluation is carried out based on a limited\\nnumber ( k) of training images made available for the set Cn\\nof distinct novel (i.e. few-shot) classes.\\nBase model. We use the MPSR FSOD method [72] as\\nthe infrastructure for our loss function and data augmenta-\\ntion search methods. MPSR adapts the Faster-RCNN to be\\nsuitable for fine-tuning-based FSOD and uses an auxiliary\\nmulti-scale positive sample refinement (MPSR) branch to\\nhandle the scale scarcity problems. This branch expands\\nthe scale space of positive samples without increasing im-\\nproper negative instances, unlike feature pyramid networks', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and image pyramids that do not change data distribution,\\nhence the scale sparsity problem. In this context, objects\\nin the images are cropped and resized in multiple sizes to\\ncreate scale pyramids. The MPSR uses two groups of loss\\nfunctions for the region proposal network (RPN) and detec-\\ntion heads, and feeds differently scaled positive samples to\\nthese loss functions together with the main detection branch.\\nFinally, we note that the proposed approach can in principle\\nbe applied to virtually any fine-tuning based FSOD model.\\n3.1. Meta-tuning loss functions\\nOur main goal is to improve few-shot detector fine-tuning\\nbased on meta-learning principles. For meta-tuning the\\nFSOD loss, we specifically focus on the classification loss\\nterm, as the FSOD errors tend to be primarily caused by\\nmisclassifications [61]. The MPSR classification loss term\\ncan be expressed as follows:\\nℓcls(x, y) =−1\\nNROINROIX\\nilog \\nef(xi,yi)\\nP\\nyef(xi,y)!\\n(1)\\nwhere NROI is the number of ROIs ( i.e. candidate regions)', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='in an image, yiis the groundtruth class label for the i-th ROI,\\nandf(xi, y)is the corresponding class yprediction score.\\nTo add more flexibility into the loss function, we re-define it\\nas a parametric function ℓcls(x, y;ρ), where ρrepresents the\\nloss function parameters. First, we introduce a temperature\\nscalar ρτ,i.e.ρ= (ρτ):\\nℓcls(x, y;ρ) =−1\\nNROINROIX\\nilog \\nef(xi,yi)/ρτ\\nP\\ny′ef(xi,y′)/ρτ!\\n(2)\\nOur motivation comes from the observations on the impor-\\ntance of temperature scaling in log loss on various other\\nproblems, such as knowledge distillation [24], few-shot clas-\\nsification [49, 78], and zero-shot learning [43]. While tem-\\nperature is typically tuned in a manual manner, here we\\naim to meta-learn it specifically for fine-tuning based FSODpurposes, giving a chance to observe the behavior of meta-\\ntuning in a simple case. We also define a more sophisticated\\nvariant of the loss function by defining the dynamic tempera-\\nturefunction fρandnovel class scaling α:\\nℓcls(x, y;ρ) =−1', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ℓcls(x, y;ρ) =−1\\nNROINROIX\\nilog \\neα(yi)f(xi,yi)/fρ(t)\\nP\\ny′eα(y′)f(xi,y′)/fρ(t)!\\n(3)\\nwhere fρ(t) = exp( ρat2+ρbt+ρc). Here, ρ= (ρa, ρb, ρc)\\nis a 3-tuple of polynomial coefficients, and t∈[0,1]is the\\nnormalized fine-tuning iteration index. The temperature can\\nincrease or decrease over time, making the predicted class\\ndistributions smoother or sharper. α(y)is set to 1fory∈Cb,\\nand otherwise the novel class score scaling coefficient ρα, as\\na way to learn base and novel score balancing.\\n3.2. Meta-tuning augmentations\\nFor meta-tuning augmentations, we focus on the photo-\\nmetric augmentations that are likely to be transferable from\\nbase to novel classes. In this context, we model the bright-\\nness, saturation, contrast, and hue transforms, with a shared\\nmagnitude parameter ( ρaug), which is known to be effective\\nfor supervised training [11].\\n3.3. Meta-tuning procedure\\nIn our work, we utilize a REINFORCE [70] based rein-\\nforcement learning (RL) approach to search for the optimal', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='loss function and augmentations, where we use the AutoML\\napproach of Wang et al. [66] on loss function search for\\nfully-supervised face recognition as our starting point.\\nIn order to meta-tune the loss function and augmentations\\nto maximize FSOD generalization abilities, we generate\\nproxy tasks over base class training data to imitate real FSOD\\ntasks over the novel classes. For this purpose, we divide base\\nclasses into two subsets, proxy-base Cp-base and proxy-novel\\nCp-novel . We then construct three non-overlapping data set\\nsplits using the base class training set: (i) Dp-pretrain contain-\\ningCp-base -only samples, used for training a temporary object\\ndetection model for meta-tuning purposes; (ii) Dp-support con-\\ntaining samples of Cp-base∪Cp-novel classes to be used as fine-\\ntuning images during meta-tuning; (iii) Dp-query containing\\nsamples of Cp-base∪Cp-novel classes to be used for evaluating\\nthe generalized FSOD performance of a fine-tuned model\\nduring meta-tuning.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='during meta-tuning.\\nWe generate a series of FSOD proxy tasks for meta-\\ntuning, similar to episodic meta-learning: at each proxy\\ntaskT, we sample a few-shot training set from Dp-support .\\nWe also sample a loss function/augmentation magnitude pa-\\nrameter combination ρ, where each ρj∈ρis modeled in\\nterms of a Gaussian distribution: ρj∼ N (µj, σ2). Using\\nthe loss function or augmentations corresponding to the sam-\\npledρ, we fine-tune the initial model on the support images\\nusing gradient-based optimization, and compute the mean\\n7342\\nInput \\nImages \\nBackbone + FPN ROI Pooling RPNObjectness \\nLocalization \\nFeature \\nExtractor Classiﬁcation Localization Reﬁnement Branch \\nShared  Weights \\nROI Cls Loss Obj. Loss \\nAPRegress. Loss \\nN(ρ; μ, σ2)Sampling \\nUpdate μ RGet reward Object Detection Figure 2. The meta-tuning approach. At each RL iteration over a proxy task, the distribution parameters modeling the loss function and', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='augmentations are updated as a function of the obtained mAP scores, towards improved training with few-samples.\\naverage precision (mAP) scores on Dp-query . We get mul-\\ntiple mAP scores by repeating this process multiple times\\nover multiple proxy support samples. Meta-tuning is then\\ncarried over by updating µvalues via the REINFORCE rule\\nafter each episode, towards finding µvalues centered around\\nwell-performing ρcombinations.\\nµ′\\nj←µj+ηR(ρ)∇µlog (p(ρj;µj, σ)) (4)\\nwhere p(ρ;µ, σ)is the Gaussian probability density function,\\nηis the RL learning rate.\\nWe apply the REINFORCE update rule using the ρwith\\nthe highest reward per episode. R(ρ)is the normalized re-\\nward function obtained by whitening the mAP scores. We\\nempirically observe that normalization improves the results\\n(Section 4) since without reward normalization, the RL up-\\ndates are scaled with respect to the inherent difficulty of the\\nproxy task, which greatly varies depending on the sampled', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='support examples. Reward normalization approximately re-\\nmoves the average reward, enabling better performing ρ\\nsamples to influence based on their relative success.\\nFinally, similar to [50], starting with σ= 0.1, we dimin-\\nishσover the RL iterations to progressively reduce explo-\\nrations by sampling more conservatively, which improves\\nconverge. The final scheme is illustrated in Figure 2.\\n4. Experiments\\nMetrics. We use mAP to evaluate the base and novel class\\ndetection results separately. To evaluate the generalized\\nFSOD performance, we use the Harmonic Mean (HM) met-\\nric to compute a balanced aggregation of base and novel\\nclass performance scores. Adapted from generalized zero-\\nshot learning [74], HM is defined as the harmonic mean of\\nmAP baseand mAP novelscores.\\nDatasets. We use Pascal VOC [13] and MS COCO [40]\\nwith the same splits defined in FSOD benchmarks [65, 72].On Pascal VOC, three separate base/novel class splits exist,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='where each one consists of 15base and 5novel classes. In\\neach split, we select 5 base classes to mimic novel classes\\nduring meta-tuning. On MS-COCO, we select 15 base\\nclasses to mimic novel classes in each proxy task, and evalu-\\nate the models for the 10-shot and 30-shot settings.\\nBaselines. We primarily use the MPSR [72] and De-\\nFRCN [53] as our baselines, which are among the best per-\\nforming fine-tuning based FSOD methods on Pascal VOC.\\nFor the DeFRCN experiments, we transfer the meta-tuned\\nloss functions and augmentation magnitudes from MPSR to\\nthe DeFRCN method, which are both based on Faster-RCNN.\\nWe take the results for FRCN [76], Ret. R-CNN [15], Meta-\\nRCNN [76], FSRW [31], MetaDet [69], FsDetView [75]\\nand ONCE [52] from [15] for a fair comparison. For the\\nMPSR, DeFRCN ( seed is set to 0) and FSCE [61], we report\\nthe results we obtain experimentally. We take the results\\nfor TFA+Hal [84], CME [36], TIP [35], DCNet [27], QA-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='FewDet [21] FADI [6], LVC [32], KFSOD [83] and FCT [22]\\nfrom the original papers. Finally, while it is difficult to fairly\\ncompare fine-tuning versus meta-learning based approaches,\\nwe provide a discussion in the supplementary material.\\nImplementation details. We use 200 RL episodes for loss\\nfunction meta-tuning, with REINFORCE learning rate set\\nto0.0005 . The meta-tuning for augmentation parameter is\\ncarried out using the trained and frozen the loss function\\nparameters. We keep the fine-tuning implementation details\\nof MPSR unchanged, which uses 4000 and8000 gradient\\ndescent iterations for 10-shot and 30-shot experiments on\\nMS-COCO, and 2000 iterations on Pascal VOC. We will\\npublish the full source code upon publication; a preliminary\\nversion is provided as supplementary material.\\n4.1. Main results\\nWe first compare the meta-tuning results against the cor-\\nresponding MPSR baseline in Table 1. In the table, Meta-\\n7343\\nMethod/ShotPascal VOC MS-COCO', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Novel Classes All Classes (HM) Novel Classes All Classes (HM)\\n1 2 3 5 10 1 2 3 5 10 10 30 10 30\\nMPSR [72] 33.1 37.2 44.3 47.1 52.1 43.1 47.4 54.5 57.2 60.8 9.1 13.7 11.5 15.0\\nMPSR+Meta-Static 33.4 39.4 45.1 47.3 52.6 43.7 50.4 55.4 57.5 61.4 10.1 14.8 12.7 16.4\\nMPSR+Meta-Dynamic 34.5 39.8 45.0 48.2 52.5 45.0 51.0 55.5 58.3 61.6 11.9 14.9 14.3 16.6\\nMPSR+Meta-ScaledDynamic 35.2 40.3 45.8 48.4 52.9 45.6 51.2 55.9 58.3 61.8 12.3 15.0 14.4 16.7\\nMPSR+Aug 34.6 38.6 46.0 48.3 52.7 45.1 49.5 56.2 58.4 62.0 9.9 14.9 12.5 16.3\\nMPSR+Meta-Static+Aug 35.3 39.1 46.1 48.4 52.7 45.9 49.9 56.2 58.3 61.8 10.2 15.2 12.8 16.7\\nMPSR+Meta-Dynamic+Aug 35.4 39.6 46.5 48.9 53.3 46.0 50.5 56.8 58.9 62.5 12.1 15.3 14.5 16.8\\nMPSR+Meta-ScaledDynamic+Aug 35.8 40.6 46.8 49.2 53.7 46.3 51.5 57.0 59.2 62.7 12.5 15.4 14.7 16.9\\nTable 1. FSOD (mAP) and G-FSOD (HM of the base and novel class mAPs) results on Pascal VOC and MS-COCO datasets for MPSR\\nbaseline method. HM stands for harmonic mean.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Static ,Meta-Dynamic ,Meta-ScaledDynamic refer to meta-\\ntuning a single temperature, dynamic temperature, and novel\\nclass scaled dynamic temperature functions, respectively.\\nSimilarly, Aug,Meta-Static+Aug ,Meta-Dynamic+Aug , and\\nMeta-ScaledDynamic+Aug refer to meta-tuning only aug-\\nmentation, single temperature and augmentation, dynamic\\ntemperature and augmentation, and novel class scaled dy-\\nnamic temperature and augmentation functions, respec-\\ntively. We observe that meta-tuning consistently improves\\nthe FSOD and G-FSOD results of the MPSR model. We\\nalso observe steady improvements gradually from the base-\\nline to Meta-Static, to Meta-Dynamic, and finally to Meta-\\nScaledDynamic. In addition, the meta-tuned augmentation\\nmagnitude parameter also contributes positively to the few-\\nshot object detection performance. The overall consistency\\nof the improvements provides positive evidence for the value\\nof loss and augmentation meta-tuning.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Pascal VOC results. In Table 2, we report the Pascal\\nVOC results for our MPSR and DeFRCN based Meta-\\nScaledDynamic+Aug approach and compare them against\\nthe state-of-the-art fine-tuning based FSOD methods. While\\nwe present the scores averaged over the three splits in this\\ntable, additional per-split FSOD and G-FSOD results can\\nbe found in the supplementary material. The left side of\\nTable 2 presents the FSOD results for the varying number of\\nsupport images. We observe that DeFRCN combined with\\nMeta-ScaledDynamic+Aug, i.e. meta-tuning of the score\\ncoefficient, dynamic temperature and the augmentation pa-\\nrameter, yields the best mAP scores in all k-shot settings\\namong all methods.\\nThe right side of Table 2 presents the G-FSOD results\\non Pascal VOC. We observe that the best-performing Meta-\\nScaledDynamic+Aug method improves the HM scores fur-\\nther above the state-of-the-art in all k-shot settings. Overall,\\nthese results suggest that the proposed framework is an ef-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='fective way for meta-learning inductive biases to be used in\\nfine-tuning-based FSOD.\\nFigure 3 presents visual detection examples without and\\nwith meta-tuned scaled dynamic temperature and augmenta-\\ntions in the first and second rows, respectively. We observevarious improvements, such as reductions in false positives,\\nimproved recall, and more precise boxes, most likely due to\\nthe improved model fitting in the low-data regime.\\nMS-COCO results. In Table 3, we compare the MPSR and\\nDeFRCN based Meta-ScaleDynamic+Aug results against\\nother fine-tuning based FSOD methods that report 10-shot\\nand30-shot results on the MS-COCO dataset. We observe\\nthat with meta-tuning, the FSOD scores of MPSR improve\\nfrom 9.1to12.5(10-shot mAP), and from 13.7to15.4(30-\\nshot mAP). We also observe that the scores of DeFRCN\\nimprove from 18.5to18.8(10-shot mAP), and from 21.9\\nto23.4(30-shot mAP), obtaining the best and second best\\nresults against all other models. Similarly, in the case of G-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='FSOD, with meta-tuning, the 10-shot HM score of DeFRCN\\nimproves from 24.0to24.4, outperforming all other models.\\nIn addition, the 30-shot HM score of DeFRCN improves\\nfrom 26.8to28.0, which is slightly below the 28.1score of\\nLVC-PL [32].\\n4.2. Ablation studies\\nMeta-tuning details. The proposed meta-tuning approach\\ninvolves three important technical details: Proxy-novel im-\\nitation ,model re-initialization , and reward normalization .\\nProxy-novel imitation refers to reinforcement learning over\\nthe sampled proxy-novel tasks, instead of the whole train-\\ning set, to mimic the test-time FSOD challenges. Model\\nre-initialization is the re-initialization of the base model for\\neach task. Without re-initialization, not only the sampled\\nloss/augmentation parameters and tasks but also the accumu-\\nlated model updates undesirably affect the rewards. Reward\\nnormalization further reduces the effect of task difficulty\\nvariance by normalizing the rewards obtained within a single', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='episode, allowing a more isolated assessment of the sampled\\nloss functions and augmentations.\\nWe evaluate the contributions of these three important\\ndetails in terms of G-FSOD HM scores using the 5-shot\\nsetting of Pascal VOC Split-1 with MPSR+Meta-Dynamic.\\nThe results averaged over 5runs are given in Table 4. We\\nobserve that each component progressively improves the\\n7344\\nMethod/ShotNovel Classes All Classes (HM)\\n1 2 3 5 10 1 2 3 5 10\\nFRCN [76] (ICCV’19) 16.1 20.6 28.8 33.4 36.5 25.9 31.7 40.0 44.3 46.7\\nTFA-fc [65] (ICML’20) 27.6 30.6 39.8 46.6 48.7 40.5 44.1 52.9 58.3 59.9\\nTFA-cos [65] (ICML’20) 31.4 32.6 40.5 46.8 48.3 44.6 46.0 53.5 58.4 59.6\\nFSCE [61] (CVPR’21) 29.2 36.3 42.5 47.1 52.2 41.8 48.8 54.2 57.7 61.0\\nRet. R-CNN [15] (CVPR’21) 31.4 37.1 41.4 46.8 48.8 44.7 50.5 54.7 59.1 60.8\\nTFA+Hal [84] (CVPR’21) 32.9 35.5 40.4 46.3 48.1 - - - - -\\nFADI [6] (NeurIPS’21) 42.2 46.5 47.9 52.4 56.9 - - - - -\\nLVC [32] (CVPR’22) 30.9 35.4 43.6 51.1 54.1 - - - - -', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='LVC-PL [32] (CVPR’22) 45.2 45.0 54.8 57.5 58.6 - - - - -\\nMPSR [72] (ECCV’20) 33.1 37.2 44.3 47.1 52.1 43.1 47.4 54.5 57.2 60.8\\nDeFRCN [53] (ICCV’21) 46.5 52.6 55.9 60.0 60.8 57.6 62.5 64.7 67.6 67.8\\nMPSR+Meta-ScaledDynamic+Aug 35.8 40.6 46.8 49.2 53.7 46.3 51.5 57.0 59.2 62.7\\nDeFRCN+Meta-ScaledDynamic+Aug 49.2 54.0 57.2 61.3 61.8 59.8 63.7 65.9 68.6 68.7\\nTable 2. FSOD (mAP) and G-FSOD (HM of the base and novel class mAPs) results on Pascal VOC. The best and the second-best results are\\nmarked with red and blue. HM stands for harmonic mean.\\nSplit-1/5-shot Split-1/10-shot Split-2/5-shot Split-2/10-shot Split-3/5-shot Split-3/10-shot\\nFigure 3. Qualitative results using MPSR without (first row) and with (second row) meta-tuning, over multiple Pascal VOC splits. Base and\\nnovel class detections are shown with green and red boxes, respectively. (Best viewed in color.)\\nMethod/ShotsNovel Classes All Classes (HM)\\n10-shot 30-shot 10-shot 30-shot\\nFRCN [76] (ICCV’19) 9.2 12.5 12.8 15.6', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='FRCN-BCE [65] (ICML’20) 6.4 10.3 10.9 16.1\\nTFA-fc [65] (ICML’20) 10.0 13.4 15.4 19.4\\nTFA-cos [65] (ICML’20) 10.0 13.7 15.6 19.8\\nMPSR [72] (ECCV’20) 9.1 13.7 11.5 15.0\\nFSCE [61] (CVPR’21) 10.5 14.4 16.0 20.2\\nRet. R-CNN [15] (CVPR’21) 10.5 13.8 16.6 20.4\\nFADI [6] (NeurIPS’21) 12.2 16.1 - -\\nDeFRCN [53] (ICCV’21) 18.5 21.9 24.0 26.8\\nLVC [32] (CVPR’22) 12.1 17.8 17.8 22.8\\nLVC-PL [32] (CVPR’22) 17.8 24.5 22.8 28.1\\nMPSR+Meta-ScaledDynamic+Aug 12.5 15.4 14.7 16.9\\nDeFRCN+Meta-ScaledDynamic+Aug 18.8 23.4 24.4 28.0\\nTable 3. Comparison of Meta-ScaledDynamic results to the fine-\\ntuning based (G-)FSOD methods on the MS-COCO dataset. The\\nbest and the second-best results are marked with red and blue.\\nHM scores, and the most significant contribution is made\\nby reward normalization, which improves from 62.1to63.3.\\nWe also observe that reward normalization considerably im-\\nproves the overall experimental stability. To quantify thisProxy-novel imit. Model re-init. Reward norm. HM\\n✗ ✗ ✗ 61.5\\n✓ ✗ ✗ 61.8', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='✓ ✗ ✗ 61.8\\n✓ ✓ ✗ 62.1\\n✓ ✓ ✓ 63.3\\nTable 4. Evaluation of meta-tuning details. Proxy-novel imitation is\\nthe imitation of novel classes using a subset of base classes. Model\\nre-initialization is the re-initialization of the base model at each\\ntask. Reward normalization is within-episode normalization of the\\nmAP scores during meta-tuning.\\nobservation, we estimate the 95 %confidence interval over\\nthe runs using CI= 1.96s√n, where s,n, and 1.96are the\\nstandard deviation, number of runs, and Z-value, respec-\\ntively [65]. According to this estimator, the normalization\\nstep narrows the confidence interval from ±0.75to±0.13,\\nproviding a clear improvement in reliability.\\nLearned loss functions. In Figure 4, we plot the learned\\nloss functions according to the µvalues obtained at the\\nend of the RL process. The upper plot shows the dynamic\\n7345\\n0.0 0.2 0.4 0.6 0.8 1.0\\ntime0.750.800.850.900.951.001.05valuesplit-1\\nsplit-2\\nsplit-3\\n0.0 0.2 0.4 0.6 0.8 1.0\\ntime0.70.80.91.01.11.2valuesplit-1 (temp.)', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='split-1 (scale)\\nsplit-2 (temp.)\\nsplit-2 (scale)\\nsplit-3 (temp.)\\nsplit-3 (scale)Figure 4. The dynamic temperature functions and score scaling co-\\nefficients learned by the meta-tuning process, using Meta-Dynamic\\n(upper) and Meta-ScaledDynamic (lower) formulations. Results\\nfor each Pascal VOC split is shown with a separate curve.\\ntemperature functions learned over three different splits. We\\nobserve that temporally attenuated temperature values are\\npreferred consistently, sharpening the predictions towards\\nthe end of the fine-tuning process. The lower plot shows\\nthe learned dynamic temperature functions with novel class\\nscore scaling. The learned scaling coefficients, i.e.µαof\\nthe learned ραdistribution, are shown as horizontal lines.\\nWe observe that similar dynamic temperature functions are\\nlearned, and µαvalues vary between 1.09to1.2, suggesting\\nthat the meta-tuning process learns to boost the novel class\\nscores. The interpretability of these outcomes, we believe,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='highlights a significant advantage of loss meta-tuning. In\\nthe context of interpretability, we observe that as the fine-\\ntuning process continues on the few-shot training set, the\\npredictions are progressively made sharper, i.e. the loss\\nbecomes more sensitive to classification errors and enforces\\ntowards making more confident correct predictions. This\\nis in alignment with one of our original motivations for\\nreducing the dominating classification errors in G-FSOD,\\nas the meta-tuning process automatically learns to enforce\\nmore accurate classifications, where the curve steepness and\\nthe numerical ranges are learned via RL.\\nLearned augmentations. The learned photometric augmen-S/M TFA [65] TFA+Hal [84] TFA+Meta-ScaledDynamic+Aug\\n1 3.4 3.8 4.7\\n2 4.6 5.0 5.8\\n3 6.6 6.9 7.1\\nTable 5. Low-shot (1-shot, 2-shot and 3-shot) experiments on MS-\\nCOCO dataset with novel classes.\\ntation magnitude values learned are 0.29,0.24,0.13, and\\n0.36for Pascal VOC split-1, split-2, split-3, and MS-COCO', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='datasets, respectively. We observe that the learned augmen-\\ntation magnitudes positively contribute to the performance.\\nAccording to the results in Table 1, the average Pascal VOC\\nsplit-1/1-shot score increases from 33.1to34.6with only\\naugmentation steps.\\nVery low-shot experiments. Finally, we evaluate the meta-\\ntuning approach in low-shot many-class settings. [84] pro-\\nposes TFA+Hal method that uses the TFA baseline and con-\\nducts 1-shot, 2-shot, and 3-shot FSOD on the MS-COCO\\ndataset. As we already observe the positive effects of the\\nloss terms and augmentation magnitudes obtained from the\\nMPSR on the DeFRCN, we similarly apply the learned pa-\\nrameters to the TFA baseline. The results are presented in\\nTable 5. We observe that results are consistently improved\\nusing the meta-tuned functions on the TFA baseline.\\n5. Conclusion\\nFine-tuning based frameworks offer simple and reliable\\napproaches to building detection models from few samples.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='However, a major limitation of the existing fine-tuning-based\\nFSOD models is their focus on the hand-crafting the de-\\nsign of fine-tuning details for few-shot training, which is\\ninherently difficult and likely to be sub-optimal. Towards\\naddressing this limitation, we propose to meta-learn the fine-\\ntuning based learning dynamics as a way of introducing\\nlearned inductive biases for few-shot learning. The proposed\\ntuning scheme uses meta-learning principles with reinforce-\\nment learning, and obtains interpretable loss functions and\\naugmentation magnitudes for few-shot training. Our compre-\\nhensive experimental results on Pascal VOC and MS COCO\\ndatasets show that the proposed meta-tuning approach consis-\\ntently provides significant performance improvements over\\nthe strong fine-tuning based few-shot detection baselines in\\nboth FSOD and G-FSOD settings.\\nWhile we restrict our experiments to loss and augmenta-\\ntion functions, meta-tuning other learning components, e.g.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='initial model, and applications to other few-shot learning\\nproblems can be interesting future work directions.\\nAcknowledgements. This work was supported in part by the\\nTUBITAK Grant 119E597 and a Google Faculty Research\\nAward.\\n7346\\nReferences\\n[1]Peyman Bateni, Jarred Barber, Jan-Willem van de Meent, and\\nFrank Wood. Enhancing few-shot image classification with\\nunlabelled examples. In Proceedings of the IEEE/CVF Win-\\nter Conference on Applications of Computer Vision (WACV) ,\\npages 2796–2805, January 2022. 2\\n[2]Peyman Bateni, Raghav Goyal, Vaden Masrani, Frank Wood,\\nand Leonid Sigal. Improved Few-Shot Visual Classification.\\narXiv e-prints , page arXiv:1912.03432, Dec. 2019. 2\\n[3]Luca Bertinetto, Joao F. Henriques, Philip Torr, and An-\\ndrea Vedaldi. Meta-learning with differentiable closed-form\\nsolvers. In Proc. Int. Conf. Learn. Represent. , 2019. 2\\n[4]Malik Boudiaf, Hoel Kervadec, Ziko Imtiaz Masud, Pablo\\nPiantanida, Ismail Ben Ayed, and Jose Dolz. Few-Shot Seg-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='mentation Without Meta-Learning: A Good Transductive\\nInference Is All You Need? In arXiv:2012.06166 [cs] , 2021.\\n1\\n[5]Kaidi Cao, Maria Brbi ´c, and Jure Leskovec. Concept learners\\nfor few-shot learning. In Proc. Int. Conf. Learn. Represent. ,\\n2021. 2\\n[6]Yuhang Cao, Jiaqi Wang, Ying Jin, Tong Wu, Kai Chen, Ziwei\\nLiu, and Dahua Lin. Few-shot object detection via association\\nand discrimination. Proc. Adv. Neural Inf. Process. Syst. ,\\n34:16570–16581, 2021. 1, 5, 7\\n[7] Liangyu Chen, Tong Yang, Xiangyu Zhang, Wei Zhang, and\\nJian Sun. Points as queries: Weakly semi-supervised object\\ndetection by points. Proc. IEEE Conf. Comput. Vis. Pattern\\nRecog. , pages 8819–8828, 2021. 1\\n[8]Tung-I Chen, Yueh-Cheng Liu, Hung-Ting Su, Yu-Cheng\\nChang, Yu-Hsiang Lin, Jia-Fong Yeh, Wen-Chin Chen, and\\nWinston Hsu. Dual-awareness attention for few-shot object\\ndetection. IEEE Transactions on Multimedia , 2021. 1, 2\\n[9]Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Wang, and Jia-Bin Huang. A Closer Look at Few-shot Classi-\\nfication. In ICLR 2019 , 2019. 1, 2\\n[10] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-\\nvan, and Quoc V Le. Autoaugment: Learning augmentation\\npolicies from data. arXiv preprint arXiv:1805.09501 , 2018. 3\\n[11] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\\nLe. Randaugment: Practical automated data augmentation\\nwith a reduced search space. In Proc. IEEE Conf. Comput.\\nVis. Pattern Recog. Workshops , pages 702–703, 2020. 3, 4\\n[12] Guneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran,\\nand Stefano Soatto. A Baseline for Few-Shot Image Classifi-\\ncation. In ICLR , page 20, 2020. 1, 2\\n[13] Mark Everingham, Luc Van Gool, Christopher KI Williams,\\nJohn Winn, and Andrew Zisserman. The pascal visual object\\nclasses (voc) challenge. International journal of computer\\nvision , 88(2):303–338, 2010. 2, 5\\n[14] Qi Fan, Wei Zhuo, Chi-Keung Tang, and Yu-Wing Tai. Few-\\nshot object detection with attention-rpn and multi-relation', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='detector. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. ,\\npages 4013–4022, 2020. 3\\n[15] Zhibo Fan, Yuchen Ma, Zeming Li, and Jian Sun. Generalized\\nfew-shot object detection without forgetting. In Proc. IEEE\\nConf. Comput. Vis. Pattern Recog. , pages 4527–4536, 2021.\\n1, 2, 3, 5, 7[16] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-\\nagnostic meta-learning for fast adaptation of deep networks.\\nInInternational Conference on Machine Learning , pages\\n1126–1135. PMLR, 2017. 1, 2\\n[17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-\\nagnostic meta-learning for fast adaptation of deep networks.\\nInProc. Int. Conf. Mach. Learn. , volume 70, pages 1126–\\n1135, 2017. 2\\n[18] Golnaz Ghiasi, Yin Cui, A. Srinivas, Rui Qian, Tsung-Yi Lin,\\nEkin Dogus Cubuk, Quoc V . Le, and Barret Zoph. Simple\\ncopy-paste is a strong data augmentation method for instance\\nsegmentation. Proc. IEEE Conf. Comput. Vis. Pattern Recog. ,\\npages 2917–2927, 2021. 1', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[19] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick\\nPérez, and Matthieu Cord. Boosting few-shot visual learning\\nwith self-supervision. In Proc. IEEE Int. Conf. on Computer\\nVision , 2019. 2\\n[20] Santiago Gonzalez and Risto Miikkulainen. Improved train-\\ning speed, accuracy, and data utilization through loss function\\noptimization. In 2020 IEEE Congress on Evolutionary Com-\\nputation (CEC) , pages 1–8. IEEE, 2020. 2, 3\\n[21] Guangxing Han, Yicheng He, Shiyuan Huang, Jiawei Ma,\\nand Shih-Fu Chang. Query adaptive few-shot object detection\\nwith heterogeneous graph convolutional networks. In Proc.\\nIEEE Int. Conf. on Computer Vision , pages 3263–3272, 2021.\\n3, 5\\n[22] Guangxing Han, Jiawei Ma, Shiyuan Huang, Long Chen, and\\nShih-Fu Chang. Few-shot object detection with fully cross-\\ntransformer. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. ,\\npages 5321–5330, 2022. 1, 2, 5\\n[23] Bharath Hariharan and Ross B. Girshick. Low-shot visual', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='recognition by shrinking and hallucinating features. Proc.\\nIEEE Int. Conf. on Computer Vision , pages 3037–3046, 2017.\\n2\\n[24] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-\\ning the knowledge in a neural network. arXiv preprint\\narXiv:1503.02531 , 2015. 4\\n[25] Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel.\\nPopulation based augmentation: Efficient learning of augmen-\\ntation policy schedules. In Proc. Int. Conf. Mach. Learn. ,\\npages 2731–2741. PMLR, 2019. 3\\n[26] Ting-I Hsieh, Yi-Chen Lo, Hwann-Tzong Chen, and Tyng-\\nLuh Liu. One-shot object detection with co-attention and\\nco-excitation. arXiv preprint arXiv:1911.12529 , 2019. 3\\n[27] Hanzhe Hu, Shuai Bai, Aoxue Li, Jinshi Cui, and Liwei Wang.\\nDense relation distillation with context-aware aggregation for\\nfew-shot object detection. In Proc. IEEE Conf. Comput. Vis.\\nPattern Recog. , pages 10185–10194, 2021. 3, 5\\n[28] Shell Xu Hu, Da Li, Jan Stühmer, Minyoung Kim, and Timo-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='thy M. Hospedales. Pushing the Limits of Simple Pipelines\\nfor Few-Shot Learning: External Data and Fine-Tuning Make\\na Difference. arXiv e-prints , page arXiv:2204.07305, Apr.\\n2022. 2\\n[29] Zeyi Huang, Yang Zou, BVK Kumar, and Dong Huang. Com-\\nprehensive attention self-distillation for weakly-supervised\\nobject detection. Proc. Adv. Neural Inf. Process. Syst. , 33,\\n2020. 1\\n7347\\n[30] Taewon Jeong and Heeyoung Kim. Ood-maml: Meta-learning\\nfor few-shot out-of-distribution detection and classification.\\nInAdvances in Neural Information Processing Systems , vol-\\nume 33, pages 3907–3916, 2020. 1\\n[31] Bingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi Feng,\\nand Trevor Darrell. Few-shot object detection via feature\\nreweighting. In Proc. IEEE Int. Conf. on Computer Vision ,\\npages 8420–8429, 2019. 1, 2, 3, 5\\n[32] Prannay Kaul, Weidi Xie, and Andrew Zisserman. Label,\\nverify, correct: A simple few shot object detection method. In\\nProc. IEEE Conf. Comput. Vis. Pattern Recog. , pages 14237–', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='14247, 2022. 1, 3, 5, 6, 7\\n[33] Michalis Lazarou, Yannis Avrithis, and Tania Stathaki. Ten-\\nsor feature hallucination for few-shot learning. ArXiv ,\\nabs/2106.05321, 2021. 2\\n[34] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and\\nStefano Soatto. Meta-learning with differentiable convex\\noptimization. Proc. IEEE Conf. Comput. Vis. Pattern Recog. ,\\npages 10649–10657, 2019. 2\\n[35] Aoxue Li and Zhenguo Li. Transformation invariant few-shot\\nobject detection. In Proc. IEEE Conf. Comput. Vis. Pattern\\nRecog. , pages 3094–3102, 2021. 3, 5\\n[36] Bohao Li, Boyu Yang, Chang Liu, Feng Liu, Rongrong Ji, and\\nQixiang Ye. Beyond max-margin: Class margin equilibrium\\nfor few-shot object detection. In Proc. IEEE Conf. Comput.\\nVis. Pattern Recog. , pages 7363–7372, 2021. 1, 2, 3, 5\\n[37] Chuming Li, Xin Yuan, Chen Lin, Minghao Guo, Wei Wu,\\nJunjie Yan, and Wanli Ouyang. Am-lfs: Automl for loss\\nfunction search. In Proc. IEEE Int. Conf. on Computer Vision ,\\npages 8410–8419, 2019. 3', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[38] Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-\\nSGD: Learning to Learn Quickly for Few-Shot Learning.\\narXiv e-prints , page arXiv:1707.09835, July 2017. 2\\n[39] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and\\nSungwoong Kim. Fast autoaugment. Proc. Adv. Neural Inf.\\nProcess. Syst. , 32, 2019. 3\\n[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\\nPietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence\\nZitnick. Microsoft coco: Common objects in context. In\\nProc. European Conf. on Computer Vision , pages 740–755.\\nSpringer, 2014. 2, 5\\n[41] Bin Liu, Yue Cao, Yutong Lin, Qi Li, Zheng Zhang, Ming-\\nsheng Long, and Han Hu. Negative margin matters: Under-\\nstanding margin in few-shot classification. arXiv preprint\\narXiv:2003.12060 , 2020. 2\\n[42] Peidong Liu, Gengwei Zhang, Bochao Wang, Hang Xu, Xi-\\naodan Liang, Yong Jiang, and Zhenguo Li. Loss function\\ndiscovery for object detection via convergence-simulation\\ndriven search. arXiv preprint arXiv:2102.04700 , 2021. 2, 3', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[43] Shichen Liu, Mingsheng Long, Jianmin Wang, and Michael I\\nJordan. Generalized Zero-Shot Learning with Deep Calibra-\\ntion Network. In NeurIPS , pages 2005–2015. 2018. 4\\n[44] Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho\\nYang, Sungju Hwang, and Yi Yang. Learning to propagate la-\\nbels: Transductive propagation network for few-shot learning.\\nInProc. Int. Conf. Learn. Represent. , 2019. 2[45] Yan Liu, Zhijie Zhang, Li Niu, Junjie Chen, and Liqing Zhang.\\nMixed supervised object detection by transferring mask prior\\nand semantic similarity. In A. Beygelzimer, Y . Dauphin, P.\\nLiang, and J. Wortman Vaughan, editors, Proc. Adv. Neural\\nInf. Process. Syst. , 2021. 1\\n[46] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\\nHierarchical vision transformer using shifted windows. Proc.\\nIEEE Int. Conf. on Computer Vision , 2021. 1\\n[47] Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, and', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Adam Trischler. Rapid adaptation with conditionally shifted\\nneurons. In ICML , 2018. 2\\n[48] Alex Nichol and John Schulman. Reptile: a scalable met-\\nalearning algorithm. arXiv: Learning , 2018. 2\\n[49] Boris N. Oreshkin, Pau Rodriguez Lopez, and Alexandre La-\\ncoste. Tadam: Task dependent adaptive metric for improved\\nfew-shot learning. In NeurIPS , 2018. 2, 4\\n[50] Matteo Papini, Andrea Battistello, and Marcello Restelli. Bal-\\nancing learning speed and stability in policy gradient via\\nadaptive exploration. In Proc. Int. Conf. on Artif. Intellig. and\\nStat., pages 1188–1199, 2020. 5\\n[51] Eunbyung Park and Junier B. Oliva. Meta-curvature. In\\nNeurIPS , 2019. 2\\n[52] Juan-Manuel Perez-Rua, Xiatian Zhu, Timothy M\\nHospedales, and Tao Xiang. Incremental few-shot object\\ndetection. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. ,\\npages 13846–13855, 2020. 1, 2, 5\\n[53] Limeng Qiao, Yuxuan Zhao, Zhiyuan Li, Xi Qiu, Jianan Wu,\\nand Chi Zhang. Defrcn: Decoupled faster r-cnn for few-shot', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='object detection. In Proc. IEEE Int. Conf. on Computer Vision ,\\npages 8681–8690, 2021. 1, 2, 3, 5, 7\\n[54] Aravind Rajeswaran, Chelsea Finn, Sham M. Kakade, and\\nSergey Levine. Meta-learning with implicit gradients. In\\nNeurIPS , 2019. 2\\n[55] Esteban Real, Chen Liang, David So, and Quoc Le. Automl-\\nzero: Evolving machine learning algorithms from scratch.\\nInProc. Int. Conf. Mach. Learn. , pages 8007–8019. PMLR,\\n2020. 3\\n[56] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\\nFaster r-cnn: Towards real-time object detection with region\\nproposal networks. Proc. Adv. Neural Inf. Process. Syst. ,\\n28:91–99, 2015. 3\\n[57] Zhongzheng Ren, Zhiding Yu, Xiaodong Yang, Ming-\\nYu Liu, Yong Jae Lee, Alexander G. Schwing, and Jan\\nKautz. Instance-aware, context-focused, and memory-\\nefficient weakly supervised object detection. In Proc. IEEE\\nConf. Comput. Vis. Pattern Recog. , 2020. 1\\n[58] Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol\\nVinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Meta-learning with latent embedding optimization. In Proc.\\nInt. Conf. Learn. Represent. , 2019. 2\\n[59] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan\\nWierstra, and Timothy Lillicrap. Meta-learning with memory-\\naugmented neural networks. In Proc. Int. Conf. Mach. Learn. ,\\nvolume 48, pages 1842–1850, 2016. 2\\n[60] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical\\nnetworks for few-shot learning. In Proc. Adv. Neural Inf.\\nProcess. Syst. , volume 30, 2017. 2\\n7348\\n[61] Bo Sun, Banghuai Li, Shengcai Cai, Ye Yuan, and Chi Zhang.\\nFsce: Few-shot object detection via contrastive proposal en-\\ncoding. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. ,\\npages 7352–7362, 2021. 1, 2, 3, 4, 5, 7\\n[62] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S.\\nTorr, and Timothy M. Hospedales. Learning to compare:\\nRelation network for few-shot learning. In Proc. IEEE Conf.\\nComput. Vis. Pattern Recog. , pages 1199–1208, 2018. 2\\n[63] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B. Tenen-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='baum, and Phillip Isola. Rethinking Few-Shot Image Clas-\\nsification: a Good Embedding Is All You Need? In Proc.\\nEuropean Conf. on Computer Vision , 2020. 1, 2\\n[64] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray\\nkavukcuoglu, and Daan Wierstra. Matching networks for\\none shot learning. In Proc. Adv. Neural Inf. Process. Syst. ,\\nvolume 29, 2016. 2\\n[65] Xin Wang, Thomas E Huang, Trevor Darrell, Joseph E Gon-\\nzalez, and Fisher Yu. Frustratingly simple few-shot object\\ndetection. arXiv preprint arXiv:2003.06957 , 2020. 1, 2, 3, 5,\\n7, 8\\n[66] Xiaobo Wang, Shuo Wang, Cheng Chi, Shifeng Zhang, and\\nTao Mei. Loss function search for face recognition. In Proc.\\nInt. Conf. Mach. Learn. , pages 10029–10038. PMLR, 2020.\\n3, 4\\n[67] Yan Wang, Wei-Lun Chao, Kilian Q. Weinberger, and Lau-\\nrens van der Maaten. Simpleshot: Revisiting nearest-\\nneighbor classification for few-shot learning. arXiv preprint\\narXiv:1911.04623 , 2019. 2\\n[68] Yu-Xiong Wang, Ross Girshick, Martial Hebert, and Bharath', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Hariharan. Low-Shot Learning from Imaginary Data.\\narXiv:1801.05401 [cs] , Jan. 2018. 2\\n[69] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Meta-\\nlearning to detect rare objects. In Proc. IEEE Int. Conf. on\\nComputer Vision , pages 9925–9934, 2019. 5\\n[70] Ronald J Williams. Simple statistical gradient-following al-\\ngorithms for connectionist reinforcement learning. Machine\\nlearning , 8(3):229–256, 1992. 4\\n[71] Aming Wu, Suqi Zhao, Cheng Deng, and Wei Liu. Gener-\\nalized and discriminative few-shot object detection via svd-\\ndictionary enhancement. Proc. Adv. Neural Inf. Process. Syst. ,\\n34:6353–6364, 2021. 3\\n[72] Jiaxi Wu, Songtao Liu, Di Huang, and Yunhong Wang. Multi-\\nscale positive sample refinement for few-shot object detection.\\nInProc. European Conf. on Computer Vision , pages 456–472.\\nSpringer, 2020. 1, 2, 3, 4, 5, 6, 7\\n[73] Xiongwei Wu, Doyen Sahoo, and Steven Hoi. Meta-rcnn:\\nMeta learning for few-shot object detection. In Proceedings', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='of the 28th ACM International Conference on Multimedia ,\\npages 1679–1687, 2020. 1\\n[74] Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-shot\\nlearning-the good, the bad and the ugly. In Proc. IEEE Conf.\\nComput. Vis. Pattern Recog. , pages 4582–4591, 2017. 5\\n[75] Yang Xiao and Renaud Marlet. Few-shot object detection\\nand viewpoint estimation for objects in the wild. In Proc. Eu-\\nropean Conf. on Computer Vision , pages 192–210. Springer,\\n2020. 1, 2, 3, 5[76] Xiaopeng Yan, Ziliang Chen, Anni Xu, Xiaoxi Wang, Xi-\\naodan Liang, and Liang Lin. Meta r-cnn: Towards general\\nsolver for instance-level low-shot learning. In Proc. IEEE Int.\\nConf. on Computer Vision , pages 9577–9586, 2019. 1, 2, 5, 7\\n[77] Huaxiu Yao, Linjun Zhang, and Chelsea Finn. Meta-learning\\nwith fewer tasks through task interpolation. In Proceeding of\\nthe 10th International Conference on Learning Representa-\\ntions , 2022. 2\\n[78] Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha. Few-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='shot learning via embedding adaptation with set-to-set func-\\ntions. Proc. IEEE Conf. Comput. Vis. Pattern Recog. , pages\\n8805–8814, 2020. 2, 4\\n[79] Li Yin, Juan M Perez-Rua, and Kevin J Liang. Sylph: A\\nhypernetwork framework for incremental few-shot object\\ndetection. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. ,\\npages 9035–9045, 2022. 1, 2, 3\\n[80] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen.\\nDeepemd: Few-shot image classification with differentiable\\nearth mover’s distance and structured classifiers. In Proc.\\nIEEE Conf. Comput. Vis. Pattern Recog. , June 2020. 2\\n[81] Gongjie Zhang, Zhipeng Luo, Kaiwen Cui, Shijian Lu, and\\nEric P Xing. Meta-detr: Image-level few-shot detection with\\ninter-class correlation exploitation. IEEE Transactions on\\nPattern Analysis and Machine Intelligence , 2022. 1, 2, 3\\n[82] Lu Zhang, Shuigeng Zhou, Jihong Guan, and Ji Zhang. Ac-\\ncurate few-shot object detection with support-query mutual\\nguidance and hybrid loss. In Proc. IEEE Conf. Comput. Vis.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Pattern Recog. , pages 14424–14432, 2021. 3\\n[83] Shan Zhang, Lei Wang, Naila Murray, and Piotr Koniusz.\\nKernelized few-shot object detection with efficient integral\\naggregation. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. ,\\npages 19207–19216, 2022. 1, 2, 5\\n[84] Weilin Zhang and Yu-Xiong Wang. Hallucination improves\\nfew-shot object detection. In Proc. IEEE Conf. Comput. Vis.\\nPattern Recog. , pages 13008–13017, 2021. 1, 2, 5, 7, 8\\n7349', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Self-Supervised Video Forensics by Audio-Visual Anomaly Detection\\nChao Feng Ziyang Chen Andrew Owens\\nUniversity of Michigan\\nAbstract\\nManipulated videos often contain subtle inconsistencies\\nbetween their visual and audio signals. We propose a video\\nforensics method, based on anomaly detection, that can\\nidentify these inconsistencies, and that can be trained solely\\nusing real, unlabeled data. We train an autoregressive model\\nto generate sequences of audio-visual features, using feature\\nsets that capture the temporal synchronization between video\\nframes and sound. At test time, we then flag videos that the\\nmodel assigns low probability. Despite being trained entirely\\non real videos, our model obtains strong performance on the\\ntask of detecting manipulated speech videos. Project site:\\nhttps://cfeng16.github.io/audio-visual-forensics.\\n1. Introduction\\nSupervised learning underlies today’s most successful\\nmethods for image and video forensics. However, the diffi-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='culty of collecting large, labeled datasets that fully capture\\nall of the possible manipulations that one might encounter\\nin the wild places significant limitations on this approach. A\\nlongstanding goal of the forensics community has been to\\ndesign methods that, instead, learn to detect manipulations\\nusing cues discovered by analyzing large amounts of real\\ndata through self-supervision [27, 47].\\nWe propose a method that identifies manipulated video\\nthrough anomaly detection . Our model learns how audio\\nand visual data temporally co-occur by training on large\\namounts of real, unlabeled video. At test time, we can then\\nflag videos that our model assigns low probability, such as\\nthose whose video and audio streams are inconsistent.\\nOne might expect that this problem could be posed as\\nsimply detecting out-of-sync examples, such as by finding\\ncases in which a speaker’s mouth does not open precisely at\\nthe onset of a spoken word. Unfortunately, videos in the wild', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='are often “naturally” misaligned due to errors in encoding or\\nrecording, such as by having a single, consistent shift by a\\nfew frames [2, 23].\\nInstead, we pose the problem as detecting anomalies in\\nwhat we call synchronization features : audio-visual features\\nInput video\\nTime (frame)Inputaudio\\nZiyang’s version(remove this text)Time delayFigure 1. Audio-visual anomaly detection. We identify fake\\nvideos by finding anomalies in their audio-visual features, using\\ngenerative models trained entirely on realvideos. In one variation\\nof our model (shown here), we use the time delay between the two\\nmodalities as our feature set, i.e., temporal misalignment between\\neach video frame and the audio stream. We learn the distribution\\nof these sequences, then flag sequences with low probability.\\nthat are designed to convey the temporal alignment between\\nvision and sound. We evaluate several feature sets, each\\nextracted from a model that has been trained to temporally', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='align audio and visual streams of a video [18, 23, 78]. In\\nFigure 1, we show one such feature set: the amount of\\ntime that each video frame appears to be temporally offset\\nfrom its corresponding sound. To detect anomalies, we fit\\nan autoregressive generative model [84, 99] to sequences\\nof synchronization features extracted from real videos, and\\nidentify low probability examples.\\nA key advantage of our formulation is that it does not\\nrequire any manipulated examples for training. It also does\\nnot require the speakers in the test set to already be present\\nin the training set. This is in contrast to previous audio-\\nvisual forensics approaches, which either require finetuning\\non datasets of manipulated video [40], or which are based\\non verifying that the speaker’s voice matches previously\\nobserved examples [25].\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the final published version of the proceedings is available on IEEE Xplore.\\n10491\\nWe evaluate our model on videos that have manipulated\\na person’s speech and face, using datasets of lip-synced and\\naudio-driven face reenactment videos, some of which are\\nalso manipulated by faceswap techniques. Our model obtains\\nstrong performance on the FakeA VCeleb [52] and KoDF [61]\\ndatasets, despite the fact that it is trained entirely on real\\nexamples obtained from other video datasets. Our model\\ngeneralizes to other spoken languages without retraining and\\nobtains robustness to a variety of postprocessing operations,\\nsuch as compression and blurring. We show through our\\nexperiments that:\\n•Video forensics can be posed as an audio-visual anomaly\\ndetection problem.\\n•Synchronization features convey information about video\\nmanipulations.\\n•Our model can successfully detect fake videos, while train-\\ning solely on real videos.\\n•Our model generalizes to many types of image postpro-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='cessing operations and to speech videos from spoken lan-\\nguages not observed during training.\\n2. Related Work\\nAudio-visual forensics. In early work, Malik and\\nFarid [71] detected audio manipulations by finding incon-\\nsistencies in reverberation. Recent work has focused on de-\\ntecting manipulated speech videos using audio-visual incon-\\nsistencies. Several approaches have directly trained audio-\\nvisual networks through supervised learning, using labels\\nindicating whether a video is manipulated [20,73]. A variety\\nof methods have recently used audio-visual self-supervision\\nfor pretraining supervised models, which are finetuned with\\n“real or fake” labels. Zeng et al. [105] used local and global\\ncontrastive learning methods to learn video features. Halias-\\nsoset al. [40] jointly solved a negative-less contrastive learn-\\ning problem [38] and a forensics task. Other work [41]\\npretrains using lip-reading data. Zhou and Lim [112] used\\naudio-visual synchronization signal implicitly, and proposed', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='a dataset for audio-visual deepfake detection1. In contrast\\nto these methods, our approach is trained entirely using real\\ndata and does not require any labels or examples of fake\\nvideos. Other work has used speaker verification [25] and\\nphoneme-viseme mismatches [7] to detect fake videos and\\nit also detects face swap manipulations, which preserve the\\nsynchronization between modalities. In contrast, our ap-\\nproach detects misaligned images and sounds and does not\\nrequire that examples from the speaker be present in the\\ntraining set.\\nAudio-visual representation learning. A variety of meth-\\nods have been proposed to learn audio-visual representa-\\ntion from videos via self-supervision. Researchers have\\nleveraged the natural semantic correspondence in the videos\\n1Their dataset is not publicly available.between frames and audio tracks [10, 75, 105] to learn multi-\\nmodal features and applied them to downstream tasks such\\nas sound localization [9, 45, 74]. Other work studies tem-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='poral synchronization between audio and visual signals\\nto learn audio-visual features [23, 58, 78], which can be\\nused for active speaker detection [8, 57, 95], source sepa-\\nration [36, 70, 110], lip reading [5, 69, 72] and so on. Our\\nmethod uses the off-the-shelf audio-visual synchronization\\nmodel to perform anomaly detection.\\nVisual face forensics. A major focus of the forensics field\\nhas been on the problem of detecting manipulated videos of\\nhuman faces. In recent years, a variety of visual face manip-\\nulation datasets are proposed, such as FaceForensics++ [87],\\nVideoForensicsHQ [34] and FFIW 10K[111]. Meanwhile,\\nmany methods are proposed to detect synthetic contents to\\nfight against their potential threats. Some work [12, 39, 63]\\nhas proposed to use hand-crafted features to capture incon-\\nsistent visual or JPEG artifacts. Other work has proposed to\\nuse deep learning to inspect specific artifacts, such as blend-\\ning [62], frequency domain [31, 35, 83], or texture [67]. A', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='variety of methods have studied the generalization between\\ndetection classifiers [16, 100].\\nAnomaly detection. A variety of methods have learned\\na distribution, then flagged unusual examples. These ex-\\namples are often considered anomalies [66, 91, 104, 113] or\\noutliers [81, 89], and are used as part of open-set recog-\\nnition [56, 106]. We formulate video forensics as the\\ntask of detecting anomalies, using a feature set that con-\\nveys information that would be hard for a forger to cre-\\nate. There have been a variety of methods proposed\\nfor learning this distribution, such as GAN discrimina-\\ntors [37,56,66,81,89,91,104,113], flow-based models [106],\\nand autoregressive models [93]. Similarly, our model is\\nbased on an autoregressive generative model [11, 84], since\\nthey have achieved strong performance at modeling com-\\nplex distributions. Other work addresses goals similar to\\nanomaly detection by creating methods that model uncer-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tainty [64] or that leverage outlier exposure [30, 44, 88].\\nSome work [13, 26, 28, 47, 50, 53, 79] has used special-\\npurpose anomaly detection methods for image/video foren-\\nsics. Other work [33, 46, 101, 108] uses supervised learning\\nto find anomalous patterns. In contrast, our method builds\\nthe likelihood function entirely on real videos and views\\nlow-probability examples as fake.\\n3. Method\\nWe formulate the problem of detecting manipulated\\nvideos as an anomaly detection problem. We model the\\ndistribution of audio-visual examples, then flag examples\\nthat have low probability. If we were to fit a model on the\\nraw data, then this would be a very challenging learning\\ntask. Instead, we learn the distribution over a feature set that\\n10492\\nAudio-visual Synchronization Model\\nAVAutoregressive PredictionTimeLikelihoodTarget features···ℒPredicted features···-1+1···-2Discretetime delaysProb.Distributionover delaysVAFeature activations\\n(a) Synchronization feature extraction (b) Anomaly detection', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Figure 2. Audio-visual anomaly detection model. (a) We extract a feature set from an audio-visual synchronization network: the number\\nof frames of delay between video frames and sound, the distribution over delays at each frame, and feature activations from the audio and\\nvisual subnetworks. (b) We train an autoregressive Transformer model to assign probabilities to synchronization features. At test time, we\\nflag low probability examples.\\nconveys subtle properties that are unlikely to be accurately\\ncaptured in manipulated video.\\n3.1. Estimating audio-visual synchronization\\nWe obtain our feautres from a network that performs\\naudio-visual synchronization [18, 23, 24, 78]. We use the\\nmodel of Chen et al. [18]. We learn a function ϕ(Vi, Aj)\\nthat indicates how likely video clip Vitemporally co-occurs\\nwith audio clip Aj. We estimate the synchronization score\\nS(i, j)of all audio-visual pairs in a temporal window:\\nS(i, j) =exp (ϕ(Vi, Aj))Pi+τ\\nk=i−τexp (ϕ(Vi, Ak)), (1)', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='where τis maximum time difference between two streams,\\nandϕ(Vi, Aj) =h(gv(Vi), ga(Aj))is calculated using late\\nfusion by a visual encoder gv, audio encoder gaand the\\nfusion module h. We also interpret S(i, j)as synchroniza-\\ntion probability. We maximize the synchronization of true\\naudio-visual pairs (Vi, Ai)using the InfoNCE loss [77]:\\nLsync=−1\\nTTX\\ni=1logS(i, i), (2)\\nfor a video of length T. We provide details about the archi-\\ntecture and training procedure in the supplement.\\nAfter training, we can use the learned model to obtain a\\nfeature set for anomaly detection. For example, we can use\\nthe rows of S, which provide a probability distribution over\\npossible alignments between video clips and audio clips.\\n3.2. Audio-visual anomaly detection\\nWe use our learned model to obtain a feature set for\\nanomaly detection. We learn the distribution of these featureson a training set of real videos. Then at test time, videos\\nwith low probability will be flagged as potential fakes. We', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='now explore two key design decisions that go into such a\\nsystem: what feature set to use, and how the distribution is\\nlearned.\\nGiven features for each frame, we learn a distribution\\npθ(x1,x2, . . . ,xN). We generally use autoregressive mod-\\nels to learn this distribution, given their success in modeling\\ncomplex distributions [14,103]. These models take the form:\\npθ(x1,x2,···,xN) =N−1Y\\ni=0pθ(xi+1|x1,···,xi).(3)\\nWe train a model ˆxi+1=fθ(x1,x2, . . . ,xi)that estimates\\nthe features of the next frame, given all of the features from\\nthe previous frames. Maximizing the log probability can be\\nposed as minimizing a per-frame loss, L:\\nL=NX\\ni=1L(ˆxi,xi). (4)\\nWe now describe different formulations of the loss func-\\ntionL, the feature representation xi. In each case, we imple-\\nment fθas a Transformer [99].\\nDiscrete time delays. We first consider a simple model\\nthat uses discrete time delay as our features, following the\\nsuccess of autoregressive models for fitting discrete data [32,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='86, 97]. Inspired by work on time delay estimation [19, 55],\\nfor every video frame, we estimate how far ahead (or behind)\\nit appears to be from the audio signal. For each frame, we\\nsetxito be the time delay with the highest probability, i.e.,\\nxi= arg maxj(S(i, j)). We then set Lto be the cross\\nentropy loss between the ground truth and predicted time\\n10493\\ndelay. This amounts to solving a categorization problem\\nwith2τ+ 1possible labels for each frame.\\nDistributions over delays. While discrete time delays are\\nstraightforward to represent in the model, they discard im-\\nportant information, such as when there is ambiguity in the\\ndelay. We, therefore, propose a model that directly predicts\\nthe entries of the time delay distribution. We set the features\\nxito be the rows of S, i.e. the probability of each possible\\ndelay, and use cross entropy loss:\\nL(ˆxi,xi) =−2τ+1X\\njxi,jlog(ˆxi,j). (5)\\nWe constrain the predictions made by our model fθto\\nsum to 1 by applying a softmax.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Audio-visual network activations. The feature activations\\nwithin the audio-visual synchronization network convey in-\\nformation about the time delay. We, therefore, ask whether\\nthese activations can be directly used as features for anomaly\\ndetection. We concatenate the representations of the visual\\nand audio subnetworks, gvandga. To provide a straight-\\nforward comparison with the time delay distribution model,\\nwe reduce the dimensionality of the features by projecting\\nthem onto the top 2τ+ 1principal components, following\\nother work in autoregressive models of features [85]. We\\nuse squared distance as our loss: L(ˆxi,xi) =∥xi−ˆxi∥2.\\n4. Results\\nWe evaluate the different variations of our model on a\\nvariety of video forensics tasks.\\n4.1. Implementation details\\nSynchronization model. Following Chen et al. [18], we\\nuse ResNet-18 2D+3D [42, 43] as the visual encoder, using\\n5 frames (25 fps) as input. The audio encoder uses VGG-\\nM [17] and extracts features from 0.2s audio clips (16kHz).', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='We fuse audio and visual data using a Transformer that has 3\\nstandard Transformer encoder blocks [99], 4 attention heads,\\nand 512 channels. We train using the cropped faces provided\\nby each dataset. Please see the supplement for details.\\nAnomaly detection model. We use a decoder-only autore-\\ngressive Transformer [32, 65, 84] to learn the distribution\\nover synchronization features. We use 2 decoder blocks [99],\\neach with 16 attention heads and 256 channels. For models\\nthat use time delay or continuous distribution, we set the\\nmaximum delay to be τ= 15 frames, resulting in the distri-\\nbution Si∈R31for each video frame. We use sequences of\\nlength N= 50 from 2.0s video.\\nHyperparameters. We resample videos to 25 fps and au-\\ndios to 16kHz. We represent audio segments as mel spectro-\\ngrams of size 21×80by short-time Fourier transform (STFT)with 80 mel filter banks, a hop length of 160, and a window\\nsize of 320. Please see more details in the supp.\\n4.2. Dataset', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='4.2. Dataset\\nWe train our model on real, unlabeled speech video, and\\nevaluate it on forensics datasets.\\nTraining datasets. We train our models on Lip Reading\\nSentences 2 (LRS2, 97k videos) [3] and Lip Reading Sen-\\ntences 3 (LRS3, 120k videos) [4]. The videos in each contain\\ntightly cropped face tracks. We divide each dataset into 3\\nsplits and train the audio-visual synchronization model and\\nthe autoregressive model on different splits.\\nEvaluation datasets. We evaluate on two video foren-\\nsics datasets, spanning several different types of manipula-\\ntions that change the speech and face of a human speaker.\\nFakeA VCeleb [52], which is derived from V oxCeleb2 [21].\\nThis dataset contains 500 real videos and 19,500 fake\\nvideos manipulated by Faceswap [59], FSGAN [76], and\\nWav2Lip [82], and fake sounds that are generated by\\nSV2TTS [48]. The examples in the dataset contain different\\ncombinations of these manipulations. We use the dataset’s', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='provided face crops. We sample 2400 videos (400 real videos\\nand 2000 fake videos) as train/val splits and 600 videos (100\\nreal videos and 500 fake videos) as test split. We note that\\nour method does not use any videos from train/val splits,\\nsince it is trained from another dataset (LRS2 or LRS3).\\nSecond, we evaluate on KoDF [61], a large-scale Korean-\\nlanguage deepfake detection dataset. It contains 62,166\\nreal videos and 175,776 fake videos, where fake videos are\\ngenerated by 6 synthesized methods: FaceSwap [1], FS-\\nGAN [76], DeepFaceLab [80], FOMM [92], ATFHP [102]\\nand Wav2Lip [82]. We extract faces by using face detec-\\ntion [107] and alignment [15].\\n4.3. Evaluation methods\\nFollowing common practice [16, 40, 41, 52, 61, 62, 83, 87,\\n100, 109], we evaluate using average precision (AP) and\\nAUC. These evaluation metrics are widely used for cross-\\ndataset generalization and unsupervised models since they\\navoid the need to threshold the predictions. We compare our', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='approach to both supervised and self-supervised methods at\\nthe video level. Unless otherwise stated, we use time delay\\ndistributions as our feature set (Sec. 3.2).\\nSupervised methods. For supervised methods, we re-\\ntrain several state-of-the-art detectors on FakeA VCeleb [52]:\\n1)Xception [87]: a popular baseline for forensics detec-\\ntion; 2) LipForensics [41]: a detector is built on high-level\\nsemantic embeddings of mouth and targets irregularities in\\nmouth movements; 3) AD DFD [112]: a multimodal detector\\nwith audio and video branches, utilizes audio-visual synchro-\\nnization signal implicitly for detection; 4) FTCN [109]: a\\n10494\\nPretrained\\ndatasetCategory\\nMethod Modality RVFA FVRA-WL FVFA-FS FVFA-GAN FVFA-WL A VG-FV\\nAP AUC AP AUC AP AUC AP AUC AP AUC AP AUCSupervisedXception [87] V ImageNet [29] – – 88.2 88.3 92.3 93.5 67.6 68.5 91.0 91.0 84.8 85.3\\nLipForensics [41] V LRW [22] – – 97.8 97.7 99.9 99.9 61.5 68.1 98.6 98.7 89.4 91.1', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='AD DFD [112] AV Kinetics [51] 74.9 73.3 97.0 97.4 99.6 99.7 58.4 55.4 100. 100. 88.8 88.1\\nFTCN [109] V – – – 96.2 97.4 100. 100. 77.4 78.3 95.6 96.5 92.3 93.1\\nRealForensics [40] V LRW [22] – – 88.8 93.0 99.3 99.1 99.8 99.8 93.4 96.7 95.3 97.1UnsupervisedA VBYOL [38, 40] AV LRW [22] 50.0 50.0 73.4 61.3 88.7 80.8 60.2 33.8 73.2 61.0 73.9 59.2\\nVQ-GAN [32] V LRS2 [3] – – 50.3 49.3 57.5 53.0 49.6 48.0 62.4 56.9 55.0 51.8\\nOurs AV LRS2 [3] 62.4 71.6 93.6 93.7 95.3 95.8 94.1 94.3 93.8 94.1 94.2 94.5\\nOurs AV LRS3 [4] 70.7 80.5 91.1 93.0 91.0 92.3 91.6 92.7 91.4 93.1 91.3 92.8\\nTable 1. Manipulation detection on FakeA VCeleb. We report AP scores (%) and AUC scores (%), following the evaluation protocol of\\nHaliassos et al. [41], in which supervised methods are evaluated on unseen manipulation types (unsupervised methods are not trained with\\nlabels and fake examples). We report results with combinations of real/fake video/audio, using different manipulation algorithms. We report', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the average performance over four fake video (FV) categories in A VG-FV . We retrained all supervised models on FakeA VCeleb [52].\\nvideo forensics detector leverages temporal incoherence to\\nboost generalization capability; 5) RealForensics [40]: it\\nfirst pretrains the network by audio-visual BYOL [38] frame-\\nwork and then finetunes the pretrained model on forensics\\ndatasets by multi-task learning to obtain robust and general\\nface forgery detection.\\nSelf-supervised methods. Since we are not aware of any\\nexisting methods that consider self-supervised speech video\\nforensics, we adapt two existing methods to the task. First,\\nwe consider an audio-visual contrastive learning model,\\nwhich we call A VBYOL , that learns to determine whether\\nthe visual and audio streams of a video do (or do not) match,\\nan approach that has been used as a part of other audio-visual\\nforensics models [25, 40]. We adapt the model of Haliassos\\net al. [40], which uses BYOL [38] to learn a joint audio-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='visual embedding for pretraining. Instead of pretraining,\\nwe directly use the model’s audio-visual similarity score\\nto flag fake examples. Second, we use a generative model\\nVQGAN [32], trained on LRS2 [3], for anomaly detection.\\nVQGAN converts an image into a sequence of discrete codes,\\nthen uses an autoregressive Transformer to learn the distri-\\nbution of codes. We use the code’s log likelihood, averaged\\nover each video frame, for anomaly detection.\\n4.4. Evaluation\\nIn real-world scenarios, the deployed detectors are ex-\\npected to recognize fake videos manipulated by unseen\\ntechniques. Thus, following the standard procedure used\\nin [40, 41, 109, 112], we conduct the experiment to eval-\\nuate the cross-manipulation generalization ability of our\\nmodel on the FakeA VCeleb dataset [52] which videos are\\nmanipulated in various ways. Since our approach and other\\nself-supervised baselines learn from real, unmanipulated\\nvideos and perform zero-shot fake video detection, all the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='fake videos during evaluation are considered as manipulated\\nby unseen methods.We split FakeA VCeleb dataset [52] into five categories\\nbased on the manipulation methods and manipulated modal-\\nities: 1) RVFA : real video with fake audio by SV2TTS [48];\\n2)FVRA-WL : real audio with fake video by Wav2Lip [82];\\n3)FVFA-WL : fake video by Wav2Lip [82], and fake audio\\nby SV2TTS [48]; 4) FVFA-FS : fake video by Faceswap [59]\\nand Wav2Lip [82], and fake audio by SV2TTS [48];\\n5)FVFA-GAN : fake video by FaceswapGAN [76] and\\nWav2Lip [82], and fake audio by SV2TTS [48]. For su-\\npervised methods, we hold out the evaluated category and\\ntrain the models on the remaining categories. Note that some\\napproaches are only able to detect the manipulation on a\\ncertain modality, we do not report their performance on the\\ncategories with the manipulation only on the other modalities\\n(since they can not differentiate real and fake videos).\\nWe show our results in Tab. 1. Our method substantially', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='outperforms both self-supervised methods A VBYOL [38,40]\\nand VQGAN [32] on each category by a large margin. More\\nimportantly, our method works on par with or outperforms\\nsome supervised methods on certain categories, especially\\nFVFA-GAN, even though our method does not use any la-\\nbeled supervision or fake examples. Moreover, our method\\nhas quite consistent performances and it can achieve AP over\\n90% in the most of categories. While Xception [87], Lip-\\nForensics [41], AD DFD [112] and FTCN [109] work well\\non 75% of the settings, there are settings where performance\\ncollapses to near-chance ( e.g., AD DFD [112] on FVFA-\\nGAN). Interestingly, two self-supervised baselines struggle\\nto detect fake videos, perhaps because both models do not\\nnecessarily capture the subtle information that would be\\nneeded to detect manipulations. In addition, VQGAN [32]\\ncompresses the visual signal using a codebook, which might\\ndrop the artifact clues and harm the detection performance.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Moreover, our model trained on LRS2 [3] works on par with\\nthe one trained on LRS3 [4], indicating that our method’s\\nperformance is not tied to a single training set.\\n10495\\nMethod ModalityKoDF [61]\\nAP AUC\\nSupervised\\n(transfer)Xception [87] V 76.9 77.7\\nLipForensics [41] V 89.5 86.6\\nAD DFD [112] AV 79.6 82.1\\nFTCN [109] V 66.8 68.1\\nRealForensics [40] V 95.7 93.6\\nUnsupervisedA VBYOL [38, 40] AV 74.9 78.9\\nVQ-GAN [32] V 46.8 45.5\\nOurs AV 87.6 86.9\\nTable 2. Generalization to Korean speech. AP scores ( %) and\\nAUC scores ( %) are reported on KoDF dataset [61]. Supervised\\nmethods are trained on FakeA VCeleb dataset [52]. Ours is trained\\non LRS2 [3]. Best results are in bold .\\nCross-dataset generalization. We also evaluate the gen-\\neralization capability of our model by evaluating it on the\\nKoDF dataset [61], following [40, 41, 109, 112]. We focus\\non the audio-driven synthesis examples in the dataset, where\\nvideos are manipulated by ATFHP [102] or Wav2Lip [82],', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and randomly selected 100 real and 100 fake videos. We\\ntrain the supervised models on FakeA VCeleb [52] to evaluate\\ntheir generalization ability. Many of these training videos\\nshare the same technique used in KoDF for synthesis [82].\\nAs the results are shown in Tab. 2, our approach obtains a\\ncomparable performance to many supervised methods. Al-\\nthough our system is trained on the English speech datasets,\\nit still generalizes to KoDF [61] dataset of Korean speech,\\nperhaps because it learns low-level lip motion cues that are\\nbroadly useful. We provide more results in the supplement.\\nQualitative results. We visualize the ground truth and\\npredicted time delay distributions generated by our autore-\\ngressive continuous time delay model (Sec. 3.2) in Fig. 3. We\\nuse the four main categories from FakeA VCeleb dataset [52].\\nFor each one, we display a heat map indicating the predicted\\ntime delay distribution, using a model that obtains the ground', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='truth distributions of the previous frames as input. We also\\nplot the cumulative prediction loss (Eq. 4) over time. From\\nFig. 3, we can see that our autoregressive model accurately\\npredicts the ground truth for real video, which results in a\\nlower score. For fake videos, we can find clear differences\\nbetween ground truth and predicted time delay distribution,\\nleading to higher prediction loss.\\n4.5. Robustness to unseen perturbations\\nWhen the fake video is redistributed, it may undergo many\\ntypes of postprocessing that result in corruption, making de-\\ntection more difficult. Thus, it is important for forensics\\nmodels to be robust to the types of postprocessing operations\\nthey may encounter in the wild. Following [40, 41, 109], we\\nuse the set of visual perturbations proposed in [49]: 1) Color\\nsaturation change; 2) Block-wise distortion; 3) Color con-\\nFigure 3. Time delay distribution predictions for real vs. fake\\nexamples. We visualize the time delay distributions from the syn-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='chronization model and predicted results generated by the autore-\\ngressive model for four random samples from different categories.\\nSynchronization probabilities are in a range from 0\\n 1. We\\nshow the predictions of the autoregressive model when feeding it\\nground truth observations of the previous timesteps. We show cu-\\nmulative prediction error (indicating the probability of being fake)\\nfor each sample over time steps in the last row.\\ntrast change; 4) Gaussian blur; 5) Gaussian noise; 6) JPEG\\ncompression; 7) Video compression rate change. We set the\\nintensity levels from 0 to 5 for each perturbation.\\nWe compare our model with four supervised methods\\nXceptionNet [87] FTCN [112], AD DFD [112] and Real-\\nForensics [40]. As shown in Fig. 4, our self-supervised\\nmodel is overall more robust to unseen visual perturbations\\non average compared with these supervised methods, with\\nthe exception of RealForensics [40]. This is also true when\\nwe consider “worst case” performance, by taking the mini-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='mum performance over all types of augmentation of a given\\nintensity level. Interestingly, we obtain this performance\\neven though our model is trained in a very different way\\nfrom other works, suggesting that the feature set continues\\nto convey useful information to the anomaly detection model,\\neven in the presence of significant corruption.\\n4.6. Feature set analysis\\nWe evaluate the effectiveness of different feature sets used\\nby our anomaly detection model. As described in Sec. 3.2,\\n10496\\n5060708090100AUC (%)\\nSaturation\\n JPEG\\n Block-wise\\n Gaussian Noise\\n Gaussian Blur\\n0 2 4\\nIntensity5060708090100AUC (%)\\nCompression\\n0 2 4\\nIntensity\\nContrast\\n0 2 4\\nIntensity\\nAverage\\n0 2 4\\nIntensity\\nRival\\nChance\\nFTCN\\nXception\\nAD DFD\\nRealForensics\\nOursFigure 4. Robustness to unseen perturbations. AUC scores (%) of different detectors as a function of perturbation intensities. There are 6', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='intensity levels in total from [49]. “Average” represents the average over 7 perturbations under each intensity. “Rival” means we pick the\\nworst performance across 7 perturbations under each intensity.\\nwe start with discrete time delays as our feature represen-\\ntation and optimize the model with the cross entropy loss.\\nThen, we use continuous time delay distributions as repre-\\nsentations instead, where we optimize models with different\\nobjective functions: 1) Soft CE: we use the time delay distri-\\nbution as the target (akin to a “soft” label) and use the cross\\nentropy loss (Sec. 3.2); 2) CE: we map each distribution into\\none-hot encoding as the target by using arg max and employ\\nthe cross entropy loss; 3) BCE: we use the distribution as\\nthe target while treating each synchronization score S(i, j)\\n(Sec. 3.1) within the same time step independently. We use\\nthe sigmoid function and binary cross entropy loss to train\\nthe model. We also use our network’s feature activations', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='as in Sec. 3.2: 1) audio-visual feature activations (activation-\\nA V); 2) visual-only feature activations (activation-V). Be-\\nsides, we consider using a combination of different feature\\nsets where we concatenate continuous time delay distribu-\\ntions andaudio-visual feature activations (Act.-A V +dist.)\\nas a new feature. Similar to audio-visual feature activations\\nas in Sec. 3.2, we use squared distance as the loss for the\\nconcatenation of these two types of feature sets.\\nWe also compare with a simple model based on Naive\\nBayes and discrete time delays. This model assumes that\\neach frame’s time delay is independent, and obtains a proba-\\nbility for the entire sequence by multiplying the probability\\nof each frame’s time delay. This amounts to simply detecting\\nlarge misalignments since in practice the Naive Bayes model\\nwill assign probability solely based on the magnitude of each\\ndelay.\\nFinally, we consider a version of the model that autore-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='gressively predicts the entire distribution of time delays,\\ninspired by autoregressive models, such as PixelCNN [96]\\nthat generate images in a raster scan order. We autoregres-\\nsively predict each element of the 2D matrix ˆS(i, j), whereModel Feature set LA VG-ALL A VG-FV\\nAP AUC AP AUC\\nBayes - - 73.1 85.1 72.4 86.0\\nOursdiscrete delay CE 80.8 86.5 80.0 86.6\\ndistribution CE 84.8 87.9 90.3 92.2\\ndistribution BCE 78.6 83.4 80.5 84.8\\ndistribution Soft CE 87.8 90.0 94.2 94.5\\nactivation-A V MSE 86.5 87.1 91.5 91.9\\nAct.-A V +dist. MSE 85.5 87.0 90.0 91.3\\nactivation-V MSE – – 77.6 85.9\\ndiscrete prob. – 83.4 86.9 88.6 91.1\\nTable 3. Feature set analysis. AP (%) and AUC ( %) are reported\\non FakeA VCeleb [52] when using different feature sets. Best results\\nare in bold. A VG-ALL means the average over all categories. A VG-\\nFV represents the average over four fake video categories.\\nˆS(i, j)is created by vector quantizing the entries of the syn-\\nchronization probability S(i, j)using k-means (see supp.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='for details).\\nAnalysis. We evaluate each variant on FakeA VCeleb [52]\\nand report results in Tab. 3. These results suggest that\\nall formulations achieve performance significantly better\\nthan chance, indicating that these feature sets are useful\\nfor anomaly detection. As in Tab. 3, the time delay distri-\\nbution model outperforms the discrete time delay model,\\nsuggesting that there is important information conveyed in\\nthe probability of unlikely delays. The autoregressive model\\nthat uses distribution as input and soft labels (soft CE) per-\\nforms best since it forces the output prediction to match\\nthe distribution from the synchronization model. Interest-\\ningly, the model that uses audio-visual feature activations\\nobtains performance close to that of the soft CE model, indi-\\ncating that the networks’ audio-visual features convey useful\\n10497\\nSynchronization\\ndatasetAuto-regression\\ndatasetA VG\\nAP AUC\\nLRS2 [3]LRS2 [3] 87.8 90.0\\nLRS3 [4] 85.0 89.6\\nLRS2 [3] +LRS3 [4] 85.1 89.9', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='LRS3 [4]LRS2 [3] 86.6 89.0\\nLRS3 [4] 87.2 90.3\\nLRS2 [3] +LRS3 [4] 87.2 90.6\\nTable 4. Dataset ablation. AP scores ( %) and AUC scores ( %) are\\nreported on FakeA VCeleb [52] dataset by using different datasets to\\ntrain synchronization model or atuoregressive model. Best results\\nare in bold .\\ninformation. Finally, the multimodal activation-A V model\\nsignificantly outperforms the visual-only activation-V model,\\nsuggesting that having access to both modalities is useful for\\nour anomaly detection model.\\n4.7. Ablation study\\nDifferent training dataset. We ask how the choice of\\ndataset affects the quality of the model. To test this, we train\\nour synchronization and autoregressive models on different\\ndatasets to analyze the generalization abilities of each compo-\\nnent, i.e., training the synchronization model on LRS2/LRS3\\nand training the autoregressive model on LRS3/LRS2 or\\nLRS2 +LRS3 with the same hyperparameters. As shown\\nin Tab. 4, there is no significant performance change when', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='we train these two components on different combinations of\\ndatasets, including when they are trained on the same dataset.\\nThis suggests that the distribution of time delay predictions\\nmay be stable between these speech video datasets.\\nInfluence of sequence length. To explore the influence of\\ninput sequence length for the autoregressive model, we sam-\\nple the same amount of training videos for sequence lengths\\nNof 10, 20, 30, 40, 50, and 60, and keep other hyperpa-\\nrameters the same. We test these models on FakeA VCeleb\\ndataset [52]. Fig. 5 shows that as the sequence length in-\\ncreases, the performance increases with it.\\nEffect of time delay distribution maximum offset. We\\nalso study how the length of time delay distribution would\\naffect the performance of the autoregressive model with dis-\\ntribution over delays. We experiment with maximum offset\\nτ∈ {5,10,15,20,25}resulting in the delay distribution\\nlength of {11,21,31,41,51}. We test these models on the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='FakeA VCeleb dataset [52]. Fig. 5 shows that as distribution\\nlength increases, the performance first increases, after which\\npoint results plateau or slightly decrease. This may be due\\nto the fact that when considering larger ranges of offsets, the\\ndistribution spreads over a large number of unlikely possi-\\nbilities, making important information less apparent after\\nnormalization.\\n102030405060\\nSequence length74778083868992Average\\nAP\\nAUC\\n11 21 31 41 51\\nDistribution length80828486889092\\nFigure 5. Hyperparameter ablation. We evaluate with different in-\\nput sequence lengths for our autoregressive model on FakeA VCeleb\\n(left), and study the effect of the time delay distribution’s maximum\\ntemporal offset (right).\\n5. Conclusion\\nWe have proposed a method for detecting video manip-\\nulation by self-supervised anomaly detection. To do this,\\nwe create novel feature sets that convey audio-visual syn-\\nchronization. We then show that fake videos can be detected', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='by flagging examples with unlikely sequences of these fea-\\ntures, according to a learned distribution. Our model obtains\\nstrong performance on the FakeA VCeleb and KoDF datasets,\\ndespite the fact that it was trained only on real video. It also\\nobtains robustness to visual postprocessing operations and to\\nvideos containing other spoken languages. We see our work\\nas opening in two directions. The first is in posing forensics\\nas an anomaly detection problem with a self-supervised fea-\\nture set. While we have proposed one such model, based\\non autoregressive sequence models, the field of anomaly de-\\ntection offers many possible future approaches. The second\\ndirection is in developing new feature sets that are well-\\nsuited to forensics problems, beyond the synchronization\\nfeatures used in this work.\\nLimitations and Broader Impacts. Our work provides\\nmethods that can potentially be applied to detecting mali-\\ncious video manipulations and disinformation. While we', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='have shown that our model is capable of detecting several\\ntypes of fake video, there may be other techniques that our\\nmodel fails to detect. In particular, due to the design of\\nour use of synchronization-based features, our model is not\\nwell suited to detecting manipulations that leave the synchro-\\nnization between motion and sound relatively consistent,\\nsuch as those that change a speaker’s appearance without\\nsignificantly changing the motion of their mouth.\\nAcknowledgements. We thank David Fouhey, Richard\\nHiggins, Sarah Jabbour, Yuexi Du, Mandela Patrick, Deva\\nRamanan, Haochen Wang, and Aayush Bansal for helpful\\ndiscussions. This work was supported in part by DARPA\\nSemafor. The views, opinions and/or findings expressed\\nare those of the authors and should not be interpreted as\\nrepresenting the official views or policies of the Department\\nof Defense or the U.S. Government.\\n10498\\nReferences\\n[1] Faceswap. https://github.com/deepfakes/faceswap, 2022. 4', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[2]Triantafyllos Afouras, Yuki M. Asano, Francois Fagan, An-\\ndrea Vedaldi, and Florian Metze. Self-supervised object\\ndetection from audio-visual correspondence, 2021. 1\\n[3]T. Afouras, J. S. Chung, A. Senior, O. Vinyals, and A.\\nZisserman. Deep audio-visual speech recognition. In\\narXiv:1809.02108 , 2018. 4, 5, 6, 8, 1\\n[4]Triantafyllos Afouras, Joon Son Chung, and Andrew Zis-\\nserman. Lrs3-ted: a large-scale dataset for visual speech\\nrecognition. arXiv preprint arXiv:1809.00496 , 2018. 4, 5, 8\\n[5]Triantafyllos Afouras, Joon Son Chung, and Andrew Zisser-\\nman. Asr is all you need: Cross-modal distillation for lip\\nreading. In ICASSP 2020-2020 IEEE International Confer-\\nence on Acoustics, Speech and Signal Processing (ICASSP) ,\\npages 2143–2147. IEEE, 2020. 2\\n[6]Triantafyllos Afouras, Andrew Owens, Joon Son Chung, and\\nAndrew Zisserman. Self-supervised learning of audio-visual\\nobjects from video. In European Conference on Computer\\nVision , 2020. 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Vision , 2020. 2\\n[7]Shruti Agarwal, Hany Farid, Ohad Fried, and Maneesh\\nAgrawala. Detecting deep-fake videos from phoneme-\\nviseme mismatches. In Proceedings of the IEEE/CVF confer-\\nence on computer vision and pattern recognition workshops ,\\npages 660–661, 2020. 2\\n[8]Juan Le ´on Alc ´azar, Fabian Caba, Ali K Thabet, and Bernard\\nGhanem. Maas: Multi-modal assignation for active speaker\\ndetection. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision , pages 265–274, 2021. 2\\n[9]Relja Arandjelovic and Andrew Zisserman. Objects that\\nsound. In Proceedings of the European conference on com-\\nputer vision (ECCV) , pages 435–451, 2018. 2\\n[10] Yuki Asano, Mandela Patrick, Christian Rupprecht, and An-\\ndrea Vedaldi. Labelling unlabelled videos from scratch with\\nmulti-modal self-supervision. Advances in Neural Informa-\\ntion Processing Systems , 33:4660–4671, 2020. 2\\n[11] Yoshua Bengio, R ´ejean Ducharme, and Pascal Vincent. A', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='neural probabilistic language model. Advances in neural\\ninformation processing systems , 13, 2000. 2\\n[12] Tiziano Bianchi and Alessandro Piva. Image forgery\\nlocalization via block-grained analysis of jpeg artifacts.\\nIEEE Transactions on Information Forensics and Security ,\\n7(3):1003–1017, 2012. 2\\n[13] Luca Bondi, Silvia Lameri, David Guera, Paolo Bestagini,\\nEdward J Delp, Stefano Tubaro, et al. Tampering detection\\nand localization through clustering of camera-based cnn\\nfeatures. In CVPR Workshops , volume 2, 2017. 2\\n[14] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,\\nJared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\\nguage models are few-shot learners. Advances in neural\\ninformation processing systems , 33:1877–1901, 2020. 3\\n[15] Adrian Bulat and Georgios Tzimiropoulos. How far are we\\nfrom solving the 2d & 3d face alignment problem? (and a\\ndataset of 230,000 3d facial landmarks). In International', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Conference on Computer Vision , 2017. 4[16] Lucy Chai, David Bau, Ser-Nam Lim, and Phillip Isola.\\nWhat makes fake images detectable? understanding proper-\\nties that generalize. In European conference on computer\\nvision , pages 103–120. Springer, 2020. 2, 4\\n[17] Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and\\nAndrew Zisserman. Return of the devil in the details:\\nDelving deep into convolutional nets. arXiv preprint\\narXiv:1405.3531 , 2014. 4\\n[18] H Chen, W Xie, T Afouras, A Nagrani, A Vedaldi, and A\\nZisserman. Audio-visual synchronisation in the wild. In\\nProceedings of the 32nd British Machine Vision Conference .\\nBritish Machine Vision Association, 2021. 1, 3, 4, 2\\n[19] Ziyang Chen, David F Fouhey, and Andrew Owens. Sound\\nlocalization by self-supervised time delay estimation. In\\nComputer Vision–ECCV 2022: 17th European Conference,\\nTel Aviv, Israel, October 23–27, 2022, Proceedings, Part\\nXXVI , pages 489–508. Springer, 2022. 3\\n[20] Komal Chugh, Parul Gupta, Abhinav Dhall, and Ra-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='manathan Subramanian. Not made for each other-audio-\\nvisual dissonance-based deepfake detection and localization.\\nInProceedings of the 28th ACM international conference\\non multimedia , pages 439–447, 2020. 2\\n[21] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman.\\nV oxceleb2: Deep speaker recognition. Proc. Interspeech\\n2018 , pages 1086–1090, 2018. 4\\n[22] J. S. Chung and A. Zisserman. Lip reading in the wild. In\\nAsian Conference on Computer Vision , 2016. 5\\n[23] Joon Son Chung and Andrew Zisserman. Out of time: auto-\\nmated lip sync in the wild. In Asian conference on computer\\nvision , pages 251–263. Springer, 2016. 1, 2, 3\\n[24] Soo-Whan Chung, Joon Son Chung, and Hong-Goo Kang.\\nPerfect match: Improved cross-modal embeddings for audio-\\nvisual synchronisation. In ICASSP 2019-2019 IEEE In-\\nternational Conference on Acoustics, Speech and Signal\\nProcessing (ICASSP) , pages 3965–3969. IEEE, 2019. 3\\n[25] Davide Cozzolino, Matthias Nießner, and Luisa Verdoliva.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Audio-visual person-of-interest deepfake detection. arXiv\\npreprint arXiv:2204.03083 , 2022. 1, 2, 5\\n[26] Davide Cozzolino and Luisa Verdoliva. Single-image splic-\\ning localization through autoencoder-based anomaly detec-\\ntion. In 2016 IEEE international workshop on information\\nforensics and security (WIFS) , pages 1–6. IEEE, 2016. 2\\n[27] Davide Cozzolino and Luisa Verdoliva. Noiseprint: a cnn-\\nbased camera model fingerprint. IEEE Transactions on\\nInformation Forensics and Security , 15:144–159, 2019. 1\\n[28] Dario D’Avino, Davide Cozzolino, Giovanni Poggi, and\\nLuisa Verdoliva. Autoencoder with recurrent neural\\nnetworks for video forgery detection. arXiv preprint\\narXiv:1708.08754 , 2017. 2\\n[29] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\\ndatabase. In 2009 IEEE conference on computer vision and\\npattern recognition , pages 248–255. Ieee, 2009. 5\\n[30] Akshay Raj Dhamija, Manuel G ¨unther, and Terrance Boult.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Reducing network agnostophobia. Advances in Neural In-\\nformation Processing Systems , 31, 2018. 2\\n10499\\n[31] Ricard Durall, Margret Keuper, and Janis Keuper. Watch\\nyour up-convolution: Cnn based generative deep neural net-\\nworks are failing to reproduce spectral distributions. In Pro-\\nceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition , pages 7890–7899, 2020. 2\\n[32] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\\ntransformers for high-resolution image synthesis. In Pro-\\nceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition , pages 12873–12883, 2021. 3, 4, 5,\\n6, 1\\n[33] Jianwei Fei, Yunshu Dai, Peipeng Yu, Tianrun Shen, Zhihua\\nXia, and Jian Weng. Learning second order local anomaly\\nfor general face forgery detection. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 20270–20280, 2022. 2\\n[34] Gereon Fox, Wentao Liu, Hyeongwoo Kim, Hans-Peter', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Seidel, Mohamed Elgharib, and Christian Theobalt. Video-\\nforensicshq: Detecting high-quality manipulated face videos.\\nIn2021 IEEE International Conference on Multimedia and\\nExpo (ICME) , pages 1–6. IEEE, 2021. 2\\n[35] Joel Frank, Thorsten Eisenhofer, Lea Sch ¨onherr, Asja Fis-\\ncher, Dorothea Kolossa, and Thorsten Holz. Leveraging\\nfrequency analysis for deep fake image recognition. In Inter-\\nnational conference on machine learning , pages 3247–3258.\\nPMLR, 2020. 2\\n[36] Ruohan Gao and Kristen Grauman. Visualvoice: Audio-\\nvisual speech separation with cross-modal consistency. In\\n2021 IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition (CVPR) , pages 15490–15500. IEEE, 2021.\\n2\\n[37] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville,\\nand Yoshua Bengio. Generative adversarial nets. In Z.\\nGhahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q.\\nWeinberger, editors, Advances in Neural Information Pro-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='cessing Systems , volume 27. Curran Associates, Inc., 2014.\\n2\\n[38] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin\\nTallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch,\\nBernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-\\nlaghi Azar, et al. Bootstrap your own latent-a new approach\\nto self-supervised learning. Advances in neural information\\nprocessing systems , 33:21271–21284, 2020. 2, 5, 6, 1\\n[39] Hui Guo, Shu Hu, Xin Wang, Ming-Ching Chang, and Si-\\nwei Lyu. Eyes tell all: Irregular pupil shapes reveal gan-\\ngenerated faces. In ICASSP 2022-2022 IEEE International\\nConference on Acoustics, Speech and Signal Processing\\n(ICASSP) , pages 2904–2908. IEEE, 2022. 2\\n[40] Alexandros Haliassos, Rodrigo Mira, Stavros Petridis,\\nand Maja Pantic. Leveraging real talking faces via self-\\nsupervision for robust forgery detection. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 14950–14962, 2022. 1, 2, 4, 5, 6', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[41] Alexandros Haliassos, Konstantinos V ougioukas, Stavros\\nPetridis, and Maja Pantic. Lips don’t lie: A generalisable and\\nrobust approach to face forgery detection. In Proceedings of\\nthe IEEE/CVF conference on computer vision and pattern\\nrecognition , pages 5039–5049, 2021. 2, 4, 5, 6, 1[42] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can\\nspatiotemporal 3d cnns retrace the history of 2d cnns and\\nimagenet? In Proceedings of the IEEE conference on Com-\\nputer Vision and Pattern Recognition , pages 6546–6555,\\n2018. 4\\n[43] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In Proceed-\\nings of the IEEE conference on computer vision and pattern\\nrecognition , pages 770–778, 2016. 4\\n[44] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich.\\nDeep anomaly detection with outlier exposure. arXiv\\npreprint arXiv:1812.04606 , 2018. 2\\n[45] Xixi Hu, Ziyang Chen, and Andrew Owens. Mix and local-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ize: Localizing sound sources in mixtures. Computer Vision\\nand Pattern Recognition (CVPR) , 2022. 2\\n[46] Ziheng Hu, Hongtao Xie, Yuxin Wang, Jiahong Li,\\nZhongyuan Wang, and Yongdong Zhang. Dynamic\\ninconsistency-aware deepfake video detection. In IJCAI ,\\n2021. 2\\n[47] Minyoung Huh, Andrew Liu, Andrew Owens, and Alexei A\\nEfros. Fighting fake news: Image splice detection via\\nlearned self-consistency. European Conference on Com-\\nputer Vision (ECCV) , 2018. 1, 2\\n[48] Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan\\nShen, Fei Ren, Patrick Nguyen, Ruoming Pang, Ignacio\\nLopez Moreno, Yonghui Wu, et al. Transfer learning from\\nspeaker verification to multispeaker text-to-speech synthe-\\nsis.Advances in neural information processing systems , 31,\\n2018. 4, 5\\n[49] Liming Jiang, Ren Li, Wayne Wu, Chen Qian, and\\nChen Change Loy. Deeperforensics-1.0: A large-scale\\ndataset for real-world face forgery detection. In Proceedings\\nof the IEEE/CVF conference on computer vision and pattern', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='recognition , pages 2889–2898, 2020. 6, 7, 2\\n[50] Sri Kalyan Yarlagadda, David G ¨uera, Paolo Bestagini,\\nFengqing Maggie Zhu, Stefano Tubaro, and Edward J Delp.\\nSatellite image forgery detection and localization using gan\\nand one-class classifier. arXiv e-prints , pages arXiv–1802,\\n2018. 2\\n[51] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-\\nman action video dataset. arXiv preprint arXiv:1705.06950 ,\\n2017. 5\\n[52] Hasam Khalid, Shahroz Tariq, Minha Kim, and Simon Woo.\\nFakeavceleb: A novel audio-video multimodal deepfake\\ndataset. In J. Vanschoren and S. Yeung, editors, Proceed-\\nings of the Neural Information Processing Systems Track on\\nDatasets and Benchmarks , volume 1, 2021. 2, 4, 5, 6, 7, 8,\\n1, 3\\n[53] Hasam Khalid and Simon S Woo. Oc-fakedect: Classifying\\ndeepfakes using one-class variational autoencoder. In Pro-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition workshops , pages 656–657, 2020. 2\\n[54] Diederik P Kingma and Jimmy Ba. Adam: A method for\\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\\n2014. 2\\n10500\\n[55] Charles Knapp and Glifford Carter. The generalized correla-\\ntion method for estimation of time delay. IEEE transactions\\non acoustics, speech, and signal processing , 1976. 3\\n[56] Shu Kong and Deva Ramanan. Opengan: Open-set recog-\\nnition via open data generation. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision ,\\npages 813–822, 2021. 2\\n[57] Okan K ¨op¨ukl¨u, Maja Taseska, and Gerhard Rigoll. How\\nto design a three-stage architecture for audio-visual ac-\\ntive speaker detection in the wild. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision ,\\npages 1193–1203, 2021. 2\\n[58] Bruno Korbar, Du Tran, and Lorenzo Torresani. Cooperative\\nlearning of audio and video models from self-supervised syn-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='chronization. Advances in Neural Information Processing\\nSystems , 31, 2018. 2\\n[59] Iryna Korshunova, Wenzhe Shi, Joni Dambre, and Lucas\\nTheis. Fast face-swap using convolutional neural networks.\\nInProceedings of the IEEE international conference on\\ncomputer vision , pages 3677–3685, 2017. 4, 5\\n[60] Prajwal KR, Rudrabha Mukhopadhyay, Jerin Philip, Ab-\\nhishek Jha, Vinay Namboodiri, and CV Jawahar. Towards\\nautomatic face-to-face translation. In Proceedings of the\\n27th ACM international conference on multimedia , pages\\n1428–1436, 2019. 1\\n[61] Patrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo\\nPark, and Gyeongsu Chae. Kodf: A large-scale korean\\ndeepfake detection dataset. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision , pages 10744–\\n10753, 2021. 2, 4, 6\\n[62] Lingzhi Li, Jianmin Bao, Ting Zhang, Hao Yang, Dong\\nChen, Fang Wen, and Baining Guo. Face x-ray for more gen-\\neral face forgery detection. In Proceedings of the IEEE/CVF', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='conference on computer vision and pattern recognition ,\\npages 5001–5010, 2020. 2, 4\\n[63] Yuezun Li and Siwei Lyu. Exposing deepfake videos\\nby detecting face warping artifacts. arXiv preprint\\narXiv:1811.00656 , 2018. 2\\n[64] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhanc-\\ning the reliability of out-of-distribution image detection in\\nneural networks. arXiv preprint arXiv:1706.02690 , 2017. 2\\n[65] Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich,\\nRyan Sepassi, Lukasz Kaiser, and Noam Shazeer. Gener-\\nating wikipedia by summarizing long sequences. In Inter-\\nnational Conference on Learning Representations , 2018.\\n4\\n[66] Yezheng Liu, Zhe Li, Chong Zhou, Yuanchun Jiang, Jian-\\nshan Sun, Meng Wang, and Xiangnan He. Generative ad-\\nversarial active learning for unsupervised outlier detection.\\nIEEE Transactions on Knowledge and Data Engineering ,\\n32(8):1517–1528, 2019. 2\\n[67] Zhengzhe Liu, Xiaojuan Qi, and Philip HS Torr. Global\\ntexture enhancement for fake face detection in the wild.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='InProceedings of the IEEE/CVF conference on computer\\nvision and pattern recognition , pages 8060–8069, 2020. 2\\n[68] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic\\ngradient descent with warm restarts. arXiv preprint\\narXiv:1608.03983 , 2016. 2[69] Pingchuan Ma, Brais Martinez, Stavros Petridis, and Maja\\nPantic. Towards practical lipreading with distilled and ef-\\nficient models. In ICASSP 2021-2021 IEEE International\\nConference on Acoustics, Speech and Signal Processing\\n(ICASSP) , pages 7608–7612. IEEE, 2021. 2\\n[70] Sagnik Majumder, Ziad Al-Halah, and Kristen Grauman.\\nMove2hear: Active audio-visual source separation. In Pro-\\nceedings of the IEEE/CVF International Conference on Com-\\nputer Vision , pages 275–285, 2021. 2\\n[71] Hafiz Malik and Hany Farid. Audio forensics from acoustic\\nreverberation. In 2010 IEEE International Conference on\\nAcoustics, Speech and Signal Processing , 2010. 2\\n[72] Brais Martinez, Pingchuan Ma, Stavros Petridis, and Maja', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Pantic. Lipreading using temporal convolutional networks.\\nInICASSP 2020-2020 IEEE International Conference on\\nAcoustics, Speech and Signal Processing (ICASSP) , pages\\n6319–6323. IEEE, 2020. 2\\n[73] Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket\\nBera, and Dinesh Manocha. Emotions don’t lie: An audio-\\nvisual deepfake detection method using affective cues. In\\nProceedings of the 28th ACM international conference on\\nmultimedia , pages 2823–2832, 2020. 2\\n[74] Shentong Mo and Pedro Morgado. Localizing visual sounds\\nthe easy way. In European Conference on Computer Vision\\n(ECCV) , 2022. 2\\n[75] Pedro Morgado, Nuno Vasconcelos, and Ishan Misra. Audio-\\nvisual instance discrimination with cross-modal agreement.\\nInProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition , pages 12475–12486, 2021.\\n2\\n[76] Yuval Nirkin, Yosi Keller, and Tal Hassner. Fsgan: Subject\\nagnostic face swapping and reenactment. In Proceedings of', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the IEEE/CVF international conference on computer vision ,\\npages 7184–7193, 2019. 4, 5\\n[77] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\\nsentation learning with contrastive predictive coding. arXiv\\npreprint arXiv:1807.03748 , 2018. 3\\n[78] Andrew Owens and Alexei A Efros. Audio-visual scene\\nanalysis with self-supervised multisensory features. Euro-\\npean Conference on Computer Vision (ECCV) , 2018. 1, 2,\\n3\\n[79] Daniel P ´erez-Cabo, David Jim ´enez-Cabello, Artur Costa-\\nPazo, and Roberto J L ´opez-Sastre. Deep anomaly detec-\\ntion for generalized face anti-spoofing. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition Workshops , pages 0–0, 2019. 2\\n[80] Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu,\\nSugasa Marangonda, Chris Um ´e, Mr Dpfks, Carl Shift\\nFacenheim, RP Luis, Jian Jiang, et al. Deepfacelab: A\\nsimple, flexible and extensible face swapping framework.\\n2020. 4\\n[81] Stanislav Pidhorskyi, Ranya Almohsen, and Gianfranco', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Doretto. Generative probabilistic novelty detection with\\nadversarial autoencoders. Advances in neural information\\nprocessing systems , 31, 2018. 2\\n[82] KR Prajwal, Rudrabha Mukhopadhyay, Vinay P Nambood-\\niri, and CV Jawahar. A lip sync expert is all you need for\\n10501\\nspeech to lip generation in the wild. In Proceedings of the\\n28th ACM International Conference on Multimedia , pages\\n484–492, 2020. 4, 5, 6, 3\\n[83] Yuyang Qian, Guojun Yin, Lu Sheng, Zixuan Chen, and\\nJing Shao. Thinking in frequency: Face forgery detection\\nby mining frequency-aware clues. In European conference\\non computer vision , pages 86–103. Springer, 2020. 2, 4\\n[84] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\\nAmodei, Ilya Sutskever, et al. Language models are unsu-\\npervised multitask learners. OpenAI blog , 1(8):9, 2019. 1,\\n2, 4\\n[85] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\\nand Mark Chen. Hierarchical text-conditional image gen-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='eration with clip latents. arXiv preprint arXiv:2204.06125 ,\\n2022. 4\\n[86] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gener-\\nating diverse high-fidelity images with vq-vae-2. Advances\\nin neural information processing systems , 32, 2019. 3\\n[87] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Chris-\\ntian Riess, Justus Thies, and Matthias Nießner. Faceforen-\\nsics++: Learning to detect manipulated facial images. In\\nProceedings of the IEEE/CVF international conference on\\ncomputer vision , pages 1–11, 2019. 2, 4, 5, 6, 1\\n[88] Lukas Ruff, Robert A Vandermeulen, Nico G ¨ornitz, Alexan-\\nder Binder, Emmanuel M ¨uller, Klaus-Robert M ¨uller, and\\nMarius Kloft. Deep semi-supervised anomaly detection.\\narXiv preprint arXiv:1906.02694 , 2019. 2\\n[89] Mohammad Sabokrou, Mohammad Khalooei, Mahmood\\nFathy, and Ehsan Adeli. Adversarially learned one-class clas-\\nsifier for novelty detection. In Proceedings of the IEEE con-\\nference on computer vision and pattern recognition , pages', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='3379–3388, 2018. 2\\n[90] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P\\nKingma. Pixelcnn++: Improving the pixelcnn with dis-\\ncretized logistic mixture likelihood and other modifications.\\narXiv preprint arXiv:1701.05517 , 2017. 1, 2\\n[91] Thomas Schlegl, Philipp Seeb ¨ock, Sebastian M Waldstein,\\nUrsula Schmidt-Erfurth, and Georg Langs. Unsupervised\\nanomaly detection with generative adversarial networks to\\nguide marker discovery. In International conference on\\ninformation processing in medical imaging , pages 146–157.\\nSpringer, 2017. 2\\n[92] Aliaksandr Siarohin, St ´ephane Lathuili `ere, Sergey Tulyakov,\\nElisa Ricci, and Nicu Sebe. First order motion model for im-\\nage animation. Advances in Neural Information Processing\\nSystems , 32, 2019. 4\\n[93] Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Er-\\nmon, and Nate Kushman. Pixeldefend: Leveraging gener-\\native models to understand and defend against adversarial\\nexamples. arXiv preprint arXiv:1710.10766 , 2017. 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[94] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya\\nSutskever, and Ruslan Salakhutdinov. Dropout: a simple\\nway to prevent neural networks from overfitting. The journal\\nof machine learning research , 15(1):1929–1958, 2014. 2\\n[95] Ruijie Tao, Zexu Pan, Rohan Kumar Das, Xinyuan Qian,\\nMike Zheng Shou, and Haizhou Li. Is someone speaking?\\nexploring long-term temporal features for audio-visual ac-\\ntive speaker detection. In Proceedings of the 29th ACMInternational Conference on Multimedia , pages 3927–3935,\\n2021. 2\\n[96] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt,\\nOriol Vinyals, Alex Graves, et al. Conditional image genera-\\ntion with pixelcnn decoders. Advances in neural information\\nprocessing systems , 29, 2016. 7\\n[97] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\\nrepresentation learning. Advances in neural information\\nprocessing systems , 30, 2017. 3\\n[98] Aaron Van Oord, Nal Kalchbrenner, and Koray\\nKavukcuoglu. Pixel recurrent neural networks. In', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='International conference on machine learning , pages\\n1747–1756. PMLR, 2016. 1, 2\\n[99] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. Advances in neural\\ninformation processing systems , 30, 2017. 1, 3, 4, 2\\n[100] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew\\nOwens, and Alexei A Efros. Cnn-generated images are\\nsurprisingly easy to spot... for now. Computer Vision and\\nPattern Recognition (CVPR) , 2020. 2, 4\\n[101] Yue Wu, Wael AbdAlmageed, and Premkumar Natarajan.\\nMantra-net: Manipulation tracing network for detection and\\nlocalization of image forgeries with anomalous features.\\nInProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition , pages 9543–9552, 2019. 2\\n[102] Ran Yi, Zipeng Ye, Juyong Zhang, Hujun Bao, and Yong-\\nJin Liu. Audio-driven talking face video generation with\\nlearning-based personalized head pose. arXiv preprint', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='arXiv:2002.10137 , 2020. 4, 6\\n[103] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\\nfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive\\nmodels for content-rich text-to-image generation. arXiv\\npreprint arXiv:2206.10789 , 2022. 3\\n[104] Houssam Zenati, Manon Romain, Chuan-Sheng Foo, Bruno\\nLecouat, and Vijay Chandrasekhar. Adversarially learned\\nanomaly detection. In 2018 IEEE International conference\\non data mining (ICDM) , pages 727–736. IEEE, 2018. 2\\n[105] Zhaoyang Zeng, Daniel McDuff, Yale Song, et al. Con-\\ntrastive learning of global and local video representa-\\ntions. Advances in Neural Information Processing Systems ,\\n34:7025–7040, 2021. 2\\n[106] Hongjie Zhang, Ang Li, Jie Guo, and Yanwen Guo. Hybrid\\nmodels for open set recognition. In European Conference\\non Computer Vision , pages 102–117. Springer, 2020. 2\\n[107] Shifeng Zhang, Xiangyu Zhu, Zhen Lei, Hailin Shi, Xiaobo', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Wang, and Stan Z Li. S3fd: Single shot scale-invariant\\nface detector. In Proceedings of the IEEE international\\nconference on computer vision , pages 192–201, 2017. 4\\n[108] Tianchen Zhao, Xiang Xu, Mingze Xu, Hui Ding, Yuanjun\\nXiong, and Wei Xia. Learning self-consistency for deepfake\\ndetection. In Proceedings of the IEEE/CVF international\\nconference on computer vision , pages 15023–15033, 2021.\\n2\\n[109] Yinglin Zheng, Jianmin Bao, Dong Chen, Ming Zeng, and\\nFang Wen. Exploring temporal coherence for more gen-\\neral video face forgery detection. In Proceedings of the\\n10502\\nIEEE/CVF International Conference on Computer Vision ,\\npages 15044–15054, 2021. 4, 5, 6, 1\\n[110] Hang Zhou, Xudong Xu, Dahua Lin, Xiaogang Wang, and\\nZiwei Liu. Sep-stereo: Visually guided stereophonic audio\\ngeneration by associating source separation. In Proceedings\\nof the European Conference on Computer Vision (ECCV) ,\\n2020. 2\\n[111] Tianfei Zhou, Wenguan Wang, Zhiyuan Liang, and Jian-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='bing Shen. Face forensics in the wild. In Proceedings of\\nthe IEEE/CVF conference on computer vision and pattern\\nrecognition , pages 5778–5788, 2021. 2\\n[112] Yipin Zhou and Ser-Nam Lim. Joint audio-visual deepfake\\ndetection. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision , pages 14800–14809, 2021.\\n2, 4, 5, 6, 1\\n[113] Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cris-\\ntian Lumezanu, Daeki Cho, and Haifeng Chen. Deep autoen-\\ncoding gaussian mixture model for unsupervised anomaly\\ndetection. In International conference on learning represen-\\ntations , 2018. 2\\n10503', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP\\nRunnan Chen1,2Youquan Liu2,3Lingdong Kong2,4Xinge Zhu5Yuexin Ma6Yikang Li2\\nYuenan Hou2,†Yu Qiao2Wenping Wang7,†\\n1The University of Hong Kong2Shanghai AI Laboratory3Hochschule Bremerhaven4National University of Singapore\\n5The Chinese University of Hong Kong6ShanghaiTech University7Texas A&M University\\nAbstract\\nContrastive Language-Image Pre-training (CLIP)\\nachieves promising results in 2D zero-shot and few-shot\\nlearning. Despite the impressive performance in 2D, apply-\\ning CLIP to help the learning in 3D scene understanding\\nhas yet to be explored. In this paper, we make the first\\nattempt to investigate how CLIP knowledge benefits 3D\\nscene understanding. We propose CLIP2Scene, a simple\\nyet effective framework that transfers CLIP knowledge\\nfrom 2D image-text pre-trained models to a 3D point cloud\\nnetwork. We show that the pre-trained 3D network yields\\nimpressive performance on various downstream tasks,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='i.e., annotation-free and fine-tuning with labelled data for\\nsemantic segmentation. Specifically, built upon CLIP , we\\ndesign a Semantic-driven Cross-modal Contrastive Learning\\nframework that pre-trains a 3D network via semantic and\\nspatial-temporal consistency regularization. For the former,\\nwe first leverage CLIP’s text semantics to select the positive\\nand negative point samples and then employ the contrastive\\nloss to train the 3D network. In terms of the latter, we\\nforce the consistency between the temporally coherent point\\ncloud features and their corresponding image features. We\\nconduct experiments on SemanticKITTI, nuScenes, and\\nScanNet. For the first time, our pre-trained network achieves\\nannotation-free 3D semantic segmentation with 20.8% and\\n25.08% mIoU on nuScenes and ScanNet, respectively. When\\nfine-tuned with 1% or 100% labelled data, our method\\nsignificantly outperforms other self-supervised methods,\\nwith improvements of 8% and 1% mIoU, respectively.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Furthermore, we demonstrate the generalizability for\\nhandling cross-domain datasets. Code is publicly available1.\\n1. Introduction\\n3D scene understanding is fundamental in autonomous\\ndriving, robot navigation, etc [26,28]. Current deep learning-\\nSymbol†denotes the corresponding authors.\\n1https://github.com/runnanchen/CLIP2Scene .\\nSemantic and \\nSpatial -Temporal\\nConsistency\\nRegularizationImage\\nEncoderAnnotation -free\\n1% annotation\\nCLIP2Scene\\nText\\nEncoderCLIPHow CLIP benefits \\n3D scene \\nunderstanding?\\n100% annotationSemantic -driven Cross -modal  Contrastive Learning\\ncar, bus\\npedestrian\\ncarFigure 1. We explore how CLIP knowledge benefits 3D scene\\nunderstanding. To this end, we propose CLIP2Scene, a Semantic-\\ndriven Cross-modal Contrastive Learning framework that leverages\\nCLIP knowledge to pre-train a 3D point cloud segmentation net-\\nwork via semantic and spatial-temporal consistency regularization.\\nCLIP2Scene yields impressive performance on annotation-free 3D', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='semantic segmentation and significantly outperforms other self-\\nsupervised methods when fine-tuning on annotated data.\\nbased methods have shown inspirational performance on 3D\\npoint cloud data [15, 32, 33, 38, 47, 56, 62]. However, some\\ndrawbacks hinder their real-world applications. The first\\none comes from their heavy reliance on the large collection\\nof annotated point clouds, especially when high-quality 3D\\nannotations are expensive to acquire [39,40,44,51]. Besides,\\nthey typically fail to recognize novel objects that are never\\nseen in the training data [11,45]. As a result, it may need ex-\\ntra annotation efforts to train the model on recognizing these\\nnovel objects, which is both tedious and time-consuming.\\nContrastive Vision-Language Pre-training (CLIP) [48]\\nprovides a new perspective that mitigates the above issues\\nin 2D vision. It was trained on large-scale free-available\\nimage-text pairs from websites and built vision-language', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n7020\\ncorrelation to achieve promising open-vocabulary recogni-\\ntion. MaskCLIP [61] further explores semantic segmenta-\\ntion based on CLIP. With minimal modifications to the CLIP\\npre-trained network, MaskCLIP can be directly used for the\\nsemantic segmentation of novel objects without additional\\ntraining efforts. PointCLIP [59] reveals that the zero-shot\\nclassification ability of CLIP can be generalized from the\\n2D image to the 3D point cloud. It perspectively projects a\\npoint cloud frame into different views of 2D depth maps that\\nbridge the modal gap between the image and the point cloud.\\nThe above studies indicate the potential of CLIP on enhanc-\\ning the 2D segmentation and 3D classification performance.\\nHowever, whether and how CLIP knowledge benefits 3D', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='scene understanding is still under-explored.\\nIn this paper, we explore how to leverage CLIP’s 2D\\nimage-text pre-learned knowledge for 3D scene understand-\\ning. Previous cross-modal knowledge distillation meth-\\nods [44, 51] suffer from the optimization-conflict issue, i.e.,\\nsome of the positive pairs are regarded as negative samples\\nfor contrastive learning, leading to unsatisfactory represen-\\ntation learning and hammering the performance of down-\\nstream tasks. Besides, they also ignore the temporal coher-\\nence of the multi-sweep point cloud, failing to utilize the\\nrich inter-sweep correspondence. To handle the mentioned\\nproblems, we propose a novel Semantic-driven Cross-modal\\nContrastive Learning framework that fully leverages CLIP’s\\nsemantic and visual information to regularize a 3D network.\\nSpecifically, we propose Semantic Consistency Regulariza-\\ntion and Spatial-Temporal Consistency Regularization. In\\nsemantic consistency regularization, we utilize CLIP’s text', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='semantics to select the positive and negative point samples\\nfor less-conflict contrastive learning. For spatial-temporal\\nconsistency regularization, we take CLIP’s image pixel fea-\\nture to impose a soft consistency constraint on the temporally\\ncoherent point features. Such an operation also alleviates the\\neffects of imperfect image-to-point calibration.\\nWe conduct several downstream tasks on the indoor and\\noutdoor datasets to verify how the pre-trained network bene-\\nfits the 3D scene understanding. The first one is annotation-\\nfree semantic segmentation. Following MaskCLIP, we place\\nclass names into multiple hand-crafted templates as prompts\\nand average the text embeddings generated by CLIP to con-\\nduct the annotation-free segmentation. For the first time,\\nour method achieves 20.8% and 25.08% mIoU annotation-\\nfree 3D semantic segmentation on the nuScenes [24] and\\nScanNet [20] datasets without training on any labelled data.\\nSecondly, we compare with other self-supervised methods', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='in label-efficient learning. When fine-tuning the 3D network\\nwith 1% or 100% labelled data on the nuScenes dataset,\\nour method significantly outperforms state-of-the-art self-\\nsupervised methods, with improvements of 8% and 1%\\nmIoU, respectively. Besides, to verify the generalization\\ncapability, we pre-train the network on the nuScenes datasetand evaluate it on SemanticKITTI [3]. Our method still\\nsignificantly outperforms state-of-the-art methods. The key\\ncontributions of our work are summarized as follows.\\n•The first work that distils CLIP knowledge to a 3D\\nnetwork for 3D scene understanding.\\n•We propose a novel Semantic-driven Cross-modal Con-\\ntrastive Learning framework that pre-trains a 3D net-\\nwork via spatial-temporal and semantic consistency\\nregularization.\\n•We propose a novel Semantic-guided Spatial-Temporal\\nConsistency Regularization that forces the consistency\\nbetween the temporally coherent point cloud features\\nand their corresponding image features.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='•For the first time, our method achieves promising re-\\nsults on annotation-free 3D scene segmentation. When\\nfine-tuning with labelled data, our method significantly\\noutperforms state-of-the-art self-supervised methods.\\n2. Related Work\\nZero-shot Learning in 3D. The objective of zero-shot learn-\\ning (ZSL) is to recognize objects that are unseen in the\\ntraining set. Many efforts have been devoted to the 2D recog-\\nnition tasks [1, 2, 4, 8, 21, 25, 34, 37, 41 –43, 46, 54, 55, 58, 60],\\nand few works concentrate on performing ZSL in the 3D\\ndomain [11, 16 –18, 45]. [18] applies ZSL to 3D tasks, where\\nthey train PointNet [47] on ”seen” samples and test on ”un-\\nseen” samples. Subsequent work [16] addresses the hubness\\nproblem caused by the low-quality point cloud features. [17]\\nproposes the triplet loss to boost the performance under the\\ntransductive setting, where the ”unseen” class is observed\\nand unlabeled in the training phase. [11] makes the first at-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tempt to explore the transductive zero-shot segmentation for\\n3D scene understanding. Recently, some studies introduced\\nCLIP into zero-shot learning. MaskCLIP [61] investigates\\nthe problem of utilizing CLIP to help the 2D dense prediction\\ntasks and exhibits encouraging zero-shot semantic segmen-\\ntation performance. PointCLIP [59] is the pioneering work\\nthat applies CLIP to 3D recognition and shows impressive\\nperformance on zero-shot and few-shot classification tasks.\\nOur work takes a step further to investigate how the rich\\nsemantic and visual knowledge in CLIP can benefit the 3D\\nsemantic segmentation tasks.\\nSelf-supervised Representation Learning. The purpose\\nof self-supervised learning is to obtain a good representa-\\ntion that benefits the downstream tasks. The dominant ap-\\nproaches resort to contrastive learning to pre-train the net-\\nwork [7, 9, 9, 10, 12 –14, 22, 23, 27, 30]. Recently, inspired\\nby the success of CLIP, leveraging the pre-trained model', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='of CLIP to the downstream tasks has raised the commu-\\nnity’s attention [35, 36, 49, 50, 57]. DenseCLIP [49] utilizes\\n7021\\nSpatial -Temporal Consistency Regularization\\nImage\\nEncoderText\\nEncoder\\ncar, bus\\nPedestrian\\n…A \\nphoto \\nof a { }; \\nThis is \\nthe { } \\nin the \\nscene; \\n…Semantic Consistency Regularization\\nPoint\\nEncoderCLIP\\npixel -to-text mapping\\npixel -point -text pairspixel -to-point mapping3D Networkpoint -text pairs\\n…………………\\nMulti -sweeps \\ncalibration\\n……grid 1 grid 2grid 3pulling force\\nSemantic -guided fusion features \\ntext embedding\\npoint featuretext embedding\\npoint feature\\npixel feature\\npoint featureprompts\\n𝑃1\\n𝑃2\\n𝑃3image feature\\n…………grid 1\\ngrid 2\\ngrid 3Figure 2. Illustration of the Semantic-driven Cross-modal Contrastive Learning. Firstly, we obtain the text embedding ti, image pixel\\nfeature xi, and point feature piby text encoder, image encoder, and point encoder, respectively. Secondly, we leverage CLIP knowledge to', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='construct positive and negative samples for contrastive learning. Thus we obtain point-text pairs {xi, ti}M\\ni=1and all pixel-point-text pairs in\\na short temporal {ˆxk\\ni,ˆpk\\ni, tk\\ni}ˆM,K\\ni=1,k=1. Here, {xi, ti}M\\ni=1and{ˆxk\\ni,ˆpk\\ni, tk\\ni}ˆM,K\\ni=1,k=1are used for Semantic Consistency Regularization and\\nSpatial-Temporal Consistency Regularization, respectively. Lastly, we perform Semantic Consistency Regularization by pulling the point\\nfeatures to their corresponding text embedding and Spatial-Temporal Consistency Regularization by mimicking the temporally coherent\\npoint features to their corresponding pixel features.\\nthe CLIP’s pre-trained knowledge for dense image pixel\\nprediction. DetCLIP [57] proposes a pre-training method\\nequipped with CLIP for open-world detection. We leverage\\nthe image-text pre-trained CLIP knowledge to help 3D scene\\nunderstanding.\\nCross-modal Knowledge Distillation. Recently, increasing\\nstudies have focused on transferring knowledge from 2D', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='images to 3D point clouds for self-supervised representation\\nlearning [44, 51]. PPKT [44] resorts to the InfoNCE loss\\nto help the 3D network distil rich knowledge from the 2D\\nimage backbone. SLidR [51] further introduce the super-\\npixel to boost the cross-modal knowledge distillation. In this\\npaper, we first attempt to pre-train a 3D network with CLIP’s\\nknowledge.\\n3. Methodology\\nConsidering the impressive open-vocabulary performance\\nachieved by CLIP in image classification and segmentation,\\nnatural curiosities have been raised. Can CLIP endow the\\nability to a 3D network for annotation-free scene understand-\\ning? And further, will it promote the network performance\\nwhen fine-tuned on labelled data? To answer the above ques-\\ntions, we study the cross-modal knowledge transfer of CLIP\\nfor 3D scene understanding, termed CLIP2Scene . Our work\\nis a pioneer in exploiting CLIP knowledge for 3D sceneunderstanding. In what follows, we revisit the CLIP applied', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='in 2D open-vocabulary classification and semantic segmen-\\ntation, then present our CLIP2Scene in detail. Our approach\\nconsists of three major components: Semantic Consistency\\nRegularization, Semantic-Guided Spatial-Temporal Consis-\\ntency Regularization, and Switchable Self-Training Strategy.\\n3.1. Revisiting CLIP\\nContrastive Vision-Language Pre-training (CLIP) miti-\\ngates the following drawbacks that dominate the computer\\nvision field: 1. Deep models need a large amount of format-\\nted and labelled training data, which is expensive to acquire;\\n2. The model’s generalization ability is weak, making it diffi-\\ncult to migrate to a new scenario with unseen objects. CLIP\\nconsists of an image encoder (ResNet [31] or ViT [6]) and\\na text encoder (Transformer [53]), both respectively project\\nthe image and text representation to a joint embedding space.\\nDuring training, CLIP constructs positive and negative sam-\\nples from 400 million image-text pairs to train both encoders', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='with a contrastive loss, where the large-scale image-text pairs\\nare free-available from the Internet and assumed to contain\\nevery class of images and most concepts of text. Therefore,\\nCLIP can achieve promising open-vocabulary recognition.\\nFor 2D zero-shot classification, CLIP first places the class\\nname into a pre-defined template to generate the text embed-\\n7022\\ncar road bicycle ... building\\nText embeddingsFigure 3. Illustration of the image pixel-to-text mapping. The\\ndense pixel-text correspondence {xi, ti}M\\ni=1is extracted by the\\noff-the-shelf method MaskCLIP [61].\\ndings and then encodes images to obtain image embeddings.\\nNext, it calculates the similarities between images and text\\nembeddings to determine the class. MaskCLIP further ex-\\ntends CLIP into 2D semantic segmentation. Specifically,\\nMaskCLIP modifies the attention pooling layer of the CLIP’s\\nimage encoder, thus performing pixel-level mask prediction\\ninstead of the global image-level prediction.\\n3.2. CLIP2Scene', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='3.2. CLIP2Scene\\nAs shown in Fig. 2, we first leverage CLIP and 3D net-\\nwork to respectively extract the text embeddings, image pixel\\nfeature, and point feature. Secondly, we construct positive\\nand negative samples based on CLIP’s knowledge. Lastly,\\nwe impose Semantic Consistency Regularization by pulling\\nthe point features to their corresponding text embedding.\\nAt the same time, we apply Spatial-Temporal Consistency\\nRegularization by forcing the consistency between tempo-\\nrally coherent point features and their corresponding pixel\\nfeatures. In what follows, we present the details and insights.\\n3.2.1 Semantic Consistency Regularization\\nAs CLIP is pre-trained on 2D images and text, our first con-\\ncern is the domain gap between 2D images and the 3D point\\ncloud. To this end, we build dense pixel-point correspon-\\ndence and transfer image knowledge to the 3D point cloudvia the pixel-point pairs. Specifically, we calibrate the Li-\\nDAR point cloud with corresponding images captured by six', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='cameras. Therefore, the dense pixel-point correspondence\\n{xi, pi}M\\ni=1can be obtained accordingly, where xiandpi\\nindicates i-th paired image feature and point feature, which\\nare respectively extracted by the CLIP’s image encoder and\\nthe 3D network. Mis the number of pairs. Note that it is\\nan online operation and is irreverent to the image and point\\ndata augmentation.\\nPrevious methods [44, 51] provide a promising solution\\nto cross-modal knowledge transfer. They first construct\\npositive pixel-point pairs {xi, pi}M\\ni=1and negative pairs\\n{xi, pj}(i̸=j), and then pull in the positive pairs while\\npushing away the negative pairs in the embedding space via\\nthe InfoNCE loss. Despite the encourageable performance\\nof previous methods in transferring cross-modal knowledge,\\nthey are both confronted with the same optimization-conflict\\nissue. For example, suppose i-th pixel xiandj-th point\\npjare in the different positions of the same instance with', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the same semantics. However, the InfoNCE loss will try\\nto push them away, which is unreasonable and hammer\\nthe performance of the downstream tasks [51]. In light\\nof this, we propose a Semantic Consistency Regularization\\nthat leverages the CLIP’s semantic information to allevi-\\nate this issue. Specifically, we generate the dense pixel-\\ntext pairs {xi, ti}M\\ni=1by following the off-the-shelf method\\nMaskCLIP [61] (Fig. 3), where tiis the text embedding gen-\\nerated from the CLIP’s text encoder. Note that the pixel-text\\nmappings are free-available from CLIP without any addi-\\ntional training. We then transfer pixel-text pairs to point-text\\npairs{pi, ti}M\\ni=1and utilize the text semantics to select the\\npositive and negative point samples for contrastive learning.\\nThe objective function is as follows:\\nLSinfo=−CX\\nc=1logP\\nti∈c,piexp(D(ti, pi)/τ)P\\nti∈c,tj/∈c,pjexp(D(ti, pj)/τ),\\n(1)\\nwhere ti∈cindicates that tiis generated by c-th classes\\nname, and Cis the number of classes. Ddenotes the scalar', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='product operation and τis a temperature term ( τ >0).\\nSince the text is composed of class names placed into\\npre-defined templates, the text embedding represents the\\nsemantic information of the corresponding class. Therefore,\\nthose points with the same semantics will be restricted near\\nthe same text embedding, and those with different semantics\\nwill be pushed away. To this end, our Semantic Consistency\\nRegularization causes less conflict in contrastive learning.\\n3.2.2 Semantic-guided Spatial-temporal Consistency\\nRegularization\\nBesides semantic consistency regularization, we consider\\nhow image pixel features help to regularize a 3D network.\\n7023\\nImage 𝐼Pixel -to-point mapping\\n𝑃1𝑃2𝑃3\\nMulti -sweeps calibration\\n……………grid 1\\ngrid 2\\ngrid 3\\n……grid 1\\ngrid 2\\ngrid 3\\n{𝑓𝑛}𝑛=1𝑁ො𝑥𝑖𝑘,ො𝑝𝑖𝑘\\n𝑛=1,𝑘=1𝑁,𝐾Text embeddingFigure 4. Illustration of the image pixel-to-point mapping (left)\\nand semantic-guided fusion feature generation (right). We build\\nthe grid-wise correspondence between an image Iand the tem-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='porally coherent LiDAR point cloud {Pk}K\\nk=1within Sseconds\\nand generate semantic-guided fusion features for individual grids.\\nBoth {ˆxk\\ni,ˆpk\\ni}ˆM,K\\ni=1,k=1and{fn}N\\nn=1are used to perform Spatial-\\nTemporal Consistency Regularization.\\nThe natural alternative directly pulls in the point feature with\\nits corresponding pixel in the embedding space. However,\\nthe noise-assigned semantics of the image pixel and the im-\\nperfect pixel-point mapping hinder the downstream task’s\\nperformance. To this end, we propose a novel semantic-\\nguided Spatial-Temporal Consistency Regularization to al-\\nleviate the problem by imposing a soft constraint on points\\nwithin local space and time.\\nSpecifically, given an image Iand temporally coherent\\nLiDAR point cloud {Pk}K\\nk=1, where Kis the number of\\nsweeps within Sseconds. Note that the image is matched to\\nthe first frame of the point cloud P1with pixel-point pairs\\n{ˆx1\\ni,ˆp1\\ni}ˆM\\ni=1. We register the rest of the point cloud to the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='first frame via the calibration matrices and map them to the\\nimage (Fig. 4). Thus we obtain all pixel-point-text pairs\\nin a short temporal {ˆxk\\ni,ˆpk\\ni, tk\\ni}ˆM,K\\ni=1,k=1. Next, we divide\\nthe entire stitched point cloud into regular grids {gn}N\\nn=1,\\nwhere the temporally coherent points are located in the same\\ngrid. We impose the spatial-temporal consistency constraint\\nwithin individual grids by the following objective function:\\nLSSR=X\\ngnX\\n(ˆi,ˆk)∈gn(1−sigmoid (D(ˆpˆk\\nˆi, fn)))/N, (2)\\nwhere (ˆi,ˆk)∈gnindicates the pixel-point pair {ˆxk\\ni,ˆpk\\ni}\\nis located in the n-th grid. {fn}N\\nn=1is a semantic-guided\\ncross-modal fusion feature formulated by:\\nfn=X\\n(ˆi,ˆk)∈gnaˆk\\nˆi∗ˆxˆk\\nˆi+bˆk\\nˆi∗ˆpˆk\\nˆi, (3)\\nwhere aˆk\\nˆiandbˆk\\nˆiare attention weight calculated by:aˆk\\nˆi=exp(D(ˆxˆk\\nˆi, t1\\nˆi)/λ)\\nP\\n(ˆi,ˆk)∈gnexp(D(ˆxˆk\\nˆi, t1\\nˆi)/λ) + exp( D(ˆpˆk\\nˆi, t1\\nˆi)/λ),\\nbˆk\\nˆi=exp(D(ˆpˆk\\nˆi, t1\\nˆi)/λ)\\nP\\n(ˆi,ˆk)∈gnexp(D(ˆxˆk\\nˆi, t1\\nˆi)/λ) + exp( D(ˆpˆk\\nˆi, t1\\nˆi)/λ),\\n(4)\\nwhere λis the temperature term.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Actually, those pixel and point features within the local\\ngridgnare restricted near a dynamic centre fn. Thus, such a\\nsoft constraint alleviates the noisy prediction and calibration\\nerror issues. At the same time, it imposes Spatio-Temporal\\nRegularization on the temporally coherent point features.\\n3.2.3 Switchable Self-training Strategy\\nWe combine the loss function LSinfo andLSSR to end-\\nto-end train the whole network, where the CLIP’s image\\nand text encoder backbone are frozen during training. We\\nfind that method worked only when the pixel-point feature\\n{xi, pi}M\\ni=1and{ˆxk\\ni,ˆpk\\ni}ˆM,K\\ni=1,k=1, which are used in LSinfo\\nandLSSR, are generated from different learnable linear layer.\\nOn top of that, we further put forward an effective strategy to\\npromote performance. Specifically, after contrastive learning\\nof the 3D network for a few epochs, we randomly switch\\nthe point pseudo label between the paired image pixel’s\\npseudo label and the point’s predicted label. Since different', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='modality networks learn different feature representations,\\nthey can filter different types of error introduced by noisy\\npseudo labels. By this switchable operation, the error flows\\ncan be reduced by mutually [29].\\n4. Experiments\\nDatasets. We conduct extensive experiments on two large-\\nscale outdoor LiDAR semantic segmentation datasets, i.e.,\\nSemanticKITTI [3] and nuScenes [5, 24], and one indoor\\ndataset ScanNet [20]. The nuScenes dataset contains 700\\nscenes for training, 150 scenes for validation, and 150 scenes\\nfor testing, where 16 classes are utilized for LiDAR semantic\\nsegmentation. As for SemanticKITTI, it contains 19 classes\\nfor training and evaluation. It has 22 sequences, where\\nsequences 00 to 10, 08, and 11 to 21 are used for training,\\nvalidation, and testing, respectively. ScanNet [20] contains\\n1603 scans with 20 classes, where 1201 scans are for training,\\n312 scans are for validation, and 100 scans are for testing.\\nImplementation Details. We follow SLidR [51] to pre-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='train the network on the nuScenes [5, 24] dataset. The\\nnetwork is pre-trained on all keyframes from 600 scenes.\\nBesides, the pre-trained network is fine-tuned on Se-\\nmanticKITTI [3] to verify the generalization ability. We\\n7024\\nTable 1. Comparisons (mIoU) among self-supervised methods on\\nthe nuScenes [24], SemanticKITTI [3], and ScanNet [20] valsets.\\nInitializationnuScenes SemanticKITTI ScanNet\\n1% 100% 1% 100% 5% 100%\\nRandom 42.2 69 .1 32.5 52 .1 46.1 63 .3\\nPPKT [44] 48.0 70 .1 39.1 53 .1 47.5 64 .2\\nSLidR [51] 48.2 70 .4 39.6 54 .3 47.9 64 .9\\nPointContrast [55] 47.2 69 .2 37.1 52 .3 47.6 64 .5\\nCLIP2Scene 56.3 71 .542.6 55 .048.4 65 .1\\nTable 2. Annotation-free 3D semantic segmentation performance\\n(mIoU) on the nuScenes [24] and ScanNet [20] valsets.\\nMethod nuScenes ScanNet\\nCLIP2Scene 20.80 25.08\\nleverage the CLIP model to generate image features and\\ntext embedding. Following MaskCLIP, we modify the at-\\ntention pooling layer of the CLIP’s image encoder, thus', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='extracting the dense pixel-text correspondences. We take\\nSPVCNN [52] as the 3D network to produce the point-wise\\nfeature. The framework is developed on PyTorch, where the\\nCLIP model is frozen during training. The training time is\\nabout 40 hours for 20 epochs on two NVIDIA Tesla A100\\nGPUs. The optimizer is SGD with a cosine scheduler. We\\nset the temperature λandτto be 1 and 0.5, respectively. The\\nsweep number is set to be 3 empirically. Besides, We adopt\\nMinkowskiNet14 [19] as the backbone for evaluation on the\\nScanNet dataset, where the number of sweeps is set to be 1\\nand the training epochs is 30. As for the Switchable Self-\\nTraining Strategy, we randomly switch the point supervision\\nsignal after 10 epochs. We apply several data augmentations\\nin contrastive learning, including random rotation along the\\nz-axis and random flip on the point cloud, random horizontal\\nflip, and random crop-resize on the image.\\n4.1. Annotation-free Semantic Segmentation', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='After pre-training the network, we show the performance\\nof the 3D network when it is not fine-tuned on any anno-\\ntations (Table 2). As no previous method reports the 3D\\nannotation-free segmentation performance, we compare our\\nmethod with different setups (Table 3). In what follows, we\\ndescribe the experimental settings and give insights into our\\nmethod and the different settings.\\nSettings. We conduct experiments on the nuScenes and\\nScanNet datasets to evaluate the annotation-free semantic\\nsegmentation performance. Following MaskCLIP [61], we\\nplace the class name into 85 hand-craft prompts and feed it\\ninto the CLIP’s text encoder to produce multiple text features.\\nWe then average the text features and feed the averaged fea-\\ntures to the classifier for point-wise prediction. Besides, to\\nexplore how to effectively transfer CLIP’s knowledge to the\\n3D network for annotation-free segmentation, We conductTable 3. Ablation study on the nuScenes [24] valset for annotation-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='free 3D semantic segmentation.\\nAblation Target Setting mIoU (%)\\n- Baseline 15.1\\nPromptsnuScenes 15.1(+0.0)\\nSemanticKITTI 13.9(−1.2)\\nCityscapes 11.3(−3.8)\\nAll 15.3(+0.2)\\nRegularizationw/o StCR 19.8(+4.7)\\nw/o SCR 16.8(+1.7)\\nKL 0.0(−15.1)\\nTraining Strategyw/o S3 18.8(+3.7)\\nST 10.1(−4.0)\\nSweeps1 sweep 18.7(+3.6)\\n3 sweeps 20.8(+5.7)\\n5 sweeps 20.6(+5.5)\\nmerged 18.6(+3.5)\\nFull Configuration CLIP2Scene 20.8(+5.7)\\nthe following experiments to highlight the effectiveness of\\ndifferent modules in our framework.\\nBaseline. The input of the 3D semantic segmentation net-\\nwork is only one sweep, and we pre-train the framework via\\nsemantic consistency regularization.\\nPrompts (nuScenes, SemanticKITTI, Cityscapes, All).\\nBased on the baseline, we respectively replace the nuScenes,\\nSemanticKITTI, Cityscapes, and all class names into the\\nprompts to produce the text embedding.\\nRegularization (w/o StCR, w/o SCR, KL). Based on the\\nfull method, we remove the Spatial-temporal Consistency', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Regularization (w/o StCR) and remove the Semantic Con-\\nsistency Regularization (w/o SCR). Besides, we abuse both\\nStCR and SCR and distill the image feature to the point\\ncloud by Kullback–Leibler (KL) divergence loss.\\nTraining Strategies (w/o S3, ST). We abuse the Switchable\\nSelf-Training Strategy (w/o S3) in the full method. Besides,\\nwe show the performance of only training the 3D network\\nby their own predictions after ten epochs (ST).\\nSweeps Number (1 sweep, 3 sweeps, 5 sweeps, and\\nmerged). We set the sweep number Kto be 1, 3, and\\n5, respectively. Besides, we also take three sweeps of the\\npoint cloud as the input to pre-train the network (merged).\\nEffect of Different Prompts. To verify how text embedding\\naffects the performance, we generate various text embedding\\nby the class name from different datasets (nuScenes, Se-\\nmanticKITTI, and Cityscapes) and all classes for pre-training\\nthe framework. As shown in Table 3, we find that even learn-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ing with other datasets’ text embedding (SemanticKITTI\\nand Cityscapes), the 3D network could still recognize the\\nnuScenes’s objects with decent performance (13.9% and\\n11.3% mIoU, respectively). The result shows that the 3D\\nnetwork is capable of open-vocabulary recognition ability.\\n7025\\nGround truth Ours* OursBus\\n Motorcycle\\n Car\\n TruckFigure 5. Qualitative results of annotation-free semantic segmentation on nuScenes dataset. Note that we show the results of individual\\nclasses. From the left to the right column are bus,motorcycle ,car, and truck , respectively. The first row [ground truth] is the annotated\\nsemantic label. The second row [ours*] is our prediction of the highlighted target. The third row [ours] is our prediction of full classes.\\nEffect of Semantic and Spatial-temporal Consistency\\nRegularization. We remove Spatial-temporal Consistency\\nRegularization (w/o SCR) from our method. Experiments\\nshow that the performance is dramatically decreased, indicat-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ing the effectiveness of our design. Besides, we also distill\\nthe image feature to the point cloud by KL divergence loss,\\nwhere the text embeddings calculate the logits. However,\\nsuch a method fails to transfer the semantic information\\nfrom the image. The main reason is the noise-assigned se-\\nmantics of the image pixel and the imperfect pixel-point\\ncorrespondence due to the calibration error.\\nEffect of Switchable Self-training Strategy. To examine\\nthe effect of the Switchable Self-Training Strategy, we either\\ntrain the network with image supervision (w/o S3) or train\\nthe 3D network by their own predictions. Both trials witness\\na performance drop, indicating Switchable Self-Training\\nStrategy is efficient in cross-modal self-supervised learning.\\nEffect of Sweep Numbers. Intuitively, the performance of\\nour method benefits from more sweeps information. There-\\nfore, we also show the performance when restricting sweep\\nsize to 1, 3, and 5, respectively. However, we observe that', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the performance of 5 sweeps is similar to 3 sweeps but is\\nmore computationally expensive. Thus, we empirically set\\nthe sweep number to be 3.\\nQualitative Evaluation. The qualitative evaluations of indi-\\nvidual classes (bus, motorcycle, car, and truck) are in Fig. 5,\\nindicating that our method is able to perceive the objects\\neven without training on any annotated data. However, we\\nalso observe the false positive predictions around the ground\\ntruth objects. We will resolve this issue in future work.4.2. Annotation-efficient Semantic Segmentation\\nThe pre-trained 3D network also boosts the performance\\nwhen few labeled data are available for training. We directly\\ncompare SLidR [51], the only published method for image-\\nto-Lidar self-supervised representation distillation. Besides,\\nwe also compared PPKT [44] and PointContrast [55]. In\\nthe following, we introduce SLidR and PPKT and compare\\nthem in detail.\\nPPKT. PPKT is a cross-modal self-supervised method for', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the RGB-D dataset. It performs 2D-to-3D knowledge distil-\\nlation via pixel-to-point contrastive loss. For a fair compari-\\nson, we use the same 3D network and training protocol but\\nreplace our semantic and Spatio-Temporal Regularization\\nwith InfoNCE loss. The framework is trained 50 epochs on\\n4096 randomly selected image-to-point pairs.\\nSLidR. SLidR is an image-to-Lidar self-supervised method\\nfor autonomous driving data. Compared with PPKT, it in-\\ntroduces image super-pixel into cross-modal self-supervised\\nlearning. For a fair comparison, we replace our loss function\\nwith their superpixel-driven contrastive loss.\\nPerformance. As shown in Table 1, our method significantly\\noutperforms the state-of-the-art methods when fine-tuned on\\n1% and 100% nuScenes dataset, with the improvement of\\n8.1% and 1.1%, respectively. Compared with the random ini-\\ntialization, the improvement is 14.1% and 2.4%, respectively,\\nindicating the efficiency of our Semantic-driven Cross-modal', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Contrastive Learning framework. The qualitative results are\\nshown in Fig. 6. Besides, we also verify the cross-domain\\ngeneralization ability of our method. When pre-training the\\n7026\\nInput Ground Truth SLidR OursFigure 6. Qualitative results of fine-tuning on 1% nuScenes dataset. From the first row to the last row are the input LiDAR scan, ground\\ntruth, prediction of SLidR, and our prediction, respectively. Note that we show the results by error map, where the red point indicates the\\nwrong prediction. Apparently, our method achieves decent performance.\\n3D network on the nuScenes dataset and fine-tuning on 1%\\nand 100% SemanticKITTI dataset, our method significantly\\noutperforms other state-of-the-art self-supervised methods.\\nDiscussions. PPKT and SLidR reveal that contrastive loss is\\npromising for transferring knowledge from image to point\\ncloud. Like self-supervised learning, constructing the pos-\\nitive and negative samples is vital to unsupervised cross-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='modal knowledge distillation. However, previous methods\\nsuffer from optimization-conflict issues, i.e., some negative\\npaired samples are actually positive pairs. For example, the\\nroad occupies a large proportion of the point cloud in a scene\\nand is supposed to have the same semantics in the semantic\\nsegmentation task. When randomly selecting training sam-\\nples, most negatively defined road-road points are actually\\npositive. When feedforwarding such samples into contrastive\\nlearning, the contrastive loss will push them away in the\\nembedding space, leading to unsatisfactory representation\\nlearning and hammering the downstream tasks’ performance.\\nSLidR introduces superpixel-driven contrastive learning to\\nalleviate such issues. The motivation is that the visual rep-\\nresentation of the image pixel and the projected points are\\nconsistent intra-superpixel. Although avoiding selecting neg-\\native image-point pairs from the same superpixel, the conflict', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='still exists inter-superpixel. In our CLIP2Scene, we intro-\\nduce the free-available dense pixel-text correspondence to\\nalleviate the optimization conflicts. The text embedding rep-\\nresents the semantic information and can be used to select\\nmore reasonable training samples for contrastive learning.Besides training sample selection, the previous method\\nalso ignores the temporal coherence of the multi-sweep point\\ncloud. That is, for LiDAR points mapping to the same image\\npixel, their feature is restricted to be consistent. Besides,\\nconsidering the calibration error between the LiDAR scan\\nand the camera image. We relax the pixel-to-point mapping\\nto image grid-to-point grid mapping for consistency regu-\\nlarization. To this end, our Spatial-temporal consistency\\nregularization leads to a more rational point representation.\\nLast but not least, we find that randomly switching the\\nsupervision signal benefits self-supervised learning. Essen-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tially, different modality networks learn different feature\\nrepresentations. They can filter different types of errors\\nintroduced by noisy pseudo labels. By this switchable opera-\\ntion, the error flows can be reduced mutually.\\n5. Conclusion\\nWe explored how CLIP knowledge benefits 3D scene\\nunderstanding in this work, termed CLIP2Scene. To ef-\\nficiently transfer CLIP’s image and text features to a 3D\\nnetwork, we propose a novel Semantic-driven Cross-modal\\nContrastive Learning framework including Semantic Regu-\\nlarization and Spatial-Temporal Regularization. For the first\\ntime, our pre-trained 3D network achieves annotation-free\\n3D semantic segmentation with decent performance. Be-\\nsides, our method significantly outperforms state-of-the-art\\nself-supervised methods when fine-tuning with labelled data.\\n7027\\nReferences\\n[1]Zeynep Akata, Florent Perronnin, Za ¨ıd Harchaoui, and\\nCordelia Schmid. Label-embedding for attribute-based classi-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='fication. In IEEE/CVF Conference on Computer Vision and\\nPattern Recognition , pages 819–826, 2013. 2\\n[2]Zeynep Akata, Scott E. Reed, Daniel Walter, Honglak Lee,\\nand Bernt Schiele. Evaluation of output embeddings for fine-\\ngrained image classification. In IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition , pages 2927–2936,\\n2015. 2\\n[3]Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel,\\nSven Behnke, Cyrill Stachniss, and Jurgen Gall. Semantickitti:\\nA dataset for semantic scene understanding of lidar sequences.\\nInIEEE/CVF International Conference on Computer Vision ,\\npages 9297–9307, 2019. 2, 5, 6\\n[4]Max Bucher, St ´ephane Herbin, and Fr ´ed´eric Jurie. Generat-\\ning visual representations for zero-shot classification. In\\nIEEE/CVF International Conference on Computer Vision\\nWorkshop , pages 2666–2673, 2017. 2\\n[5]Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,\\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal\\ndataset for autonomous driving. In IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pages 11621–\\n11631, 2020. 5\\n[6]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\\nend object detection with transformers. In European Confer-\\nence on Computer Vision , pages 213–229, 2020. 3\\n[7]Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-\\notr Bojanowski, and Armand Joulin. Unsupervised learning\\nof visual features by contrasting cluster assignments. In Ad-\\nvances in Neural Information Processing Systems , volume 33,\\npages 9912–9924, 2020. 2\\n[8]Soravit Changpinyo, Wei-Lun Chao, Boqing Gong, and Fei\\nSha. Synthesized classifiers for zero-shot learning. In\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 5327–5336, 2016. 2\\n[9]Nenglun Chen, Lingjie Liu, Zhiming Cui, Runnan Chen,\\nDuygu Ceylan, Changhe Tu, and Wenping Wang. Unsuper-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='vised learning of intrinsic structural representation points.\\nInIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 9121–9130, 2020. 2\\n[10] Runnan Chen, Zhu Xinge, Nenglun Chen, Dawei Wang, Wei\\nLi, Yuexin Ma, Ruigang Yang, and Wenping Wang. Referring\\nself-supervised learning on 3d point cloud. 2021. 2\\n[11] Runnan Chen, Xinge Zhu, Nenglun Chen, Wei Li, Yuexin Ma,\\nRuigang Yang, and Wenping Wang. Zero-shot point cloud\\nsegmentation by transferring geometric primitives. arXiv\\npreprint arXiv:2210.09923 , 2022. 1, 2\\n[12] Runnan Chen, Xinge Zhu, Nenglun Chen, Dawei Wang, Wei\\nLi, Yuexin Ma, Ruigang Yang, and Wenping Wang. Towards\\n3d scene understanding by referring synthetic models. arXiv\\npreprint arXiv:2203.10546 , 2022. 2\\n[13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\\noffrey Hinton. A simple framework for contrastive learningof visual representations. In International Conference on\\nMachine Learning , pages 1597–1607, 2020. 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[14] Xinlei Chen and Kaiming He. Exploring simple siamese rep-\\nresentation learning. In IEEE/CVF Conference on Computer\\nVision and Pattern Recognition , pages 15750–15758, 2021. 2\\n[15] Ran Cheng, Ryan Razani, Ehsan Taghavi, Enxu Li, and Bing-\\nbing Liu. (af)2-s3net: Attentive feature fusion with adaptive\\nfeature selection for sparse semantic segmentation network.\\nInIEEE Conference on Computer Vision and Pattern Recog-\\nnition , pages 12547–12556, 2021. 1\\n[16] Ali Cheraghian, Shafin Rahman, Dylan Campbell, and Lars\\nPetersson. Mitigating the Hubness Problem for Zero-Shot\\nLearning of 3D Objects. arXiv preprint arXiv:1907.06371 ,\\n2019. 2\\n[17] Ali Cheraghian, Shafin Rahman, Townim F Chowdhury, Dy-\\nlan Campbell, and Lars Petersson. Zero-shot learning on 3d\\npoint cloud objects and beyond. International Journal of\\nComputer Vision , 130(10):2364–2384, 2022. 2\\n[18] Ali Cheraghian, Shafin Rahman, and Lars Petersson. Zero-\\nShot Learning of 3D Point Cloud Objects. In IEEE Inter-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='national Conference on Machine Vision Applications , pages\\n1–6, 2019. 2\\n[19] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d\\nspatio-temporal convnets: Minkowski convolutional neural\\nnetworks. In IEEE/CVF Conference on Computer Vision and\\nPattern Recognition , pages 3075–3084, 2019. 6\\n[20] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber,\\nThomas Funkhouser, and Matthias Nießner. Scannet: Richly-\\nannotated 3d reconstructions of indoor scenes. In IEEE/CVF\\nConference on Computer Vision and Pattern Recognition ,\\npages 5828–5839, 2017. 2, 5, 6\\n[21] Berkan Demirel, Ramazan Gokberk Cinbis, and Nazli Ikizler-\\nCinbis. Attributes2classname: A discriminative model\\nfor attribute-based unsupervised zero-shot learning. In\\nIEEE/CVF International Conference on Computer Vision ,\\npages 1241–1250, 2017. 2\\n[22] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsu-\\npervised visual representation learning by context prediction.\\nInIEEE/CVF International Conference on Computer Vision ,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='pages 1422–1430, 2015. 2\\n[23] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Ried-\\nmiller, and Thomas Brox. Discriminative unsupervised fea-\\nture learning with convolutional neural networks. In Advances\\nin Neural Information Processing Systems , volume 27, 2014.\\n2\\n[24] Whye Kit Fong, Rohit Mohan, Juana Valeria Hurtado, Lubing\\nZhou, Holger Caesar, Oscar Beijbom, and Abhinav Valada.\\nPanoptic nuscenes: A large-scale benchmark for lidar panop-\\ntic segmentation and tracking. IEEE Robotics and Automation\\nLetters , 7:3795–3802, 2022. 2, 5, 6\\n[25] Chuang Gan, Ming Lin, Yi Yang, Yueting Zhuang, and\\nAlexander Hauptmann. Exploring semantic inter-class re-\\nlationships (sir) for zero-shot action recognition. In AAAI\\nConference on Artificial Intelligence , 2015. 2\\n[26] Biao Gao, Yancheng Pan, Chengkun Li, Sibo Geng, and\\nHuijing Zhao. Are we hungry for 3d lidar data for seman-\\ntic segmentation? a survey of datasets and methods. IEEE\\nTransactions on Intelligent Transportation Systems , 2021. 1', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='7028\\n[27] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin\\nTallec, Pierre Richemond, Elena Buchatskaya, Carl Doer-\\nsch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-\\nlaghi Azar, et al. Bootstrap your own latent-a new approach to\\nself-supervised learning. In Advances in Neural Information\\nProcessing Systems , volume 33, pages 21271–21284, 2020. 2\\n[28] Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu,\\nand Mohammed Bennamoun. Deep learning for 3d point\\nclouds: A survey. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence , 43(12):4338–4364, 2020. 1\\n[29] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu,\\nWeihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching:\\nRobust training of deep neural networks with extremely noisy\\nlabels. Advances in neural information processing systems ,\\n31, 2018. 5\\n[30] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\\nGirshick. Momentum contrast for unsupervised visual repre-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='sentation learning. In IEEE/CVF Conference on Computer\\nVision and Pattern Recognition , pages 9729–9738, 2020. 2\\n[31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In IEEE/CVF\\nConference on Computer Vision and Pattern Recognition ,\\npages 770–778, 2016. 3\\n[32] Fangzhou Hong, Lingdong Kong, Hui Zhou, Xinge Zhu,\\nHongsheng Li, and Ziwei Liu. Unified 3d and 4d panoptic\\nsegmentation via dynamic shifting network. arXiv preprint\\narXiv:2203.07186 , 2022. 1\\n[33] Yuenan Hou, Xinge Zhu, Yuexin Ma, Chen Change Loy,\\nand Yikang Li. Point-to-Voxel Knowledge Distillation for\\nLiDAR Semantic Segmentation. In IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition , pages 8479–8488,\\n2022. 1\\n[34] Siyuan Huang, Yichen Xie, Song-Chun Zhu, and Yixin Zhu.\\nSpatio-temporal self-supervised representation learning for\\n3d point clouds. In IEEE/CVF International Conference on\\nComputer Vision , pages 6535–6545, 2021. 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[35] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf\\non a diet: Semantically consistent few-shot view synthesis.\\nInIEEE/CVF International Conference on Computer Vision ,\\npages 5885–5894, 2021. 2\\n[36] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann.\\nDecomposing nerf for editing via feature field distillation.\\narXiv preprint arXiv:2205.15585 , 2022. 2\\n[37] Elyor Kodirov, Tao Xiang, and Shaogang Gong. Semantic\\nautoencoder for zero-shot learning. In IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pages 4447–\\n4456, 2017. 2\\n[38] Lingdong Kong, Youquan Liu, Runnan Chen, Yuexin Ma,\\nXinge Zhu, Yikang Li, Yuenan Hou, Yu Qiao, and Ziwei Liu.\\nRethinking range view representation for lidar segmentation.\\narXiv preprint arXiv:2303.05367 , 2023. 1\\n[39] Lingdong Kong, Niamul Quader, and Venice Erin Liong.\\nConda: Unsupervised domain adaptation for lidar segmen-\\ntation via regularized domain concatenation. In IEEE In-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ternational Conference on Robotics and Automation , 2023.\\n1\\n[40] Lingdong Kong, Jiawei Ren, Liang Pan, and Ziwei Liu.\\nLasermix for semi-supervised lidar semantic segmentation.InIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , 2023. 1\\n[41] Christoph H. Lampert, Hannes Nickisch, and Stefan Harmel-\\ning. Learning to detect unseen object classes by between-class\\nattribute transfer. In IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition , pages 951–958, 2009. 2\\n[42] Christoph H. Lampert, Hannes Nickisch, and Stefan Harmel-\\ning. Attribute-based classification for zero-shot visual object\\ncategorization. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence , 36:453–465, 2014. 2\\n[43] Yan Li, Zhen Jia, Junge Zhang, Kaiqi Huang, and Tieniu Tan.\\nDeep semantic structural constraints for zero-shot learning.\\nInAAAI Conference on Artificial Intelligence , 2018. 2\\n[44] Yueh-Cheng Liu, Yu-Kai Huang, Hung-Yueh Chiang, Hung-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Ting Su, Zhe-Yu Liu, Chin-Tang Chen, Ching-Yu Tseng, and\\nWinston H Hsu. Learning from 2d: Contrastive pixel-to-\\npoint knowledge transfer for 3d pretraining. arXiv preprint\\narXiv:2104.04687 , 2021. 1, 2, 3, 4, 6, 7\\n[45] Bj¨orn Michele, Alexandre Boulch, Gilles Puy, Maxime\\nBucher, and Renaud Marlet. Generative zero-shot learning for\\nsemantic segmentation of 3d point clouds. In International\\nConference on 3D Vision , pages 992–1002, 2021. 1, 2\\n[46] Ashish Mishra, M. Shiva Krishna Reddy, Anurag Mittal, and\\nHema A. Murthy. A generative model for zero shot learning\\nusing conditional variational autoencoders. In IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\nWorkshop , pages 2269–22698, 2018. 2\\n[47] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\\nPointnet: Deep learning on point sets for 3d classification and\\nsegmentation. In IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pages 652–660, 2017. 1, 2', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\\ntransferable visual models from natural language supervision.\\nInInternational Conference on Machine Learning , pages\\n8748–8763. PMLR, 2021. 1\\n[49] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong\\nTang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu.\\nDenseclip: Language-guided dense prediction with context-\\naware prompting. In IEEE/CVF Conference on Computer\\nVision and Pattern Recognition , pages 18082–18091, 2022. 2\\n[50] David Rozenberszki, Or Litany, and Angela Dai. Language-\\ngrounded indoor 3d semantic segmentation in the wild. In\\nEuropean Conference on Computer Vision , pages 125–141,\\n2022. 2\\n[51] Corentin Sautier, Gilles Puy, Spyros Gidaris, Alexandre\\nBoulch, Andrei Bursuc, and Renaud Marlet. Image-to-\\nlidar self-supervised distillation for autonomous driving data.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='InIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 9891–9901, 2022. 1, 2, 3, 4, 5, 6, 7\\n[52] Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin,\\nHanrui Wang, and Song Han. Searching efficient 3d archi-\\ntectures with sparse point-voxel convolution. In European\\nConference on Computer Vision , pages 685–702, 2020. 6\\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\n7029\\nPolosukhin. Attention is all you need. In Advances in Neural\\nInformation Processing Systems , volume 30, 2017. 3\\n[54] Yongqin Xian, Zeynep Akata, Gaurav Sharma, Quynh N.\\nNguyen, Matthias Hein, and Bernt Schiele. Latent embed-\\ndings for zero-shot classification. In IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pages 69–77,\\n2016. 2\\n[55] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas\\nGuibas, and Or Litany. Pointcontrast: Unsupervised pre-\\ntraining for 3d point cloud understanding. In European Con-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ference on Computer Vision , pages 574–591, 2020. 2, 6,\\n7\\n[56] Jianyun Xu, Ruixiang Zhang, Jian Dou, Yushi Zhu, Jie Sun,\\nand Shiliang Pu. Rpvnet: A deep and efficient range-point-\\nvoxel fusion network for lidar point cloud segmentation. In\\nIEEE/CVF International Conference on Computer Vision ,\\npages 16024–16033, 2021. 1\\n[57] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang,\\nDan Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang\\nXu. Detclip: Dictionary-enriched visual-concept paral-\\nleled pre-training for open-world detection. arXiv preprint\\narXiv:2209.09407 , 2022. 2, 3\\n[58] ´Eloi Zablocki, Patrick Bordes, Benjamin Piwowarski, Laure\\nSoulier, and Patrick Gallinari. Context-aware zero-shot learn-\\ning for object recognition. In International Conference on\\nMachine Learning , pages 7292–7303, 2019. 2\\n[59] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng\\nMiao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li. Point-\\nclip: Point cloud understanding by clip. In IEEE/CVF Con-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ference on Computer Vision and Pattern Recognition , pages\\n8552–8562, 2022. 2\\n[60] Zaiwei Zhang, Rohit Girdhar, Armand Joulin, and Ishan\\nMisra. Self-supervised pretraining of 3d features on any point-\\ncloud. In IEEE/CVF International Conference on Computer\\nVision , pages 10252–10263, 2021. 2\\n[61] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free\\ndense labels from clip. In European Conference on Computer\\nVision , pages 696–712, 2022. 2, 4, 6\\n[62] Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, Yuexin\\nMa, Wei Li, Hongsheng Li, and Dahua Lin. Cylindrical and\\nasymmetrical 3d convolution networks for lidar segmentation.\\nInIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 9939–9948, 2021. 1\\n7030', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='itKD: Interchange Transfer-based Knowledge Distillation for 3D Object\\nDetection\\nHyeon Cho1, Junyong Choi1,2, Geonwoo Baek1, and Wonjun Hwang1,3\\n1Ajou University,2Hyundai Motor Company,3Naver AI Lab\\nch0104@ajou.ac.kr, chldusxkr@hyundai.com, bkw0622@ajou.ac.kr, wjhwang@ajou.ac.kr\\nAbstract\\nPoint-cloud based 3D object detectors recently have\\nachieved remarkable progress. However, most studies are\\nlimited to the development of network architectures for im-\\nproving only their accuracy without consideration of the\\ncomputational efficiency. In this paper, we first propose\\nan autoencoder-style framework comprising channel-wise\\ncompression and decompression via interchange transfer-\\nbased knowledge distillation. To learn the map-view feature\\nof a teacher network, the features from teacher and student\\nnetworks are independently passed through the shared au-\\ntoencoder; here, we use a compressed representation loss\\nthat binds the channel-wised compression knowledge from', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='both student and teacher networks as a kind of regulariza-\\ntion. The decompressed features are transferred in opposite\\ndirections to reduce the gap in the interchange reconstruc-\\ntions. Lastly, we present an head attention loss to match the\\n3D object detection information drawn by the multi-head\\nself-attention mechanism. Through extensive experiments,\\nwe verify that our method can train the lightweight model\\nthat is well-aligned with the 3D point cloud detection task\\nand we demonstrate its superiority using the well-known\\npublic datasets; e.g., Waymo and nuScenes.1\\n1. Introduction\\nConvolutional neural network (CNN)-based 3D object\\ndetection methods using point clouds [13] [35] [36] [43]\\n[49] have attracted wide attention based on their outstand-\\ning performance for self-driving cars. Recent CNN-based\\nworks have required more computational complexity to\\nachieve higher precision under the various wild situation.\\nSome studies [23] [36] [43] have proposed methods to im-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='prove the speed of 3D object detection through which the\\nnon-maximum suppression (NMS) or anchor procedures\\nare removed but the network parameters are still large.\\n1Our code is available at https://github.com/hyeon-jo/interchange-\\ntransfer-KD.\\nFigure 1. Performance comparison between teacher and stu-\\ndent networks for a point-cloud based 3D object detection. The\\ntop example images are qualitatively compared between the results\\nof teacher, student and our networks. Specifically, the first row im-\\nages are an input sample with labels and the center heatmap head\\nof the teacher network. The second row examples are responses\\nof teacher, student, and ours for the yellow circle on the heatmap\\n(or the blue dash circle on the input). The bottom image quantita-\\ntively shows the computational complexity and the corresponding\\naccuracy of teacher, student and our networks, respectively. Best\\nviewed in color.\\nKnowledge distillation (KD) is one of the parameter', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='compression techniques, which can effectively train a com-\\npact student network through the guidance of a deep teacher\\nnetwork, as shown in the example images of Fig. 1. Starting\\nwith Hinton’s work [9], many KD studies [10] [20] [28] [44]\\nhave transferred the discriminative teacher knowledge to the\\nstudent network for classification tasks. From the viewpoint\\nof the detection task, KD should be extended to the regres-\\nsion problem, including the object locations, which is not\\neasy to straight-forwardly apply the classification-based KD\\nmethods to the detection task. To alleviate this problem, KD\\nmethods for object detection have been developed for mim-\\nicking the output of the backbone network [15] ( e.g., region\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n13540', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='13540\\nproposal network) or individual detection head [2] [32].\\nNevertheless, these methods have only been studied for de-\\ntecting 2D image-based objects, and there is a limit to ap-\\nplying them to sparse 3D point cloud-based data that have\\nnot object-specific color information but only 3D position-\\nbased object structure information.\\nTaking a closer look at differences between 2D and 3D\\ndata, there is a large gap in that 2D object detection usually\\npredicts 2D object locations based on inherent color infor-\\nmation with the corresponding appearances, but 3D object\\ndetection estimates 3D object boxes from inputs consist-\\ning of only 3D point clouds. Moreover, the number of the\\npoint clouds constituting objects varies depending on the\\ndistances and presence of occlusions [42]. Another chal-\\nlenge in 3D object detection for KD is that, compared to\\n2D object detection, 3D object detection methods [4] [6]\\n[43] [21] have more detection head components such as 3D', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='boxes, and orientations. These detection heads are highly\\ncorrelated with each other and represent different 3D char-\\nacteristics. In this respect, when transferring the detection\\nheads of the teacher network to the student network using\\nKD, it is required to guide the distilled knowledge under the\\nconsideration of the correlation among the multiple detec-\\ntion head components.\\nIn this paper, we propose a novel interchange transfer-\\nbased KD (itKD) method designed for the lightweight\\npoint-cloud based 3D object detection. The proposed itKD\\ncomprises two modules: (1) a channel-wise autoencoder\\nbased on the interchange transfer of reconstructed knowl-\\nedge and (2) a head relation-aware self-attention on multi-\\nple 3D detection heads. First of all, through a channel-wise\\ncompressing and decompressing processes for KD, the in-\\nterchange transfer-based autoencoder effectively represents\\nthe map-view features from the viewpoint of 3D representa-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tion centric-knowledge. Specifically, the encoder provides\\nan efficient representation by compressing the map-view\\nfeature in the channel direction to preserve the spatial po-\\nsitions of the objects and the learning of the student net-\\nwork could be regularized by the distilled position infor-\\nmation of objects in the teacher network. For transferring\\nthe interchange knowledge to the opposite networks, the\\ndecoder of the student network reconstructs the map-view\\nfeature under the guidance of the teacher network while\\nthe reconstruction of the teacher network is guided by the\\nmap-view feature of the student network. As a result, the\\nstudent network can effectively learn how to represent the\\n3D map-view feature of the teacher. Furthermore, to refine\\nthe teacher’s object detection results as well as its repre-\\nsentation, our proposed head relation-aware self-attention\\ngives a chance to learn the pivotal information that should\\nbe taught to the student network for improving the 3D de-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tection results by considering the inter-head relation among\\nthe multiple detection head and the intra-head relation ofthe individual detection head.\\nIn this way, we implement a unified KD framework to\\nsuccessfully learn the 3D representation and 3D detection\\nresults of the teacher network for the lightweight 3D point\\ncloud object detection. We also conduct extensive ablation\\nstudies for thoroughly validating our approach in Waymo\\nand nuScenes datasets. The results reveal the outstanding\\npotential of our approach for transferring distilled knowl-\\nedge that can be utilized to improve the performance of 3D\\npoint cloud object detection models.\\nOur contributions are summarized as follows:\\n• For learning the 3D representation-centric knowledge\\nfrom the teacher network, we propose the channel-\\nwise autoencoder regularized in the compressed do-\\nmain and the interchange knowledge transfer method\\nwherein the reconstructed features are guided by the\\nopposite networks.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='opposite networks.\\n• For detection head-centric knowledge of the teacher,\\nwe suggest the head relation-aware self-attention\\nwhich can efficiently distill the detection properties\\nunder the consideration of the inter-head relation and\\nintra-head relation of the multiple 3D detection heads.\\n• Our work is the best attempt to reduce the parame-\\nters of point cloud-based 3D object detection using\\nKD. Additionally, we validate its superiority using\\ntwo large datasets that reflect real-world driving con-\\nditions, e.g., Waymo and NuScenes.\\n2. Related Works\\n2.1. 3D Object Detection based on Point Cloud\\nDuring the last few years, encouraged by the success\\nof CNNs, the development of object detectors using CNNs\\nis developing rapidly. Recently, many 3D object detectors\\nhave been studied and they can be briefly categorized by\\nhow they extract representations from point clouds; e.g.,\\ngrid-based [35] [36] [49] [13] [43], point-based [18] [23]', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[17] [25] [39] and hybrid-based [3] [40] [8] [48] [22] meth-\\nods. In detail, V ote3Deep [5] thoroughly exploited feature-\\ncentric voting to build CNNs for detecting objects in point\\nclouds. In [29], they have studied on the task of amodal\\n3D object detection in RGB-D images, where a 3D region\\nproposal network (RPN) to learn objectness from geomet-\\nric shapes and the joint object recognition network to extract\\ngeometric features in 3D and color features in 2D. The 3D\\nfully convolutional network [14] was straightforwardly ap-\\nplied to point cloud data for vehicle detection. In the early\\ndays, V oxelNet [49] has designed an end-to-end trainable\\ndetector based on learning-based voxelization using fully\\nconnected layers. In [35], they encoded the point cloud by\\nV oxelNet and used the sparse convolution to achieve the fast\\ndetection. HVNet [41] fused the multi-scale voxel feature\\nencoder at the point-wise level and projected into multi-\\n13541', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='13541\\nple pseudo-image feature maps for solving the various sizes\\nof the feature map. In [26], they replaced the point cloud\\nwith a grid-based bird’s-eye view (BEV) RGB-map and uti-\\nlized YOLOv2 to detect the 3D objects. PIXOR [36] con-\\nverted the point cloud to a 3D BEV map and carried out the\\nreal-time 3D object detection with an RPN-free single-stage\\nbased model.\\nRecently, PointPillars (PP)-based method [13] utilized\\nthe PointNet [19] to learn the representation of point clouds\\norganized in vertical columns for achieving the fast 3D ob-\\nject detection. To boost both performance and speed over\\nPP, a pillar-based method [33] that incorporated a cylin-\\ndrical projection into multi-view feature learning was pro-\\nposed. More recently, CenterPoint [43] was introduced as\\nan anchor-free detector that predicted the center of an ob-\\nject using a PP or V oxelNet-based feature encoder. In this\\npaper, we employ the backbone architecture using Center-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Point because it is simple, near real-time, and achieves good\\nperformance in the wild situation.\\n2.2. Knowledge Distillation\\nKD is one of the methods used for compressing deep\\nneural networks and its fundamental key is to imitate the\\nknowledge extracted from the teacher network, which has\\nheavy parameters as well as good accuracy. Hinton et\\nal. [9] performed a knowledge transfer using KL diver-\\ngence; FitNet [20] proposed a method for teaching student\\nnetworks by imitating intermediate layers. On the other\\nhand, TAKD [16] and DGKD [28] used multiple teacher\\nnetworks for transferring more knowledge to the student\\nnetwork in spite of large parameter gaps. Recently, some\\nstudies have been proposed using the layers shared between\\nthe teacher and the student networks for KD. Specifically,\\nin [37], KD was performed through softmax regression as\\nthe student and teacher networks shared the same classi-\\nfier. IEKD [10] proposed a method to split the student net-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='work into inheritance and exploration parts and mimic the\\ncompact teacher knowledge through a shared latent feature\\nspace via an autoencoder.\\nBeyond its use in classification, KD for detection should\\ntransfer the regression knowledge regarding the positions of\\nthe objects to the student network. For this purpose, a KD\\nfor 2D object detection [15] was first proposed using fea-\\nture map mimic learning. In [2], they transferred the detec-\\ntion knowledge of the teacher network using hint learning\\nfor an RPN, weighted cross-entropy loss for classification,\\nand bound regression loss for regression. Recently, Wang et\\nal. [32] proposed a KD framework for detection by utilizing\\nthe cross-location discrepancy of feature responses through\\nfine-grained feature imitation.\\nAs far as we know, there are few KD studies [7] [47]\\n[34] [38] on point cloud-based 3D object detection so far.\\nHowever, looking at similar studies on 3D knowledge trans-fer, SE-SSD [47] presented a knowledge distillation-based', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='self-ensembling method for exploiting soft and hard tar-\\ngets with constraints to jointly optimize the model with-\\nout extra computational cost during inference time. Object-\\nDGCNN [34] proposed a NMS-free 3D object detection via\\ndynamic graphs and a set-to-set distillation. They used the\\nset-to-set distillation method for improving the performance\\nwithout the consideration of the model compression. An-\\nother latest study is SparseKD [38] which suggested a label\\nKD method that distills a few pivotal positions determined\\nby teacher classification response to enhance the logit KD\\nmethod. On the other hand, in this paper, we are more in-\\nterest in how to make the student network lighter, or lower\\ncomputational complexity, by using the KD for 3D object\\ndetection.\\n3. Methodology\\n3.1. Background\\nThe 3D point cloud object detection methods [13] [49]\\ngenerally consists of three components; a point cloud en-\\ncoder, a backbone network, and detection heads. In this', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='paper, we employ CenterPoint [43] network as a backbone\\narchitecture. Since the parameter size of the backbone net-\\nwork2is the largest among components of the 3D object\\ndetector, we aim to construct the student network by re-\\nducing the channel sizes of the backbone network for ef-\\nficient network. We design our method to teach the student\\n3D representation-centric knowledge and detection head-\\ncentric knowledge of the teacher network, respectively.\\n3.2. Interchange Transfer\\nWe adopt an autoencoder framework to effectively trans-\\nfer the meaningful distilled knowledge regarding 3D detec-\\ntion from the teacher to the student network. The traditional\\nencoder-based KD methods [10] [11] have been limited to\\nthe classification task, which transfers only compressed cat-\\negorical knowledge to the student network. However, from\\nthe viewpoint of the detection task, the main KD goal of\\nthis paper is transferring the distilled knowledge regarding', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='not only categorical features but also object location-related\\nfeatures. Particularly, unlike 2D detectors, 3D object detec-\\ntors should regress more location information such as object\\norientations, 3D box sizes, etc., and it results in increasing\\nthe importance of how to transfer the 3D location features\\nto the student network successfully.\\nFor this purpose, we transfer the backbone knowledge\\nthat contains 3D object representation from the teacher net-\\nwork to the student through the compressed and recon-\\nstructed knowledge domains. As shown in Fig. 2, we in-\\n2The total parameter size of the 3D detector is about 5.2M and the\\nbackbone size is approximately 4.8M, which is 92%. Further details are\\nfound in the supplementary material.\\n13542\\nFigure 2. Overview of the proposed knowledge distillation method. The teacher and student networks take the same point clouds as', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='inputs. Then, the map-view features MtandMsare extracted from the teacher and student networks, respectively. The channel-wise\\nautoencoder transfers the knowledge obtained from MttoMsby using the compressed representation loss and interchange transfer loss\\nconsecutively. The head relation-aware self-attention provides the relation-aware knowledge of multiple detection head to the student\\nnetwork using the attention head loss. The dotted lines of the modules denote that there are shared network parameters between the teacher\\nand student networks. The light-yellow boxes are buffer layers for sampling the features to match the channel sizes of networks.\\ntroduce a channel-wise autoencoder which consists of an\\nencoder in which the channel dimension of the autoencoder\\nis gradually decreased and a decoder in the form of increas-\\ning the channel dimension. Note that spatial features play\\na pivotal role in the detection task and we try to preserve', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='the spatial information by encoding features in the channel\\ndirection. We propose a compressed representation loss to\\ncoarsely guide location information of the objects to the stu-\\ndent network in Fig. 2, and the compressed representation\\nloss has an effect similar to the regularization of the autoen-\\ncoder that binds the coordinates of the objectness between\\nthe teacher and student networks. The compressed repre-\\nsentation loss function Lcris represented as follows:\\nLcr=mobj◦ S[E(θenc, Mt), E(θenc, Ms)]\\n=mobj◦ S[Mt\\nenc, Ms\\nenc],(1)\\nwhere Eis a shared encoder, which has the parameters θenc,\\nandSdenotes l1loss as a similarity measure. MtandMs\\nare outputs of the teacher and student backbones, respec-\\ntively. mobjrepresents a binary mask to indicate object lo-\\ncations in backbone output like [38] and ◦is an element-\\nwise product.\\nAfter performing the coarse representation-based knowl-\\nedge distillation in a compressed domain, the fine represen-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tation features of the teacher network are required to teach\\nthe student network from the viewpoint of 3D object detec-\\ntion. In this respect, the decoder reconstructs the fine map-view features in the channel direction from the compressed\\nfeatures. Through the proposed interchange transfer loss,\\nthe reconstructed features are guided from the opposite net-\\nworks, not their own stem networks, as shown in Fig. 2.\\nSpecifically, since the teacher network is frozen and we use\\nthe shared autoencoder for both student and teacher net-\\nworks, we can teach the reconstructed fine features from the\\nstudent network to resemble the output of the teacher net-\\nwork Mtrather than the student Ms. Moreover, the recon-\\nstructed fine features from the teacher network can guide\\nthe student’s output, Msat the same time. The proposed\\ninterchange transfer loss Litis defined as follows:\\nLt→s=S[Ms, D(θdec, Mt\\nenc)], (2)\\nLs→t=S[Mt, D(θdec, Ms\\nenc)], (3)\\nLit=Ls→t+Lt→s, (4)', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Lit=Ls→t+Lt→s, (4)\\nwhere Dis the decoder that contains the network param-\\neterθdec, which is a shared parameter. We hereby present\\nthe representation-based KD for 3D object detection in both\\ncompressed and decompressed domains to guide the stu-\\ndent network to learn the map-view feature of the teacher\\nnetwork efficiently.\\n3.3. Head Relation-Aware Self-Attention\\nFundamentally, our backbone network, e.g., Center-\\nPoint [43], has various types of 3D object characteristics\\n13543\\nFigure 3. Head Relation-Aware Self-Attention. We make the object center-head feature from object center locations in the detection\\nhead feature and use it as different shaped inputs to self-attentions for inter-head relation and intra-head relation. In the self-attention for\\ninter-head relation, we use the object center-head feature as an input for the self-attention. In the self-attention for intra-head relation, the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='detection heads are separately used for the independent self-attention functions. The outputs of the self-attentions are concatenated by c⃝\\noperations and the head relation-aware self-attention is generated through the fusion layer.\\non detection heads. Specifically, the locations, size, and di-\\nrection of an object are different properties, but they are in-\\nevitably correlated to each other because they come from\\nthe same object. However, the traditional KD methods [2]\\n[34] were only concerned with how the student network\\nstraight-forwardly mimicked the outputs of the teacher net-\\nwork without considering the relation among the detection\\nheads. To overcome this problem, we make use of the re-\\nlation of detection heads as a major factor for the detection\\nhead-centric KD.\\nOur proposed head relation-aware self-attention is di-\\nrectly inspired by the multi-head self-attention [31] in order\\nto learn the relation between the multiple detection head.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='As shown in Fig. 3, we first extract i-th instance feature\\nvi∈Rc, where cis the channel size, from the center loca-\\ntion of the object in the detection head feature. Note that,\\nsince the instance feature is extracted from the multiple de-\\ntection head, it has several object properties such as a class-\\nspecific heatmap vi\\nhm, a sub-voxel location refinement vi\\no,\\na height-above-ground vi\\nh, a 3D size vi\\ns, and a yaw rotation\\nangle vi\\nr. When there are a total of nobjects, we combine\\nthem to make an object center-head feature v∈Rn×c. We\\nuse the same object center-head feature vof dimension n\\nfor query, key, and value, which are an input of the scaled\\ndot-product attention. The self-attention function Fis com-\\nputed by\\nF(v) =softmax (v⊤·v√n)·v. (5)\\nThe proposed head relation-aware self-attention consists\\nof two different self-attentions for inter-head and intra-head\\nrelations as illustrated in Fig. 3. We propose the self-\\nattention based on the inter-head relation of the instance', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='features, which is made in order to consider the relationbetween all detected objects and their different properties,\\nrather than a single detected instance, from the global view-\\npoint. The self-attention for inter-head relation is computed\\nby\\nFinter(v) =F([vhm, vo, vh, vs, vr]). (6)\\nOn the other hand, we suggest the self-attention for intra-\\nhead relation using the individual detection heads. Here we\\nperform the attentions using only local relation in individ-\\nual detection heads designed for different properties (e.g.,\\norientation, size, etc.) and concatenate them. Its equation is\\nFintra(v) = [F(vhm),F(vo),F(vh),F(vs),F(vr)].(7)\\nWe concatenate the outputs of the self-attentions and ap-\\nply the fusion layer to calculate a final attention score that\\nconsiders the relation between the detection heads and ob-\\njects. The head relation-aware self-attention equation FRA\\nis derived by:\\nFRA(v) =G([Finter(v),Fintra(v)]), (8)\\nwhereGis the fusion layer, e.g., 1 ×1 convolution layer. The', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='student network indirectly takes the teacher’s knowledge by\\nlearning the relation between the multiple detection head of\\nthe teacher network through head attention loss as follows:\\nLattn=S(FRA(vt),FRA(vs)), (9)\\nwhere vtandvsare the object center-head features of the\\nteacher and the student, respectively.\\nConsequently, the overall loss is derived by\\nLtotal=αLsup+β(Lit+Lcr+Lattn), (10)\\nwhere Lsupis the supervised loss that consists of focal loss\\nand regression loss, and αandβare the balancing parame-\\nters, which we set as 1 for simplicity.\\n13544\\n4. Experimental Results and Discussions\\n4.1. Environment Settings\\nWaymo Waymo open dataset [30] is one of the large-\\nscale datasets for autonomous driving, which is captured\\nby the synchronized and calibrated high-quality LiDAR and\\ncamera across a range of urban and suburban geographies.\\nThis dataset provides 798 training scenes and 202 validation\\nscenes obtained by detecting all the objects within a 75m', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='radius; it has a total of 3 object categories ( e.g., vehicle,\\npedestrian, and cyclist) which have 6.1M, 2.8M, and 67K\\nsets, respectively. The mean Average Precision (mAP) and\\nmAP weighted by heading accuracy (mAPH) are the official\\nmetrics for Waymo evaluation. mAPH is a metric that gives\\nmore weight to the heading than it does to the sizes, and it\\naccounts for the direction of the object.\\nnuScenes nuScenes dataset [1] is another large-scale\\ndataset used for autonomous driving. This dataset contains\\n1,000 driving sequences. 700, 150, and 150 sequences are\\nused for training, validation, and testing, respectively. Each\\nsequence is captured approximately 20 seconds with 20 FPS\\nusing the 32-lane LiDAR. Its evaluation metrics are the av-\\nerage precision (AP) and nuScenes detection score (NDS).\\nNDS is a weighted average of mAP and true positive met-\\nrics which measures the quality of the detections in terms\\nof box location, size, orientation, attributes, and velocity.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Implementation details Following the pillar-based Cen-\\nterPoint [43] as the teacher network, we use an Adam op-\\ntimizer [12] with a weight decay of 0.01 and a cosine an-\\nnealing strategy [27] to adjust the learning rate. We set\\n0.0003 for initial learning rate, 0.003 for max learning rate,\\nand 0.95 for momentum. The networks have been trained\\nfor 36 epochs on 8 ×V100 GPUs with a batch size of 32.\\nFor Waymo dataset, we set the detection range to [-74.88m,\\n74.88m] for the X and Y axes, [-2m, 4m] for the Z-axis, and\\na grid size of (0.32m, 0.32m). In experiments on nuScenes\\ndataset, we used a (0.2m, 0.2m) grid and set the detection\\nrange to [-51.2m, 51.2m] for the X and Y-axes, [-5m, 3m]\\nfor the Z-axis, and a grid size of (0.2m, 0.2m). Compared\\nto the teacher network, the student network has 1/4less\\nchannel capacity of backbone network. Our channel-wise\\nautoencoder consists of three 1 ×1 convolution layers as the\\nencoder and three 1 ×1 convolution layers as the decoder', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='and the number of filters are 128, 64, 32 in encoder lay-\\ners and 64, 128, 384 in decoder layers. The student’s input\\nbuffer layer increases the channel size of 196 to 384 and the\\nteacher’s output buffer layer decreases the channel size 384\\nto 196.\\n4.2. Overall KD Performance Comparison\\nWe validate the performance of our method compared\\nwith well-known KD methods on the Waymo and nuScenes\\ndatasets. We re-implement the seven KD methods from 2Dclassification-based KD to 3D detection-based KD in this\\npaper. We set the baseline by applying the Kullback-Leibler\\n(KL) divergence loss [9] to the center heatmap head and l1\\nloss to the other regression heads. FitNet [20] is a method\\nthat mimics the intermediate outputs of layers and we ap-\\nply it to the output of the backbone for simplicity. We also\\nsimply extend EOD-KD [2], one of the 2D object detec-\\ntion KDs, to 3D object detection. We apply TOFD [45],\\na 3D classification-based KD, to our detection task and', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='straight-forwardly use SE-SSD [47], Object DGCNN [34],\\nand SparseKD [38] for 3D object detection KD.\\nTable 1 shows that our method almost outperforms other\\nKD methods on mAP and mAPH values for level 1 and\\nlevel 2 under all three categories of objects. Especially,\\nour performance improvement of mAPH is better than other\\nmethods, which indicates our method guides the student\\nnetwork well where the detected objects are facing. To\\nverify the generality of the proposed method, we make\\nadditional comparison results using the nuScenes dataset,\\nanother large-scale 3D dataset for autonomous driving, in\\nTable 2. Compared with the other methods, our method\\nachieves the best accuracy under the NDS and mAP met-\\nrics in the nuScenes validation set. Specifically, when the\\nstudent network shows 50.24% NDS and 38.52% mAP,\\nour method achieves 53.90% (+3.66%) NDS and 41.33%\\n(+2.81%) mAP. In detail, our method outperforms the other\\nmethods for the most of object classes except the construc-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='tion vehicle and the bicycle.\\n4.3. Ablation Studies\\nTo analyze of our proposed method in detail, we conduct\\nablation studies on the Waymo dataset, and the whole per-\\nformances are measured by mAPH at level 2 for simplicity.\\nFor the qualitative analysis, we visualize the map-view fea-\\nture at each stage to validate the what kinds of knowledge\\nare transferred from the teacher to the student by the pro-\\nposed method. For simple visualization, we apply the L1\\nnormalization to the map-view feature in the channel direc-\\ntion.\\nAs shown in Fig. 4, the objects and backgrounds are well\\nactivated in the example image of the teacher output. On the\\nother hand, the encoder output is activated by further high-\\nlighting the coarse positions of the target objects. When\\nlooking at the decoder output, we can see that all the fine\\nsurrounding information is represented again. At this point,\\nit is worth noting that compared to the teacher output, the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='target objects are highlighted a little more. From these vi-\\nsual comparisons, we can infer how our method success-\\nfully transfers the object-centered knowledge to the student.\\nWe explore the buffer layer that matches the channel size\\nof the channel-wise autoencoder without the head attention\\nloss. As shown in Table 3, we compare the three types for\\nthe buffer layer: (1) S→Tis the upsampling method that in-\\n13545\\nTable 1. Waymo evaluation. Comparisons with different KD methods in the Waymo validation set. The best accuracy is indicated in bold,\\nand the second-best accuracy is underlined.\\nMethodVehicle Pedestrian Cyclist\\nLevel 1 Level 2 Level 1 Level 2 Level 1 Level 2\\nmAP mAPH mAP mAPH mAP mAPH mAP mAPH mAP mAPH mAP mAPH\\nTeacher [43] 73.72 73.17 65.61 65.11 72.43 61.72 64.73 54.99 64.30 62.61 61.91 60.28\\nStudent ( 1/4) 64.22 63.56 56.21 55.62 63.72 53.22 56.14 46.78 53.01 51.72 50.99 49.75\\nBaseline 64.78 64.05 56.92 56.26 64.85 52.98 57.37 46.75 54.71 52.46 52.65 50.48', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='FitNet [20] 65.11 64.38 57.24 56.58 64.89 53.29 57.37 47.00 54.91 52.61 52.84 50.63\\nEOD-KD [2] 66.50 65.79 58.56 57.92 65.99 54.58 58.48 48.25 55.18 52.93 53.10 50.94\\nSE-SSD [47] 65.95 65.22 58.05 57.40 65.39 53.98 57.92 47.69 55.01 52.98 52.94 50.99\\nTOFD [45] 64.09 63.43 56.13 55.55 66.24 54.98 58.50 48.45 54.95 53.06 52.86 51.04\\nObj. DGCNN [34] 66.07 65.38 59.27 58.55 65.98 54.44 59.42 49.11 54.65 52.62 53.13 50.93\\nSparseKD [38] 65.25 64.59 56.97 56.38 67.44 54.54 59.24 47.83 55.54 53.45 53.63 51.61\\nOurs 67.43 66.72 59.44 58.81 67.26 56.02 59.73 49.61 56.09 54.24 53.96 52.19\\nTable 2. nuScenes evaluation. Comparisons with different KD methods in the nuScenes validation set. The best accuracy is indicated in\\nbold, and the second-best accuracy is underlined.\\nMethod NDS mAP car truck bus trailer con. veh. ped. motor. bicycle tr. cone barrier\\nTeacher [43] 60.16 50.25 84.04 53.48 64.29 31.90 12.50 78.93 44.01 18.18 54.87 60.30', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Student ( 1/4) 50.24 38.52 77.85 38.18 51.38 22.33 3.95 71.51 23.90 3.51 43.03 49.56\\nBaseline 51.48 39.19 78.72 37.90 50.47 22.42 3.51 72.29 26.25 4.65 44.91 50.77\\nFitNet [20] 51.42 38.90 78.30 37.40 50.40 22.20 3.80 72.10 25.70 4.25 44.20 50.60\\nEOD-KD [2] 52.49 39.82 78.40 38.60 50.90 22.70 3.90 73.20 28.20 5.30 45.00 51.97\\nSE-SSD [47] 52.21 39.53 78.69 38.56 49.81 23.70 3.72 72.86 28.27 4.25 44.24 51.18\\nTOFD [45] 52.88 40.57 79.06 39.73 52.03 24.51 3.56 73.51 29.58 5.62 45.34 52.79\\nObj. DGCNN [34] 52.91 40.34 78.95 39.24 53.37 23.96 4.13 72.98 28.63 4.99 44.72 52.46\\nSparseKD [38] 53.01 40.26 78.78 39.50 51.87 23.64 3.30 73.17 29.34 5.75 44.98 52.26\\nOurs 53.90 41.33 79.48 40.38 54.35 26.44 3.58 73.91 30.21 5.39 45.90 53.70\\nFigure 4. Feature visualization on the proposed channel-wise\\nautoencoder. (a) an example input image and (b) the output fea-\\nture of the teacher network. (c) and (d) are the output images of\\nencoder and decoder of the teacher, respectively.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='creases the student’s map-view feature to the teacher’s fea-\\nture. (2) T→Sis the downsampling method that decreases\\nthe teacher’s feature to the student’s feature. (3) (S + T) /\\n2is that the teacher’s feature is downsampled and the stu-Table 3. Buffer layer for different channel size.\\nMethod Vehicle Pedestrian Cyclist Avg.\\nS→T 58.41 48.90 51.90 53.07\\nT→S 58.62 48.78 51.75 53.05\\n(S + T) / 2 58.47 48.84 51.54 52.95\\nTable 4. Effect of shared and non-shared parameters for the\\nautoencoder.\\nMethod Vehicle Pedestrian Cyclist Avg.\\nNon-shared 56.26 45.85 48.23 50.11\\nShared 58.41 48.90 51.90 53.07\\ndent’s feature is upsampled to the median size. The exper-\\niments show that the upsampling method performs better\\nwhen considering all the classes.\\nIn Table 4, we observe the performance difference when\\nthe autoencoder parameters are shared or not. From the re-\\nsult, we can conclude that the shared parameters achieve\\nbetter performance because what we want to is for the stu-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='dent to learn the teacher’s knowledge, not the independent\\nmodel.\\nWe investigate improvements made by our interchange\\ntransfer for KD without the head attention loss as shown in\\nTable 5. Self-reconstruction is a method wherein the de-\\n13546\\nTable 5. Comparison of different reconstruction methods for\\nthe autoencoder.\\nMethod Vehicle Pedestrian Cyclist Avg.\\nSelf Recon. 56.57 47.26 50.29 51.37\\nOurs 58.41 48.90 51.90 53.07\\nTable 6. Comparison of KD methods for the multiple detection\\nhead. KL loss and l1loss denote that directly apply the loss func-\\ntion to all detection heads for KD.\\nMethod Vehicle Pedestrian Cyclist Avg.\\nStudent 55.62 46.78 49.75 50.72\\nBaseline 56.26 46.75 50.48 51.16\\nKL loss [9] 55.92 45.08 47.49 49.50\\nl1loss 55.62 45.10 48.73 49.82\\nAT [44] 56.85 47.34 50.36 51.52\\nLinter 56.41 46.90 50.90 51.40\\nLintra 57.20 47.19 51.23 51.87\\nLattn 57.10 47.34 51.79 52.08\\ncoder uses the corresponding input for the reconstruction\\nand our interchange reconstruction is a method wherein the', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='proposed Litobjective transfers the reconstructed knowl-\\nedge to the opponent network. Our interchange transfer-\\nbased reconstruction achieves better results and note that\\nour main task is not the reconstruction but the 3D object-\\nbased knowledge transfer for KD.\\n3D detection [4] [6] [43] [21] has the multiple detec-\\ntion head. To prove the superiority of the proposed head\\nattention objective for 3D object detection, we make the\\nKD comparison results against only multiple detection head\\nwithout the autoencoder, as shown in Table 6. Since the\\nheatmap head classifies objects and other heads regress 3D\\nbounding box information, Applying KL loss and l1loss to\\nall detection heads has a negative effect. However, it is re-\\nquired to consider the relation of detection heads. In this\\nrespect, our method achieves better performance than the\\nother KD methods which directly mimic the output of de-\\ntection heads or simply employ attention mechanism.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Table 7 shows the overall effect of the proposed losses\\non the KD performances. We set up the experiments by\\nadding each loss based on the supervised loss Lsup. Specif-\\nically, the interchange transfer loss Litimproves on an av-\\nerage of 1.41% mAPH and the compressed representation\\nlossLcrleads to a 0.94% performance improvement. In\\nthe end, the head attention loss Lattn helps to improve the\\nperformance and the final average mAPH is 53.54%. We\\nconclude that each proposed loss contributes positively to\\nperformance improvement in the 3D object detection-based\\nKD task.\\nFrom Table 8, we observed quantitative comparisons\\nof the computational complexity between the student net-\\nwork and the teacher network. Specifically, the student net-\\nwork, which reduced the channel by 1/4, decreased aboutTable 7. Ablation results from investigating effects of different\\ncomponents.\\nLsupLitLcrLattn Vehicle Pedestrian Cyclist Avg.\\n✓ 55.62 46.78 49.75 50.72\\n✓ ✓ 57.41 48.20 50.77 52.13', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='✓ ✓ ✓ 58.41 48.90 51.90 53.07\\n✓ ✓ ✓ ✓ 58.81 49.61 52.19 53.54\\nTable 8. Quantitative evaluation for model efficiency on\\nWaymo dataset.\\nMethod Params (M) FLOPS (G) mAPH / L2\\nPointPillars [13] 4.8 255.0 57.05\\nSECOND [35] 5.3 84.5 57.23\\nPart-A2[24] 4.6 87.1 57.43\\nIA-SSD [46] 2.7 46.1 58.08\\nSparseKD-v0.64 [38] 5.2 85.1 58.89\\nTeacher [43] 5.2 333.9 60.13\\nOurs: Student ( 1/2) 1.5 130.1 59.04\\nOurs: Student ( 1/4) 0.6 45.1 53.54\\n8.6 times compared to the parameters of the teacher, and\\nFLOPS was reduced by 7.4 times. Above all, we should\\nnot overlook the fact that the performance of the student\\nimproved from 50.72% to 53.54% mAPH/L2 by our KD\\nmethod. Furthermore, we apply our method to the student\\nwhose channel was reduced by half. The student’s perfor-\\nmance increases to 59.04%, and the parameters and FLOPS\\ncompared to the teacher are reduced by 3.5 times and 2.6\\ntimes, respectively. Compared to lightweight network-\\nbased methods [13] [35] [24] [46], our student networks are', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='able to derive stable performance with fewer parameters and\\nFLOPS in 3D object detection.\\n5. Conclusion\\nIn this paper, we propose a novel KD method that trans-\\nfers knowledge to produce a lightweight point cloud de-\\ntector. Our main method involves interchange transfer,\\nwhich learns coarse knowledge by increasing the similar-\\nity of the compressed feature and fine knowledge by de-\\ncompressing the map-view feature of the other side us-\\ning the channel-wise autoencoder. Moreover, we intro-\\nduce a method to guide multiple detection head using head\\nrelation-aware self-attention, which refines knowledge by\\nconsidering the relation of instances and properties. Ab-\\nlation studies demonstrate the effectiveness of our proposed\\nalgorithm, and extensive experiments on the two large-scale\\nopen datasets verify that our proposed method achieves\\ncompetitive performance against state-of-the-art methods.\\nAcknowledgement. This work was partly supported by NRF-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='2022R1A2C1091402, BK21 FOUR program of the NRF of Ko-\\nrea funded by the Ministry of Education (NRF5199991014091),\\nand IITP grant funded by the Korea government(MSIT) (No.2021-\\n0-00951, Development of Cloud based Autonomous Driving AI\\nlearning Software; No. 2021-0-02068, Artificial Intelligence In-\\nnovation Hub). W. Hwang is the corresponding author.\\n13547\\nReferences\\n[1] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,\\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\\nancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-\\nmodal dataset for autonomous driving. In Proceedings of\\nthe IEEE/CVF conference on computer vision and pattern\\nrecognition , pages 11621–11631, 2020. 6\\n[2] Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Man-\\nmohan Chandraker. Learning efficient object detection mod-\\nels with knowledge distillation. Advances in neural informa-\\ntion processing systems , 30, 2017. 2, 3, 5, 6, 7\\n[3] Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Fast', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='point r-cnn. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision , pages 9775–9784, 2019. 2\\n[4] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen,\\nMengchen Liu, Lu Yuan, and Lei Zhang. Dynamic head:\\nUnifying object detection heads with attentions. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition , pages 7373–7382, 2021. 2, 8\\n[5] Martin Engelcke, Dushyant Rao, Dominic Zeng Wang,\\nChi Hay Tong, and Ingmar Posner. V ote3deep: Fast ob-\\nject detection in 3d point clouds using efficient convolutional\\nneural networks. In 2017 IEEE International Conference on\\nRobotics and Automation (ICRA) , pages 1355–1361. IEEE,\\n2017. 2\\n[6] Runzhou Ge, Zhuangzhuang Ding, Yihan Hu, Yu Wang, Si-\\njia Chen, Li Huang, and Yuan Li. Afdet: Anchor free one\\nstage 3d object detection. arXiv preprint arXiv:2006.12671 ,\\n2020. 2, 8\\n[7] Xiaoyang Guo, Shaoshuai Shi, Xiaogang Wang, and Hong-\\nsheng Li. Liga-stereo: Learning lidar geometry aware rep-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='resentations for stereo-based 3d detector. In Proceedings\\nof the IEEE/CVF International Conference on Computer Vi-\\nsion, pages 3153–3163, 2021. 3\\n[8] Chenhang He, Hui Zeng, Jianqiang Huang, Xian-Sheng Hua,\\nand Lei Zhang. Structure aware single-stage 3d object detec-\\ntion from point cloud. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition , pages\\n11873–11882, 2020. 2\\n[9] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distill-\\ning the knowledge in a neural network. arXiv preprint\\narXiv:1503.02531 , 2(7), 2015. 1, 3, 6, 8\\n[10] Zhen Huang, Xu Shen, Jun Xing, Tongliang Liu, Xin-\\nmei Tian, Houqiang Li, Bing Deng, Jianqiang Huang, and\\nXian-Sheng Hua. Revisiting knowledge distillation: An in-\\nheritance and exploration framework. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 3579–3588, 2021. 1, 3\\n[11] Jangho Kim, SeongUk Park, and Nojun Kwak. Paraphras-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='ing complex network: Network compression via factor trans-\\nfer.Advances in neural information processing systems , 31,\\n2018. 3\\n[12] Diederik P Kingma and Jimmy Ba. Adam: A method for\\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\\n2014. 6\\n[13] Alex H Lang, Sourabh V ora, Holger Caesar, Lubing Zhou,\\nJiong Yang, and Oscar Beijbom. Pointpillars: Fast encodersfor object detection from point clouds. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition , pages 12697–12705, 2019. 1, 2, 3, 8\\n[14] Bo Li. 3d fully convolutional network for vehicle detection\\nin point cloud. In 2017 IEEE/RSJ International Conference\\non Intelligent Robots and Systems (IROS) , pages 1513–1518.\\nIEEE, 2017. 2\\n[15] Quanquan Li, Shengying Jin, and Junjie Yan. Mimicking\\nvery efficient network for object detection. In Proceedings\\nof the ieee conference on computer vision and pattern recog-\\nnition , pages 6356–6364, 2017. 1, 3', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='[16] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir\\nLevine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Im-\\nproved knowledge distillation via teacher assistant. In Pro-\\nceedings of the AAAI Conference on Artificial Intelligence ,\\npages 5191–5198, 2020. 3\\n[17] Jiquan Ngiam, Benjamin Caine, Wei Han, Brandon Yang,\\nYuning Chai, Pei Sun, Yin Zhou, Xi Yi, Ouais Al-\\nsharif, Patrick Nguyen, et al. Starnet: Targeted compu-\\ntation for object detection in point clouds. arXiv preprint\\narXiv:1908.11069 , 2019. 2\\n[18] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J\\nGuibas. Frustum pointnets for 3d object detection from rgb-\\nd data. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition , pages 918–927, 2018. 2\\n[19] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\\nPointnet: Deep learning on point sets for 3d classification\\nand segmentation. In Proceedings of the IEEE conference\\non computer vision and pattern recognition , pages 652–660,\\n2017. 3', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='2017. 3\\n[20] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,\\nAntoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets:\\nHints for thin deep nets. arXiv preprint arXiv:1412.6550 ,\\n2014. 1, 3, 6, 7\\n[21] Danila Rukhovich, Anna V orontsova, and Anton Konushin.\\nImvoxelnet: Image to voxels projection for monocular and\\nmulti-view general-purpose 3d object detection. In Proceed-\\nings of the IEEE/CVF Winter Conference on Applications of\\nComputer Vision , pages 2397–2406, 2022. 2, 8\\n[22] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping\\nShi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point-\\nvoxel feature set abstraction for 3d object detection. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pages 10529–10538, 2020. 2\\n[23] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointr-\\ncnn: 3d object proposal generation and detection from point\\ncloud. In Proceedings of the IEEE/CVF conference on com-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='puter vision and pattern recognition , pages 770–779, 2019.\\n1, 2\\n[24] Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang,\\nand Hongsheng Li. From points to parts: 3d object detection\\nfrom point cloud with part-aware and part-aggregation net-\\nwork. IEEE transactions on pattern analysis and machine\\nintelligence , 43(8):2647–2664, 2020. 8\\n[25] Weijing Shi and Raj Rajkumar. Point-gnn: Graph neural net-\\nwork for 3d object detection in a point cloud. In Proceedings\\nof the IEEE/CVF conference on computer vision and pattern\\nrecognition , pages 1711–1719, 2020. 2\\n13548\\n[26] Martin Simony, Stefan Milzy, Karl Amendey, and Horst-\\nMichael Gross. Complex-yolo: An euler-region-proposal\\nfor real-time 3d object detection on point clouds. In Pro-\\nceedings of the European Conference on Computer Vision\\n(ECCV) Workshops , pages 0–0, 2018. 3\\n[27] Leslie N Smith. Cyclical learning rates for training neural\\nnetworks. In 2017 IEEE winter conference on applications', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='of computer vision (WACV) , pages 464–472. IEEE, 2017. 6\\n[28] Wonchul Son, Jaemin Na, Junyong Choi, and Wonjun\\nHwang. Densely guided knowledge distillation using multi-\\nple teacher assistants. In Proceedings of the IEEE/CVF Inter-\\nnational Conference on Computer Vision , pages 9395–9404,\\n2021. 1, 3\\n[29] Shuran Song and Jianxiong Xiao. Deep sliding shapes for\\namodal 3d object detection in rgb-d images. In Proceed-\\nings of the IEEE conference on computer vision and pattern\\nrecognition , pages 808–816, 2016. 2\\n[30] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien\\nChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,\\nYuning Chai, Benjamin Caine, et al. Scalability in perception\\nfor autonomous driving: Waymo open dataset. In Proceed-\\nings of the IEEE/CVF conference on computer vision and\\npattern recognition , pages 2446–2454, 2020. 6\\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Polosukhin. Attention is all you need. Advances in neural\\ninformation processing systems , 30, 2017. 5\\n[32] Tao Wang, Li Yuan, Xiaopeng Zhang, and Jiashi Feng. Dis-\\ntilling object detectors with fine-grained feature imitation. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition , pages 4933–4942, 2019. 2,\\n3\\n[33] Yue Wang, Alireza Fathi, Abhijit Kundu, David A Ross,\\nCaroline Pantofaru, Tom Funkhouser, and Justin Solomon.\\nPillar-based object detection for autonomous driving. In\\nEuropean Conference on Computer Vision , pages 18–34.\\nSpringer, 2020. 3\\n[34] Yue Wang and Justin M Solomon. Object dgcnn: 3d object\\ndetection using dynamic graphs. Advances in Neural Infor-\\nmation Processing Systems , 34, 2021. 3, 5, 6, 7\\n[35] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embed-\\nded convolutional detection. Sensors , 18(10):3337, 2018. 1,\\n2, 8\\n[36] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='time 3d object detection from point clouds. In Proceedings of\\nthe IEEE conference on Computer Vision and Pattern Recog-\\nnition , pages 7652–7660, 2018. 1, 2, 3\\n[37] Jing Yang, Brais Martinez, Adrian Bulat, and Georgios Tz-\\nimiropoulos. Knowledge distillation via softmax regres-\\nsion representation learning. In International Conference on\\nLearning Representations , 2020. 3\\n[38] Jihan Yang, Shaoshuai Shi, Runyu Ding, Zhe Wang, and Xi-\\naojuan Qi. Towards efficient 3d object detection with knowl-\\nedge distillation. arXiv preprint arXiv:2205.15156 , 2022. 3,\\n4, 6, 7, 8\\n[39] Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia. 3dssd:\\nPoint-based 3d single stage object detector. In Proceedingsof the IEEE/CVF conference on computer vision and pattern\\nrecognition , pages 11040–11048, 2020. 2\\n[40] Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Jiaya\\nJia. Std: Sparse-to-dense 3d object detector for point cloud.\\nInProceedings of the IEEE/CVF International Conference', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='on Computer Vision , pages 1951–1960, 2019. 2\\n[41] Maosheng Ye, Shuangjie Xu, and Tongyi Cao. Hvnet: Hy-\\nbrid voxel network for lidar based 3d object detection. In\\nProceedings of the IEEE/CVF conference on computer vi-\\nsion and pattern recognition , pages 1631–1640, 2020. 2\\n[42] Zeng Yihan, Chunwei Wang, Yunbo Wang, Hang Xu, Chao-\\nqiang Ye, Zhen Yang, and Chao Ma. Learning transfer-\\nable features for point cloud detection via 3d contrastive co-\\ntraining. Advances in Neural Information Processing Sys-\\ntems, 34, 2021. 2\\n[43] Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-\\nbased 3d object detection and tracking. In Proceedings of\\nthe IEEE/CVF conference on computer vision and pattern\\nrecognition , pages 11784–11793, 2021. 1, 2, 3, 4, 6, 7, 8\\n[44] Sergey Zagoruyko and Nikos Komodakis. Paying more at-\\ntention to attention: Improving the performance of convo-\\nlutional neural networks via attention transfer. 5th interna-\\ntional conference on Learning Representations , Apr. 2017.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='1, 8\\n[45] Linfeng Zhang, Yukang Shi, Zuoqiang Shi, Kaisheng\\nMa, and Chenglong Bao. Task-oriented feature distilla-\\ntion. Advances in Neural Information Processing Systems ,\\n33:14759–14771, 2020. 6, 7\\n[46] Yifan Zhang, Qingyong Hu, Guoquan Xu, Yanxin Ma, Jian-\\nwei Wan, and Yulan Guo. Not all points are equal: Learn-\\ning highly efficient point-based detectors for 3d lidar point\\nclouds. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition , pages 18953–\\n18962, 2022. 8\\n[47] Wu Zheng, Weiliang Tang, Li Jiang, and Chi-Wing Fu. Se-\\nssd: Self-ensembling single-stage object detector from point\\ncloud. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition , pages 14494–14503,\\n2021. 3, 6, 7\\n[48] Yin Zhou, Pei Sun, Yu Zhang, Dragomir Anguelov, Jiyang\\nGao, Tom Ouyang, James Guo, Jiquan Ngiam, and Vijay Va-\\nsudevan. End-to-end multi-view fusion for 3d object detec-\\ntion in lidar point clouds. In Conference on Robot Learning ,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='pages 923–932. PMLR, 2020. 2\\n[49] Yin Zhou and Oncel Tuzel. V oxelnet: End-to-end learning\\nfor point cloud based 3d object detection. In Proceedings of\\nthe IEEE conference on computer vision and pattern recog-\\nnition , pages 4490–4499, 2018. 1, 2, 3\\n13549', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='Towards Scalable Neural Representation for Diverse Videos\\nBo He1Xitong Yang2Hanyu Wang1Zuxuan Wu3Hao Chen1\\nShuaiyi Huang1Yixuan Ren1Ser-Nam Lim2Abhinav Shrivastava1\\n1University of Maryland, College Park2Meta AI3Fudan University\\nhttps://boheumd.github.io/D-NeRV/\\nAbstract\\nImplicit neural representations (INR) have gained in-\\ncreasing attention in representing 3D scenes and images, and\\nhave been recently applied to encode videos ( e.g., NeRV [1],\\nE-NeRV [2]). While achieving promising results, existing\\nINR-based methods are limited to encoding a handful of\\nshort videos ( e.g., seven 5-second videos in the UVG dataset)\\nwith redundant visual content, leading to a model design\\nthat fits individual video frames independently and is not\\nefficiently scalable to a large number of diverse videos. This\\npaper focuses on developing neural representations for a\\nmore practical setup – encoding long and/or a large number\\nof videos with diverse visual content. We first show that', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/He_Towards_Scalable_Neural_Representation_for_Diverse_Videos_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='instead of dividing videos into small subsets and encoding\\nthem with separate models, encoding long and diverse videos\\njointly with a unified model achieves better compression re-\\nsults. Based on this observation, we propose D-NeRV, a\\nnovel neural representation framework designed to encode\\ndiverse videos by (i) decoupling clip-specific visual content\\nfrom motion information, (ii) introducing temporal reason-\\ning into the implicit neural network, and (iii) employing the\\ntask-oriented flow as intermediate output to reduce spatial\\nredundancies. Our new model largely surpasses NeRV and\\ntraditional video compression techniques on UCF101 and\\nUVG datasets on the video compression task. Moreover,\\nwhen used as an efficient data-loader, D-NeRV achieves 3%-\\n10% higher accuracy than NeRV on action recognition tasks\\non the UCF101 dataset under the same compression ratios.\\n1. Introduction\\nImplicit neural representations (INR) have achieved great', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/He_Towards_Scalable_Neural_Representation_for_Diverse_Videos_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='success in parameterizing various signals, such as 3D\\nscenes [3 –5], images [6,7], audio [6], and videos [1,2,8 –10].\\nThe key idea is to represent signals as a function approx-\\nimated by a neural network, mapping a reference coordi-\\nnate to its corresponding signal value. Recently, INR has\\nreceived increasing attention in image and video compres-\\nNeR V\\nD-NeR VNeR V\\nFigure 1. Comparison of D-NeRV and NeRV when representing\\ndiverse videos. NeRV optimizes representation to every video\\nindependently while D-NeRV encodes all videos by a shared model.\\nsion tasks [1, 2, 8, 11 –15]. Compared with learning-based\\nvideo compression techniques [16 –18], INR-based methods\\n(e.g., NeRV [1]) are more favorable due to simpler training\\npipelines and much faster video decoding speed.\\nWhile impressive progress has been made, existing INR-\\nbased methods are limited to encoding a single short video at\\na time. This prohibits the potential applications in most real-', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/He_Towards_Scalable_Neural_Representation_for_Diverse_Videos_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='world scenarios, where we need to represent and compress a\\nlarge number of diverse videos. A straightforward strategy\\nfor encoding diverse videos is to divide them into multiple\\nsubsets and model each of them by a separate neural network,\\nas shown in Figure 1 (top). However, since this strategy is\\nunable to leverage long-term redundancies across videos, it\\nachieves inferior results compared to fitting all diverse videos\\nwith a single shared model. As shown in Figure 2, under the\\nsame compression ratio (bits per pixel), the performance of\\nNeRV is consistently better when fitting a larger number of\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n6132\\nvideos. This suggests that representing multiple videos by a\\nsingle large model is generally more beneficial.', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/He_Towards_Scalable_Neural_Representation_for_Diverse_Videos_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='However, as observed empirically, the current design of\\nNeRV offers diminishing returns when scaling to large and\\ndiverse videos. We argue that the current coupled design of\\ncontent and motion information modeling exaggerates the\\ndifficulty of memorizing diverse videos. To address this, we\\npropose D-NeRV, a novel implicit neural representation that\\nis specifically designed to efficiently encode long or a large\\nnumber of diverse videos1. A representative overview of\\ndifferences between D-NeRV and NeRV is shown in Figure 1.\\nWhen representing diverse videos, NeRV encodes each video\\ninto a separate model or simply concatenates multiple videos\\ninto a long video and encodes it, while our D-NeRV can\\nrepresent different videos in a single model by conditioning\\non key-frames from each video clip.\\nCompared to NeRV , we have the following improvements.\\nFirst, we observe that the visual content of each video of-\\nten represents appearance, both background and foreground,', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/He_Towards_Scalable_Neural_Representation_for_Diverse_Videos_CVPR_2023_paper.txt'}),\n",
              " Document(page_content='which vary significantly among different videos, while the\\nmotion information often represents the semantic structure\\n(e.g., similar motion for the same action class) and can be\\nshared across different videos. Therefore, we decouple each\\nvideo clip into two parts: clip-specific visual content and\\nmotion information, which are modeled separately in our\\nmethod. Second, motivated by the vital importance of tem-\\nporal modeling in video-related tasks, instead of outputting\\neach frame independently, we introduce temporal reasoning\\ninto the INR-based network by explicitly modeling global\\ntemporal dependencies across different frames. Finally, con-\\nsidering the significant spatial redundancies in videos, rather\\nthan predicting the raw pixel values directly, we propose\\nto predict the task-oriented flow [19 –22] as an intermedi-\\nate output, and use it in conjunction with the key-frames to\\nget the final refined output. It alleviates the complexity of', metadata={'source': 'drive/MyDrive/cvpr_papers/text/part1/He_Towards_Scalable_Neural_Representation_for_Diverse_Videos_CVPR_2023_paper.txt'}),\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "TLjq79dl8ayk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "AYP_Xlqxi0XT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb -q\n",
        "!pip install chromadb tiktoken -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNujjz_119-y",
        "outputId": "bb8943e0-8318-41c0-9849-dfa9c58c0ad2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/402.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/402.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.8/402.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "persist_directory = './drive/MyDrive/cvpr_openal_vector_db_2'\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=persist_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URzXTA5f5kQ_",
        "outputId": "acc679b0-2fb6-473a-f9d1-35c2343d0de8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 797446 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 895431 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 821289 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 739464 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 761201 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 855399 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 768449 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 863259 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 781017 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 884480 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 809411 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 899048 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 818145 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 892444 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 809612 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 881955 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 807690 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 913083 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 838018 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 762948 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 860252 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 785043 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 870798 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 793425 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 904838 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 829642 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 754815 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 831185 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 747920 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 842100 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 766582 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read)).\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 809582 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 838967 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 751346 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 835210 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 752364 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 855899 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 775901 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 903233 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 821859 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 739513 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 831925 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 756875 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 861875 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 778797 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 887557 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 812902 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 738543 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 829306 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 747903 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 791564 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 901334 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 813602 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 916711 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 841334 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 765416 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 856137 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 780644 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 878846 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 802908 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 906004 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 831254 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 754392 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 862143 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 786846 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 905557 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 824757 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 749790 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-rqzFEy9NskgyGvnC0pRBU45R on tokens per min. Limit: 1000000 / min. Current: 860907 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "persist_directory = './drive/MyDrive/vector_db'\n",
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
      ],
      "metadata": {
        "id": "kdRe4PBL2Vv0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "qUp9rcku3JwA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First up, let's load our model!"
      ],
      "metadata": {
        "id": "LFgisl_3t8Ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's set up our document vector store as a Retriever tool so we can leverage it in our chain!"
      ],
      "metadata": {
        "id": "cU62nF3RwKKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_retriever = vectordb.as_retriever()"
      ],
      "metadata": {
        "id": "CFjGZSQY2bWo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With that set-up, we're good to set-up our RetrievalQA chain and leverage all the documents we have in our Vector DB!"
      ],
      "metadata": {
        "id": "2-3qh9ie2eCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "cvpr2023_qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=doc_retriever)"
      ],
      "metadata": {
        "id": "4n5ylaE8wQIm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out by itself!"
      ],
      "metadata": {
        "id": "6sOYSlh8wlYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cvpr2023_qa.run(\"Could you summarize Improving Robustness of Semantic Segmentation to Motion-Blur usingClas-Centric Augmentation?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "ZQKy7Gmrwk3n",
        "outputId": "6a3b432a-d99e-4fb9-a2bc-43ba56a3cc1d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The paper focuses on improving the robustness of semantic segmentation to motion-blur. It introduces a Class-Centric Motion-Blur Augmentation (CCMBA) strategy that leverages segmentation map annotations to introduce blur in specific regions of the image. This strategy aims to enhance distinguishability and easier training. The proposed method is generic and can be used with any supervised semantic segmentation network. The authors demonstrate the effectiveness of their approach on various datasets and report improved performance compared to baseline methods.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V9vo5C23tC0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cvpr2023_qa.run(\"What is the significant result in this Improving Robustness of Semantic Segmentation to Motion-Blur usingClas-Centric Augmentation in cvpr 2023?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "h-_j59CcYh1c",
        "outputId": "68764285-ae45-4a1c-c0e5-ab8042aa3ebe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The significant result in the paper \"Improving Robustness of Semantic Segmentation to Motion-Blur using Class-Centric Augmentation\" in CVPR 2023 is the development of a Class-Centric Motion-Blur Augmentation (CCMBA) strategy. This strategy leverages segmentation map annotations to introduce blur in specific regions of the image, improving the robustness of semantic segmentation in the presence of motion-blur. The authors demonstrate the effectiveness of their approach for both CNN and Vision Transformer-based semantic segmentation networks on PASCAL VOC and Cityscapes datasets. They also show improved generalizability to real-world blur by evaluating on the GoPro and REDS deblurring datasets.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cvpr2023_qa.run(\"What social, economic, or policy implications can be derived from Improving Robustness of Semantic Segmentation to Motion-Blur usingClas-Centric Augmentation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "fpGJnH5GY22H",
        "outputId": "9a594481-5b56-4c25-df64-ca6c2c30682c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the provided context, it is not possible to directly derive specific social, economic, or policy implications from the paper \"Improving Robustness of Semantic Segmentation to Motion-Blur using Class-Centric Augmentation.\" The paper primarily focuses on the technical aspects of improving semantic segmentation in the presence of motion blur through data augmentation techniques. It does not explicitly discuss or analyze the broader social, economic, or policy implications of this research. To understand the implications, further analysis and interpretation would be required beyond the scope of the provided context.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cvpr2023_qa.run(\"What social, economic, or policy implications can be derived from Improving Robustness of Semantic Segmentation to Motion-Blur usingClas-Centric Augmentation?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "TeBjASH4Y-Qr",
        "outputId": "b785070a-b91c-46ca-b503-fcd26931ea33"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the provided context, it is not possible to determine the specific social, economic, or policy implications that can be derived from the research paper \"Improving Robustness of Semantic Segmentation to Motion-Blur using Class-Centric Augmentation.\" The paper primarily focuses on the technical aspects of improving semantic segmentation in the presence of motion blur. It does not explicitly discuss or analyze the broader social, economic, or policy implications of the proposed method.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cvpr2023_qa.run(\"What are the limitations or constraints of Improving Robustness of Semantic Segmentation to Motion-Blur usingClas-Centric Augmentation?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VYHMGS1HZCoV",
        "outputId": "4daf98c5-fb8f-4b0e-d701-128456a9e4a9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The given context does not provide specific information about the limitations or constraints of the proposed method.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cvpr2023_qa.run(\"Are there any suggestions for future research or further studies of Robustness of Semantic Segmentation to Motion-Blur usingClas-Centric Augmentation?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "30t6ozdwZLuH",
        "outputId": "1f359b73-f80f-416b-ec8b-8b2773b82d4f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The paper does not explicitly mention suggestions for future research or further studies. However, based on the findings and contributions of the paper, some potential areas for future research could include:\\n\\n1. Exploring different types of blur: The paper focuses on motion-blur as the specific type of degradation. Future studies could investigate the robustness of semantic segmentation to other types of blur, such as defocus blur or Gaussian blur.\\n\\n2. Investigating different augmentation strategies: The paper proposes a Class-Centric Motion-Blur Augmentation (CCMBA) strategy. Further studies could explore alternative augmentation strategies to improve the robustness of semantic segmentation to motion-blur or other degradations.\\n\\n3. Evaluating on additional datasets: The paper evaluates the proposed method on PASCAL VOC, Cityscapes, GoPro, and REDS datasets. Future research could consider evaluating the method on other benchmark datasets or real-world scenarios to further validate its effectiveness and generalizability.\\n\\n4. Comparing with other state-of-the-art methods: The paper compares the proposed method with baseline methods, but future studies could compare it with other state-of-the-art methods for semantic segmentation under motion-blur or other degradations to assess its performance and potential improvements.\\n\\n5. Exploring the impact of different blur parameters: The paper mentions using a synthetic non-linear kernel for blurring. Further research could investigate the impact of different blur parameters, such as kernel size, kernel shape, or blur intensity, on the robustness of semantic segmentation.\\n\\nThese are just a few potential directions for future research based on the context provided. Actual suggestions for future studies may vary depending on the specific goals and interests of researchers in the field.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cvpr2023_qa.run(\"What is the Research Question or Hypothesis in this Improving Robustness of Semantic Segmentation to Motion-Blur usingClas-Centric Augmentation?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iDxhGgG9Zq1S",
        "outputId": "434974db-fab9-42b7-a102-01c52b6cbbc6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The research question in this study is how to improve the robustness of semantic segmentation to motion-blur using class-centric augmentation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "display(Markdown(\"The significant result in this paper is the demonstration of a novel approach to improve the robustness of semantic segmentation models to motion blur, which is a complex degradation that affects scene understanding in autonomous driving and other applications. The proposed approach, called Class-Centric Motion Blur Augmentation (CCMBA), leverages the semantic segmentation annotations to introduce blur in specific regions of the image, thereby enhancing the distinguishability between different objects and improving the model's ability to generalize to dynamic scenes. The authors show that their approach leads to improved performance compared to traditional data augmentation strategies and state-of-the-art segmentation models\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "gjLYxY8Vuu9R",
        "outputId": "b4810e37-5bed-4e30-e4dc-fa06745f9508"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The significant result in this paper is the demonstration of a novel approach to improve the robustness of semantic segmentation models to motion blur, which is a complex degradation that affects scene understanding in autonomous driving and other applications. The proposed approach, called Class-Centric Motion Blur Augmentation (CCMBA), leverages the semantic segmentation annotations to introduce blur in specific regions of the image, thereby enhancing the distinguishability between different objects and improving the model's ability to generalize to dynamic scenes. The authors show that their approach leads to improved performance compared to traditional data augmentation strategies and state-of-the-art segmentation models"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's set-up a search tool leveraging SERP API!\n",
        "\n",
        "You can find the process to set this up [here](https://serpapi.com/)"
      ],
      "metadata": {
        "id": "yf--RcSM6MIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_k9kCud-MSU",
        "outputId": "c3724d81-1102-4206-9139-bacf9ff8e7c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"SERPAPI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "9K7yvsap-EoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.utilities import SerpAPIWrapper\n",
        "\n",
        "search = SerpAPIWrapper()"
      ],
      "metadata": {
        "id": "XbOXzzQh-ASf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can combine those"
      ],
      "metadata": {
        "id": "c-2I6ElC-y1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType\n",
        "from langchain.tools import BaseTool\n",
        "from langchain.llms import OpenAI\n",
        "from langchain import LLMMathChain, SerpAPIWrapper\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name = \"Shakespeare QA System\",\n",
        "        func=shakespeare_qa.run,\n",
        "        description=\"useful for when you need to answer questions about Shakespeare's works. Input should be a fully formed question.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name = \"SERP API Search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for when you need to answer questions about ruff (a python linter). Input should be a fully formed question.\"\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "rhtl7b_D9oZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
      ],
      "metadata": {
        "id": "72tp3fOe-ox6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"What is the name of Hamlet's Mother?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "GZfR8q8m-p-y",
        "outputId": "9d9ef201-5c07-44c3-fc79-5fac7c5f943b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to find the name of Hamlet's mother from Shakespeare's play.\n",
            "Action: Shakespeare QA System\n",
            "Action Input: Who is Hamlet's mother in the play Hamlet?\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mHamlet's mother in the play Hamlet is Queen Gertrude.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
            "Final Answer: Queen Gertrude\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Queen Gertrude'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's add some memory to our application!"
      ],
      "metadata": {
        "id": "CnqYg-sH_EFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "readonlymemory = ReadOnlySharedMemory(memory=memory)"
      ],
      "metadata": {
        "id": "R9v1B7Ce_Hha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to let our Tools leverage the memory - we'll want to add the ability to read the memory to their chains!"
      ],
      "metadata": {
        "id": "ko1K3Vsl_Pb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=doc_retriever, memory=readonlymemory)"
      ],
      "metadata": {
        "id": "KHM5oJ3e_Xcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can re-create our Tools list - this time using the memory capable version of the `shakespeare_qa` chain!"
      ],
      "metadata": {
        "id": "nKTsdlKv_odP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    Tool(\n",
        "        name = \"Shakespeare QA System\",\n",
        "        func=shakespeare_qa.run,\n",
        "        description=\"useful for when you need to answer questions about Shakespeare's works. Input should be a fully formed question.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name = \"SERP API Search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for when you need to answer questions about ruff (a python linter). Input should be a fully formed question.\"\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "_2A-iS4i_v0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now add some of the prompt pieces so our agent can respond exactly as we expect!"
      ],
      "metadata": {
        "id": "FjE74x-d_4LJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import ZeroShotAgent, Tool, AgentExecutor\n",
        "\n",
        "prefix = \"\"\"Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:\"\"\"\n",
        "suffix = \"\"\"Begin!\"\n",
        "\n",
        "{chat_history}\n",
        "Question: {input}\n",
        "{agent_scratchpad}\"\"\"\n",
        "\n",
        "prompt = ZeroShotAgent.create_prompt(\n",
        "    tools,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"input\", \"chat_history\", \"agent_scratchpad\"]\n",
        ")"
      ],
      "metadata": {
        "id": "1HojDlI__-tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's add this prompt template to our OpenAI LLM!"
      ],
      "metadata": {
        "id": "nR8F_bkxANt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, LLMChain, PromptTemplate\n",
        "\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "y2wi-Yc6ARvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can set up our final agent!"
      ],
      "metadata": {
        "id": "pc-KsgdEA-YC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)\n",
        "agent_chain = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True, memory=memory)"
      ],
      "metadata": {
        "id": "w0Qu44l4BBFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test this out!"
      ],
      "metadata": {
        "id": "aVPaRaqfA2sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.run(input=\"Who is Hamlet's Mother?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "s8wcjyokA398",
        "outputId": "2e5db35a-4112-441c-f8a7-848ae637e307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to find information about Hamlet's mother from Shakespeare's works.\n",
            "Action: Shakespeare QA System\n",
            "Action Input: Who is Hamlet's mother in Shakespeare's play?\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mHamlet's mother in Shakespeare's play is Queen Gertrude.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
            "Final Answer: Hamlet's mother is Queen Gertrude.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hamlet's mother is Queen Gertrude.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.run(input=\"What age was she in the play?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "isP5VIquBHYt",
        "outputId": "823e230b-8817-45c6-952a-bfde826367aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to find the age of Queen Gertrude in the play Hamlet.\n",
            "Action: Shakespeare QA System\n",
            "Action Input: What age is Queen Gertrude in the play Hamlet?\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mQueen Gertrude's age is not explicitly mentioned in the play Hamlet.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
            "Final Answer: Queen Gertrude's age is not explicitly mentioned in the play Hamlet.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Queen Gertrude's age is not explicitly mentioned in the play Hamlet.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.run(input=\"Did she live through the play?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "0lo2Id_UBQB5",
        "outputId": "27008f4f-539a-4049-aca9-faa3c4d08ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to check if Queen Gertrude survived in the play Hamlet.\n",
            "Action: Shakespeare QA System\n",
            "Action Input: Did Queen Gertrude survive in the play Hamlet?\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mYes, Queen Gertrude survives for most of the play Hamlet. However, she eventually dies in the final act (Act 5, Scene 2) after accidentally drinking poisoned wine that was intended for Hamlet.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
            "Final Answer: Queen Gertrude does not live through the entire play. She dies in Act 5, Scene 2 after accidentally drinking poisoned wine.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Queen Gertrude does not live through the entire play. She dies in Act 5, Scene 2 after accidentally drinking poisoned wine.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "Here we have it!\n",
        "\n",
        "A system capable of querying over multiple documents - all without every needing to hit an external API!"
      ],
      "metadata": {
        "id": "o_kqcyu82_mW"
      }
    }
  ]
}